#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–‡ä»¶IOå·¥å…·æ¨¡å— - æ–‡ä»¶æ“ä½œã€ç¼“å­˜å’Œåºåˆ—åŒ–
"""

import os
import json
import yaml
import pickle
import hashlib
import shutil
from pathlib import Path
from typing import Any, Dict, List, Optional, Union, Callable
from datetime import datetime, timedelta
import threading
import time

class FileCache:
    """æ–‡ä»¶ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, cache_dir: str = "./6_output/cache", max_size: int = 1000, ttl: int = 3600):
        """
        åˆå§‹åŒ–æ–‡ä»¶ç¼“å­˜
        Args:
            cache_dir: ç¼“å­˜ç›®å½•
            max_size: æœ€å¤§ç¼“å­˜æ¡ç›®æ•°
            ttl: ç¼“å­˜ç”Ÿå­˜æ—¶é—´(ç§’)
        """
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.max_size = max_size
        self.ttl = ttl
        self._cache_index = {}
        self._lock = threading.RLock()
        
        # åŠ è½½ç¼“å­˜ç´¢å¼•
        self._load_index()
        
        # å¯åŠ¨æ¸…ç†çº¿ç¨‹
        self._start_cleanup_thread()
    
    def _get_cache_key(self, key: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®çš„å“ˆå¸Œå€¼"""
        return hashlib.md5(key.encode()).hexdigest()
    
    def _get_cache_path(self, cache_key: str) -> Path:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        return self.cache_dir / f"{cache_key}.cache"
    
    def _load_index(self):
        """åŠ è½½ç¼“å­˜ç´¢å¼•"""
        index_file = self.cache_dir / "cache_index.json"
        if index_file.exists():
            try:
                with open(index_file, 'r', encoding='utf-8') as f:
                    self._cache_index = json.load(f)
            except Exception:
                self._cache_index = {}
    
    def _save_index(self):
        """ä¿å­˜ç¼“å­˜ç´¢å¼•"""
        index_file = self.cache_dir / "cache_index.json"
        with open(index_file, 'w', encoding='utf-8') as f:
            json.dump(self._cache_index, f, indent=2)
    
    def get(self, key: str) -> Optional[Any]:
        """
        è·å–ç¼“å­˜å€¼
        Args:
            key: ç¼“å­˜é”®
        Returns:
            ç¼“å­˜å€¼ï¼Œå¦‚æœä¸å­˜åœ¨æˆ–è¿‡æœŸè¿”å›None
        """
        with self._lock:
            cache_key = self._get_cache_key(key)
            
            if cache_key not in self._cache_index:
                return None
            
            cache_info = self._cache_index[cache_key]
            
            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
            if time.time() - cache_info['timestamp'] > self.ttl:
                self._remove_cache(cache_key)
                return None
            
            # è¯»å–ç¼“å­˜æ–‡ä»¶
            cache_path = self._get_cache_path(cache_key)
            if not cache_path.exists():
                del self._cache_index[cache_key]
                return None
            
            try:
                with open(cache_path, 'rb') as f:
                    return pickle.load(f)
            except Exception:
                self._remove_cache(cache_key)
                return None
    
    def set(self, key: str, value: Any):
        """
        è®¾ç½®ç¼“å­˜å€¼
        Args:
            key: ç¼“å­˜é”®
            value: ç¼“å­˜å€¼
        """
        with self._lock:
            # æ£€æŸ¥ç¼“å­˜å¤§å°é™åˆ¶
            if len(self._cache_index) >= self.max_size:
                self._evict_oldest()
            
            cache_key = self._get_cache_key(key)
            cache_path = self._get_cache_path(cache_key)
            
            # ä¿å­˜ç¼“å­˜å€¼
            try:
                with open(cache_path, 'wb') as f:
                    pickle.dump(value, f)
                
                # æ›´æ–°ç´¢å¼•
                self._cache_index[cache_key] = {
                    'key': key,
                    'timestamp': time.time(),
                    'size': cache_path.stat().st_size
                }
                
                self._save_index()
                
            except Exception as e:
                if cache_path.exists():
                    cache_path.unlink()
                raise e
    
    def _remove_cache(self, cache_key: str):
        """åˆ é™¤ç¼“å­˜é¡¹"""
        cache_path = self._get_cache_path(cache_key)
        if cache_path.exists():
            cache_path.unlink()
        
        if cache_key in self._cache_index:
            del self._cache_index[cache_key]
    
    def _evict_oldest(self):
        """åˆ é™¤æœ€æ—§çš„ç¼“å­˜é¡¹"""
        if not self._cache_index:
            return
        
        oldest_key = min(
            self._cache_index.keys(),
            key=lambda k: self._cache_index[k]['timestamp']
        )
        
        self._remove_cache(oldest_key)
    
    def _start_cleanup_thread(self):
        """å¯åŠ¨æ¸…ç†çº¿ç¨‹"""
        def cleanup_worker():
            while True:
                time.sleep(300)  # 5åˆ†é’Ÿæ¸…ç†ä¸€æ¬¡
                self._cleanup_expired()
        
        cleanup_thread = threading.Thread(target=cleanup_worker, daemon=True)
        cleanup_thread.start()
    
    def _cleanup_expired(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        with self._lock:
            current_time = time.time()
            expired_keys = [
                key for key, info in self._cache_index.items()
                if current_time - info['timestamp'] > self.ttl
            ]
            
            for key in expired_keys:
                self._remove_cache(key)
            
            if expired_keys:
                self._save_index()
    
    def clear(self):
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"""
        with self._lock:
            for cache_key in list(self._cache_index.keys()):
                self._remove_cache(cache_key)
            
            self._cache_index = {}
            self._save_index()

class IOUtils:
    """æ–‡ä»¶IOå·¥å…·ç±»"""
    
    @staticmethod
    def read_json(file_path: Union[str, Path], encoding: str = 'utf-8') -> Dict[str, Any]:
        """
        è¯»å–JSONæ–‡ä»¶
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            encoding: æ–‡ä»¶ç¼–ç 
        Returns:
            è§£æåçš„å­—å…¸
        """
        with open(file_path, 'r', encoding=encoding) as f:
            return json.load(f)
    
    @staticmethod
    def write_json(data: Dict[str, Any], file_path: Union[str, Path], 
                   encoding: str = 'utf-8', indent: int = 2):
        """
        å†™å…¥JSONæ–‡ä»¶
        Args:
            data: è¦å†™å…¥çš„æ•°æ®
            file_path: æ–‡ä»¶è·¯å¾„
            encoding: æ–‡ä»¶ç¼–ç 
            indent: ç¼©è¿›ç©ºæ ¼æ•°
        """
        file_path = Path(file_path)
        file_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(file_path, 'w', encoding=encoding) as f:
            json.dump(data, f, ensure_ascii=False, indent=indent)
    
    @staticmethod
    def read_yaml(file_path: Union[str, Path], encoding: str = 'utf-8') -> Dict[str, Any]:
        """
        è¯»å–YAMLæ–‡ä»¶
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            encoding: æ–‡ä»¶ç¼–ç 
        Returns:
            è§£æåçš„å­—å…¸
        """
        with open(file_path, 'r', encoding=encoding) as f:
            return yaml.safe_load(f)
    
    @staticmethod
    def write_yaml(data: Dict[str, Any], file_path: Union[str, Path], 
                   encoding: str = 'utf-8'):
        """
        å†™å…¥YAMLæ–‡ä»¶
        Args:
            data: è¦å†™å…¥çš„æ•°æ®
            file_path: æ–‡ä»¶è·¯å¾„
            encoding: æ–‡ä»¶ç¼–ç 
        """
        file_path = Path(file_path)
        file_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(file_path, 'w', encoding=encoding) as f:
            yaml.safe_dump(data, f, default_flow_style=False, allow_unicode=True)
    
    @staticmethod
    def read_text(file_path: Union[str, Path], encoding: str = 'utf-8') -> str:
        """
        è¯»å–æ–‡æœ¬æ–‡ä»¶
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            encoding: æ–‡ä»¶ç¼–ç 
        Returns:
            æ–‡ä»¶å†…å®¹
        """
        with open(file_path, 'r', encoding=encoding) as f:
            return f.read()
    
    @staticmethod
    def write_text(content: str, file_path: Union[str, Path], 
                   encoding: str = 'utf-8'):
        """
        å†™å…¥æ–‡æœ¬æ–‡ä»¶
        Args:
            content: æ–‡ä»¶å†…å®¹
            file_path: æ–‡ä»¶è·¯å¾„
            encoding: æ–‡ä»¶ç¼–ç 
        """
        file_path = Path(file_path)
        file_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(file_path, 'w', encoding=encoding) as f:
            f.write(content)
    
    @staticmethod
    def read_lines(file_path: Union[str, Path], encoding: str = 'utf-8', 
                   strip: bool = True) -> List[str]:
        """
        æŒ‰è¡Œè¯»å–æ–‡ä»¶
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            encoding: æ–‡ä»¶ç¼–ç 
            strip: æ˜¯å¦å»é™¤è¡Œå°¾ç©ºç™½å­—ç¬¦
        Returns:
            è¡Œåˆ—è¡¨
        """
        with open(file_path, 'r', encoding=encoding) as f:
            lines = f.readlines()
            if strip:
                lines = [line.strip() for line in lines]
            return lines
    
    @staticmethod
    def write_lines(lines: List[str], file_path: Union[str, Path], 
                    encoding: str = 'utf-8'):
        """
        æŒ‰è¡Œå†™å…¥æ–‡ä»¶
        Args:
            lines: è¡Œåˆ—è¡¨
            file_path: æ–‡ä»¶è·¯å¾„
            encoding: æ–‡ä»¶ç¼–ç 
        """
        file_path = Path(file_path)
        file_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(file_path, 'w', encoding=encoding) as f:
            for line in lines:
                f.write(line + '\n')
    
    @staticmethod
    def copy_file(src: Union[str, Path], dst: Union[str, Path], 
                  create_dirs: bool = True):
        """
        å¤åˆ¶æ–‡ä»¶
        Args:
            src: æºæ–‡ä»¶è·¯å¾„
            dst: ç›®æ ‡æ–‡ä»¶è·¯å¾„
            create_dirs: æ˜¯å¦åˆ›å»ºç›®æ ‡ç›®å½•
        """
        src = Path(src)
        dst = Path(dst)
        
        if create_dirs:
            dst.parent.mkdir(parents=True, exist_ok=True)
        
        shutil.copy2(src, dst)
    
    @staticmethod
    def copy_directory(src: Union[str, Path], dst: Union[str, Path]):
        """
        å¤åˆ¶ç›®å½•
        Args:
            src: æºç›®å½•è·¯å¾„
            dst: ç›®æ ‡ç›®å½•è·¯å¾„
        """
        shutil.copytree(src, dst, dirs_exist_ok=True)
    
    @staticmethod
    def move_file(src: Union[str, Path], dst: Union[str, Path]):
        """
        ç§»åŠ¨æ–‡ä»¶
        Args:
            src: æºæ–‡ä»¶è·¯å¾„
            dst: ç›®æ ‡æ–‡ä»¶è·¯å¾„
        """
        shutil.move(src, dst)
    
    @staticmethod
    def delete_file(file_path: Union[str, Path]):
        """
        åˆ é™¤æ–‡ä»¶
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
        """
        file_path = Path(file_path)
        if file_path.exists():
            file_path.unlink()
    
    @staticmethod
    def delete_directory(dir_path: Union[str, Path]):
        """
        åˆ é™¤ç›®å½•
        Args:
            dir_path: ç›®å½•è·¯å¾„
        """
        dir_path = Path(dir_path)
        if dir_path.exists():
            shutil.rmtree(dir_path)
    
    @staticmethod
    def ensure_directory(dir_path: Union[str, Path]):
        """
        ç¡®ä¿ç›®å½•å­˜åœ¨
        Args:
            dir_path: ç›®å½•è·¯å¾„
        """
        Path(dir_path).mkdir(parents=True, exist_ok=True)
    
    @staticmethod
    def get_file_size(file_path: Union[str, Path]) -> int:
        """
        è·å–æ–‡ä»¶å¤§å°
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
        Returns:
            æ–‡ä»¶å¤§å°(å­—èŠ‚)
        """
        return Path(file_path).stat().st_size
    
    @staticmethod
    def get_file_hash(file_path: Union[str, Path], algorithm: str = 'md5') -> str:
        """
        è·å–æ–‡ä»¶å“ˆå¸Œå€¼
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            algorithm: å“ˆå¸Œç®—æ³•(md5, sha1, sha256ç­‰)
        Returns:
            æ–‡ä»¶å“ˆå¸Œå€¼
        """
        hash_func = hashlib.new(algorithm)
        
        with open(file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_func.update(chunk)
        
        return hash_func.hexdigest()
    
    @staticmethod
    def find_files(directory: Union[str, Path], pattern: str = "*", 
                   recursive: bool = True) -> List[Path]:
        """
        æŸ¥æ‰¾æ–‡ä»¶
        Args:
            directory: æœç´¢ç›®å½•
            pattern: æ–‡ä»¶æ¨¡å¼
            recursive: æ˜¯å¦é€’å½’æœç´¢
        Returns:
            æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        directory = Path(directory)
        
        if recursive:
            return list(directory.rglob(pattern))
        else:
            return list(directory.glob(pattern))

def with_file_cache(cache_key_func: Optional[Callable] = None, 
                   ttl: int = 3600, cache_dir: str = "./6_output/cache"):
    """
    æ–‡ä»¶ç¼“å­˜è£…é¥°å™¨
    Args:
        cache_key_func: ç¼“å­˜é”®ç”Ÿæˆå‡½æ•°
        ttl: ç¼“å­˜ç”Ÿå­˜æ—¶é—´
        cache_dir: ç¼“å­˜ç›®å½•
    """
    cache = FileCache(cache_dir=cache_dir, ttl=ttl)
    
    def decorator(func):
        def wrapper(*args, **kwargs):
            # ç”Ÿæˆç¼“å­˜é”®
            if cache_key_func:
                cache_key = cache_key_func(*args, **kwargs)
            else:
                cache_key = f"{func.__name__}_{hash(str(args) + str(kwargs))}"
            
            # å°è¯•è·å–ç¼“å­˜
            cached_result = cache.get(cache_key)
            if cached_result is not None:
                return cached_result
            
            # æ‰§è¡Œå‡½æ•°å¹¶ç¼“å­˜ç»“æœ
            result = func(*args, **kwargs)
            cache.set(cache_key, result)
            
            return result
        
        return wrapper
    return decorator

# å…¨å±€å®ä¾‹
file_cache = FileCache()
io_utils = IOUtils()

if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    
    # æµ‹è¯•JSONæ“ä½œ
    test_data = {"test": "data", "timestamp": str(datetime.now())}
    test_file = "./6_output/test/test.json"
    
    print("ğŸ§ª æµ‹è¯•æ–‡ä»¶IOå·¥å…·...")
    
    # å†™å…¥å’Œè¯»å–JSON
    IOUtils.write_json(test_data, test_file)
    loaded_data = IOUtils.read_json(test_file)
    print(f"âœ… JSONè¯»å†™æµ‹è¯•é€šè¿‡: {loaded_data}")
    
    # æµ‹è¯•ç¼“å­˜
    cache = FileCache()
    cache.set("test_key", {"cached": "value"})
    cached_value = cache.get("test_key")
    print(f"âœ… ç¼“å­˜æµ‹è¯•é€šè¿‡: {cached_value}")
    
    # æµ‹è¯•è£…é¥°å™¨
    @with_file_cache()
    def expensive_function(x):
        time.sleep(0.1)  # æ¨¡æ‹Ÿè€—æ—¶æ“ä½œ
        return x * 2
    
    start_time = time.time()
    result1 = expensive_function(42)
    first_time = time.time() - start_time
    
    start_time = time.time()
    result2 = expensive_function(42)
    second_time = time.time() - start_time
    
    print(f"âœ… ç¼“å­˜è£…é¥°å™¨æµ‹è¯•: ç¬¬ä¸€æ¬¡ {first_time:.3f}s, ç¬¬äºŒæ¬¡ {second_time:.3f}s")
    
    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")