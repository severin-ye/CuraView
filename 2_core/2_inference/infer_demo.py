import os
import time
import threading
from typing import List, Dict, Optional
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import re



def generate_response(model, tokenizer, prompt: str, max_tokens: int = 10000, temperature: float = 0.7) -> Dict:
    """
    æ¨ç†å‡½æ•°ï¼Œè´Ÿè´£ç”Ÿæˆå›å¤
    
    Args:
        model: å·²åŠ è½½çš„è¯­è¨€æ¨¡å‹
        tokenizer: å¯¹åº”çš„åˆ†è¯å™¨
        prompt: ç”¨æˆ·è¾“å…¥çš„æç¤ºæ–‡æœ¬
        max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°é‡ï¼Œé»˜è®¤512
        temperature: é‡‡æ ·æ¸©åº¦ï¼Œæ§åˆ¶ç”Ÿæˆéšæœºæ€§ï¼Œé»˜è®¤0.7
    
    Returns:
        Dict: åŒ…å«ç”Ÿæˆçš„å›å¤æ–‡æœ¬å’Œè¯¦ç»†ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
    """
    # æ„å»ºå¯¹è¯æ ¼å¼çš„æ¶ˆæ¯åˆ—è¡¨ï¼ŒåŒ…å«ç”¨æˆ·è§’è‰²å’Œå†…å®¹
    messages = [{"role": "user", "content": prompt}]
    
    # ä½¿ç”¨åˆ†è¯å™¨çš„èŠå¤©æ¨¡æ¿å°†æ¶ˆæ¯è½¬æ¢ä¸ºæ¨¡å‹å¯ç†è§£çš„æ–‡æœ¬æ ¼å¼
    # add_generation_prompt=True ä¼šæ·»åŠ åŠ©æ‰‹çš„å›å¤æç¤ºç¬¦
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    
    # å°†æ–‡æœ¬è½¬æ¢ä¸ºtoken IDå¼ é‡ï¼Œå¹¶ç§»åŠ¨åˆ°æ¨¡å‹æ‰€åœ¨çš„è®¾å¤‡ï¼ˆGPU/CPUï¼‰
    inputs = tokenizer(text, return_tensors="pt").to(model.device)
    
    # è®°å½•æ¨ç†å¼€å§‹æ—¶é—´ï¼Œç”¨äºæ€§èƒ½ç»Ÿè®¡
    start_time = time.time()
    
    # ä½¿ç”¨torch.no_grad()ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼ŒèŠ‚çœå†…å­˜å¹¶åŠ é€Ÿæ¨ç†
    with torch.no_grad():
        # è°ƒç”¨æ¨¡å‹çš„ç”Ÿæˆæ–¹æ³•è¿›è¡Œæ–‡æœ¬ç”Ÿæˆ
        outputs = model.generate(
            **inputs,                           # è§£åŒ…è¾“å…¥å¼ é‡ï¼ˆåŒ…å«input_ids, attention_maskç­‰ï¼‰
            max_new_tokens=max_tokens,          # é™åˆ¶æ–°ç”Ÿæˆçš„tokenæ•°é‡
            temperature=temperature,            # æ§åˆ¶ç”Ÿæˆçš„éšæœºæ€§ï¼Œè¶Šé«˜è¶Šéšæœº
            do_sample=True,                     # å¯ç”¨é‡‡æ ·è€Œéè´ªå¿ƒè§£ç 
            pad_token_id=tokenizer.eos_token_id,  # è®¾ç½®å¡«å……token ID
            eos_token_id=tokenizer.eos_token_id,  # è®¾ç½®ç»“æŸtoken IDï¼Œæ˜ç¡®æŒ‡å®šåœæ­¢æ¡ä»¶
            # ä»¥ä¸‹å‚æ•°ç”¨äºæé«˜ç”Ÿæˆè´¨é‡ï¼Œé¿å…ä¹±ç å’Œé‡å¤
            repetition_penalty=1.1,            # é‡å¤æƒ©ç½šï¼Œé™ä½é‡å¤å†…å®¹çš„æ¦‚ç‡
            no_repeat_ngram_size=3              # é˜²æ­¢3-gramåŠä»¥ä¸Šçš„é‡å¤åºåˆ—
        )
    
    # è®°å½•æ¨ç†ç»“æŸæ—¶é—´
    end_time = time.time()
    
    # ========== è®¡ç®—æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯ ==========
    generation_time = end_time - start_time                    # æ€»ç”Ÿæˆè€—æ—¶ï¼ˆç§’ï¼‰
    input_length = inputs['input_ids'].shape[1]               # è¾“å…¥promptçš„tokené•¿åº¦
    total_length = outputs[0].shape[0]                        # è¾“å‡ºåºåˆ—çš„æ€»tokené•¿åº¦
    generated_tokens = total_length - input_length            # æ–°ç”Ÿæˆçš„tokenæ•°é‡
    # è®¡ç®—æ¯ç§’ç”Ÿæˆçš„tokenæ•°ï¼Œé¿å…é™¤é›¶é”™è¯¯
    tokens_per_second = generated_tokens / generation_time if generation_time > 0 else 0
    
    # ========== æ”¹è¿›çš„æ–‡æœ¬è§£ç å’Œæ¸…ç† ==========
    try:
        # åªæå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†token IDsï¼Œé¿å…åŒ…å«è¾“å…¥çš„prompt
        generated_ids = outputs[0][input_length:]
        
        # è§£ç ç”Ÿæˆçš„tokenä¸ºæ–‡æœ¬
        # skip_special_tokens=True: è·³è¿‡ç‰¹æ®Štokenå¦‚<eos>, <pad>ç­‰
        # clean_up_tokenization_spaces=True: æ¸…ç†åˆ†è¯äº§ç”Ÿçš„å¤šä½™ç©ºæ ¼
        response_text = tokenizer.decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)
        
        # ä½¿ç”¨ä¸“é—¨çš„æ¸…ç†å‡½æ•°ç§»é™¤ä¹±ç å’Œç‰¹æ®Šå­—ç¬¦
        response_text = clean_text_output(response_text)
        
        # ç§»é™¤å¯èƒ½æ®‹ç•™çš„ç»“æŸæ ‡è®°æ–‡æœ¬ï¼ˆæŸäº›æ¨¡å‹å¯èƒ½ä¼šè¾“å‡ºå¯è§çš„ç»“æŸç¬¦ï¼‰
        if hasattr(tokenizer, 'eos_token') and tokenizer.eos_token:
            response_text = response_text.replace(tokenizer.eos_token, '')
        
        # ========== å¤„ç†å¯èƒ½è¢«æˆªæ–­çš„å¥å­ ==========
        # æ£€æŸ¥æ–‡æœ¬é•¿åº¦è¶³å¤Ÿä¸”å¯èƒ½å­˜åœ¨æˆªæ–­é—®é¢˜
        if response_text and len(response_text) > 10:
            # è·å–æ–‡æœ¬çš„æœ€åä¸€ä¸ªå­—ç¬¦
            last_char = response_text[-1]
            
            # å¦‚æœæœ€åä¸€ä¸ªå­—ç¬¦ä¸æ˜¯æ ‡ç‚¹ç¬¦å·ä¹Ÿä¸æ˜¯å­—æ¯æ•°å­—ï¼Œå¯èƒ½æ˜¯æˆªæ–­çš„
            if last_char not in '.!?ã€‚ï¼ï¼Ÿ' and not last_char.isalnum():
                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŒ‰å¥å­åˆ†å‰²ï¼Œä¿ç•™åˆ†éš”ç¬¦
                sentences = re.split(r'([.!?ã€‚ï¼ï¼Ÿ])', response_text)
                
                # å¦‚æœæœ‰å¤šä¸ªå¥å­ç‰‡æ®µï¼Œå°è¯•é‡ç»„åˆ°æœ€åä¸€ä¸ªå®Œæ•´å¥å­
                if len(sentences) > 2:
                    # é‡æ–°ç»„åˆå®Œæ•´çš„å¥å­ï¼ˆæ¯ä¸¤ä¸ªå…ƒç´ ç»„æˆä¸€ä¸ªå®Œæ•´å¥å­ï¼‰
                    complete_text = ''
                    for i in range(0, len(sentences) - 2, 2):  # æ­¥é•¿ä¸º2ï¼Œè·³è¿‡æœ€åå¯èƒ½ä¸å®Œæ•´çš„å¥å­
                        if i + 1 < len(sentences):
                            complete_text += sentences[i] + sentences[i + 1]  # å¥å­å†…å®¹ + æ ‡ç‚¹ç¬¦å·
                    
                    # å¦‚æœé‡ç»„åæœ‰å†…å®¹ï¼Œä½¿ç”¨é‡ç»„çš„æ–‡æœ¬
                    if complete_text:
                        response_text = complete_text
        
    except Exception as e:
        # ========== å¼‚å¸¸å¤„ç†ï¼šå¤‡ç”¨è§£ç æ–¹æ³• ==========
        print(f"âš ï¸ æ–‡æœ¬è§£ç å‡ºç°é—®é¢˜: {e}")
        
        # ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•ï¼šè§£ç å®Œæ•´è¾“å‡ºç„¶åæˆªå–
        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)
        response_text = full_response[len(text):].strip()  # ç§»é™¤è¾“å…¥éƒ¨åˆ†ï¼Œä¿ç•™ç”Ÿæˆéƒ¨åˆ†
        
        # ä»ç„¶ä½¿ç”¨æ¸…ç†å‡½æ•°å¤„ç†å¯èƒ½çš„ä¹±ç 
        response_text = clean_text_output(response_text)
    
    # ========== è¿”å›ç»“æœå­—å…¸ ==========
    return {
        'response': response_text,  # æ¸…ç†åçš„ç”Ÿæˆæ–‡æœ¬
        'stats': {                  # è¯¦ç»†çš„æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯
            'generation_time': generation_time,                           # ç”Ÿæˆè€—æ—¶ï¼ˆç§’ï¼‰
            'generated_tokens': generated_tokens,                        # ç”Ÿæˆçš„tokenæ•°é‡
            'input_length': input_length,                               # è¾“å…¥é•¿åº¦
            'total_length': total_length,                               # æ€»è¾“å‡ºé•¿åº¦
            'tokens_per_second': tokens_per_second,                     # ç”Ÿæˆé€Ÿåº¦ï¼ˆtokens/ç§’ï¼‰
            'ms_per_token': (generation_time * 1000) / generated_tokens if generated_tokens > 0 else 0  # æ¯tokenè€—æ—¶ï¼ˆæ¯«ç§’ï¼‰
        }
    }



def clean_text_output(text: str) -> str:
    """æ¸…ç†æ–‡æœ¬è¾“å‡ºï¼Œç§»é™¤ä¹±ç å’Œç‰¹æ®Šå­—ç¬¦"""
    if not text:
        return text
    
    # ç§»é™¤æ›¿æ¢å­—ç¬¦ï¼ˆé€šå¸¸è¡¨ç¤ºä¹±ç ï¼‰
    text = text.replace('\ufffd', '')  # Unicode æ›¿æ¢å­—ç¬¦
    text = text.replace('ï¿½', '')       # å¸¸è§çš„ä¹±ç å­—ç¬¦
    
    # ç§»é™¤å…¶ä»–æ§åˆ¶å­—ç¬¦ï¼ˆä¿ç•™å¸¸ç”¨çš„æ¢è¡Œã€åˆ¶è¡¨ç¬¦ï¼‰
    text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F-\x84\x86-\x9F]', '', text)
    
    # ç§»é™¤å¯èƒ½çš„ä¸å®Œæ•´çš„å¤šå­—èŠ‚å­—ç¬¦
    try:
        text = text.encode('utf-8', errors='ignore').decode('utf-8')
    except:
        pass
    
    # æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦
    text = re.sub(r'\s+', ' ', text)
    text = text.strip()
    
    return text


def display_gpu_info():
    """æ˜¾ç¤ºGPUé…ç½®ä¿¡æ¯"""
    print("ğŸ”§ GPU é…ç½®ä¿¡æ¯:")
    print(f"CUDA å¯ç”¨: {torch.cuda.is_available()}")
    
    if not torch.cuda.is_available():
        print("âš ï¸  æœªæ£€æµ‹åˆ°å¯ç”¨çš„ GPU")
        return
    
    print(f"æ£€æµ‹åˆ° GPU æ•°é‡: {torch.cuda.device_count()}")
    print(f"å½“å‰è®¾å¤‡: {torch.cuda.current_device()}")
    
    for i in range(torch.cuda.device_count()):
        gpu_name = torch.cuda.get_device_name(i)
        gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
        print(f"  GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)")
    
    cuda_devices = os.environ.get('CUDA_VISIBLE_DEVICES', 'æœªè®¾ç½®ï¼ˆå°†ä½¿ç”¨æ‰€æœ‰GPUï¼‰')
    print(f"CUDA_VISIBLE_DEVICES: {cuda_devices}")


def load_model(model_path: str):
    """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
    print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
    
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )
    
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
    return model, tokenizer


def display_model_info(model):
    """æ˜¾ç¤ºæ¨¡å‹åˆ†å¸ƒå’Œæ˜¾å­˜ä½¿ç”¨ä¿¡æ¯"""
    print("\nğŸ“Š æ¨¡å‹è®¾å¤‡åˆ†å¸ƒ:")
    
    try:
        if hasattr(model, 'hf_device_map') and model.hf_device_map:
            print("è®¾å¤‡æ˜ å°„:")
            device_map = model.hf_device_map
            if isinstance(device_map, dict):
                for layer, device in device_map.items():
                    print(f"  {layer}: {device}")
            else:
                print(f"  è®¾å¤‡æ˜ å°„ç±»å‹: {type(device_map)}")
        else:
            print(f"æ¨¡å‹ä¸»è®¾å¤‡: {next(model.parameters()).device}")
        
        # æ£€æŸ¥æ¨¡å‹å‚æ•°åˆ†å¸ƒ
        device_count = {}
        for name, param in model.named_parameters():
            device = str(param.device)
            device_count[device] = device_count.get(device, 0) + 1
        
        print("å‚æ•°åˆ†å¸ƒç»Ÿè®¡:")
        for device, count in device_count.items():
            print(f"  {device}: {count} ä¸ªå‚æ•°å¼ é‡")
            
    except Exception as e:
        print(f"æ— æ³•è·å–è®¾å¤‡æ˜ å°„ä¿¡æ¯: {e}")
        print(f"æ¨¡å‹è®¾å¤‡: {next(model.parameters()).device}")
    
    # æ˜¾ç¤ºæ¯ä¸ªGPUçš„æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
    print("\nğŸ’¾ GPU æ˜¾å­˜ä½¿ç”¨æƒ…å†µ:")
    for i in range(torch.cuda.device_count()):
        allocated = torch.cuda.memory_allocated(i) / 1024**3
        reserved = torch.cuda.memory_reserved(i) / 1024**3
        total = torch.cuda.get_device_properties(i).total_memory / 1024**3
        print(f"  GPU {i}: å·²åˆ†é… {allocated:.2f}GB / é¢„ç•™ {reserved:.2f}GB / æ€»è®¡ {total:.1f}GB")

def display_inference_stats(stats: Dict, response_text: str = ""):
    """æ˜¾ç¤ºæ¨ç†ç»Ÿè®¡ä¿¡æ¯"""
    print(f"\nğŸ“ˆ æ¨ç†æ€§èƒ½ç»Ÿè®¡:")
    print(f"  â±ï¸  æ€»ç”Ÿæˆæ—¶é—´: {stats['generation_time']:.3f} ç§’")
    print(f"  ğŸ”¢ ç”Ÿæˆtokenæ•°: {stats['generated_tokens']} ä¸ª")
    print(f"  ğŸš€ æ¨ç†é€Ÿåº¦: {stats['tokens_per_second']:.2f} tokens/ç§’")
    print(f"  âš¡ æ¯tokenç”¨æ—¶: {stats['ms_per_token']:.1f} æ¯«ç§’")
    print(f"  ğŸ“¥ è¾“å…¥é•¿åº¦: {stats['input_length']} tokens")
    print(f"  ğŸ“¤ æ€»è¾“å‡ºé•¿åº¦: {stats['total_length']} tokens")
    print(f"  ğŸ“Š è¾“å‡º/è¾“å…¥æ¯”: {stats['generated_tokens']/stats['input_length']:.2f}x")
    
    # æ–‡æœ¬è´¨é‡æ£€æŸ¥
    if response_text:
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¹±ç å­—ç¬¦
        has_garbled = 'ï¿½' in response_text or '\ufffd' in response_text
        # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸çš„å­—ç¬¦æ¯”ä¾‹
        printable_chars = sum(1 for c in response_text if c.isprintable() or c in '\n\t')
        total_chars = len(response_text)
        printable_ratio = printable_chars / total_chars if total_chars > 0 else 1.0
        
        if has_garbled:
            print(f"  âš ï¸  æ–‡æœ¬è´¨é‡: æ£€æµ‹åˆ°ä¹±ç å­—ç¬¦")
        elif printable_ratio < 0.95:
            print(f"  âš ï¸  æ–‡æœ¬è´¨é‡: å¯æ‰“å°å­—ç¬¦æ¯”ä¾‹ {printable_ratio:.1%}")
        else:
            print(f"  âœ… æ–‡æœ¬è´¨é‡: æ­£å¸¸")
    
    # æ€§èƒ½ç­‰çº§è¯„ä¼°
    speed = stats['tokens_per_second']
    if speed >= 50:
        performance_level = "ğŸ”¥ æå¿«"
    elif speed >= 30:
        performance_level = "ğŸš€ å¾ˆå¿«"
    elif speed >= 15:
        performance_level = "âœ… è‰¯å¥½"
    elif speed >= 8:
        performance_level = "âš¡ ä¸€èˆ¬"
    else:
        performance_level = "ğŸŒ è¾ƒæ…¢"
    
    print(f"  ğŸ¯ æ€§èƒ½ç­‰çº§: {performance_level}")


def run_benchmark(model, tokenizer, test_prompts: Optional[List[str]] = None, num_runs: int = 3) -> List[float]:
    """æ‰§è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•"""
    if test_prompts is None:
        test_prompts = [
            "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±",
            "è¯·å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„çŸ­è¯—",
            "è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½",
            "ç”¨Pythonå†™ä¸€ä¸ªç®€å•çš„æ’åºç®—æ³•",
            "è¯·è¯¦ç»†è§£é‡Šä¸€ä¸‹å¤§è¯­è¨€æ¨¡å‹çš„å·¥ä½œåŸç†"
        ]
    
    print(f"\nğŸ§ª å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯• (å…± {len(test_prompts)} ä¸ªé—®é¢˜ï¼Œæ¯ä¸ªè¿è¡Œ {num_runs} æ¬¡)")
    print("=" * 60)
    
    all_speeds = []
    
    for i, prompt in enumerate(test_prompts, 1):
        print(f"\nğŸ“ æµ‹è¯•é—®é¢˜ {i}/{len(test_prompts)}: {prompt[:50]}...")
        speeds_for_prompt = []
        
        for run in range(num_runs):
            print(f"\n  ğŸ”„ ç¬¬ {run+1}/{num_runs} æ¬¡è¿è¡Œ:")
            
            result = generate_response(model, tokenizer, prompt, max_tokens=200)
            speed = result['stats']['tokens_per_second']
            
            speeds_for_prompt.append(speed)
            all_speeds.append(speed)
            
            print(f"    âš¡ é€Ÿåº¦: {speed:.2f} tokens/ç§’ ({result['stats']['generated_tokens']} tokens in {result['stats']['generation_time']:.2f}s)")
        
        avg_speed = sum(speeds_for_prompt) / len(speeds_for_prompt)
        print(f"  ğŸ“Š å¹³å‡é€Ÿåº¦: {avg_speed:.2f} tokens/ç§’")
    
    # æ€»ä½“ç»Ÿè®¡
    print(f"\nğŸ† æ€»ä½“æ€§èƒ½ç»Ÿè®¡:")
    print(f"  ğŸ“ˆ å¹³å‡æ¨ç†é€Ÿåº¦: {sum(all_speeds)/len(all_speeds):.2f} tokens/ç§’")
    print(f"  âš¡ æœ€å¿«é€Ÿåº¦: {max(all_speeds):.2f} tokens/ç§’")
    print(f"  ğŸŒ æœ€æ…¢é€Ÿåº¦: {min(all_speeds):.2f} tokens/ç§’")
    
    if len(all_speeds) > 1:
        variance = sum([(x - sum(all_speeds)/len(all_speeds))**2 for x in all_speeds]) / len(all_speeds)
        std_dev = variance ** 0.5
        print(f"  ğŸ“Š é€Ÿåº¦æ ‡å‡†å·®: {std_dev:.2f}")
    
    return all_speeds


def main():
    """ä¸»å‡½æ•°"""
    # æ¨¡å‹è·¯å¾„é€‰æ‹© - æš‚æ—¶ä½¿ç”¨30Bæ¨¡å‹æµ‹è¯•
    # model_path = "/home/work/hd/_models/base/Qwen3-4B-Thinking-2507-FP8"
    model_path = "/home/work/hd/_models/base/Qwen3-30B-A3B-Thinking-2507"
    
    # 1. æ˜¾ç¤ºGPUä¿¡æ¯
    display_gpu_info()
    print("\n" + "="*50)
    
    # 2. åŠ è½½æ¨¡å‹
    model, tokenizer = load_model(model_path)
    
    # 3. æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
    display_model_info(model)
    
    # 4. æ‰§è¡Œæ¨ç†
    prompt = "è¯·è¯¦ç»†è§£é‡Šä¸€ä¸‹å¤§è¯­è¨€æ¨¡å‹çš„æ€è€ƒæ¨¡å¼ï¼ˆThinking modeï¼‰æ˜¯ä»€ä¹ˆï¼Ÿ"
    print(f"\nç”¨æˆ·é—®é¢˜: {prompt}")
    print("=" * 50)
    print("AIå›å¤:")
    
    result = generate_response(model, tokenizer, prompt)
    print(result['response'])
    
    # 5. æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯ï¼ˆåŒ…å«æ–‡æœ¬è´¨é‡æ£€æŸ¥ï¼‰
    display_inference_stats(result['stats'], result['response'])
    
    # 6. å¯é€‰ï¼šè¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
    print(f"\nâœ… æ¨ç†å®Œæˆï¼")
    print(f"ğŸ’¡ å¦‚éœ€è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•ï¼Œè¯·è°ƒç”¨: run_benchmark(model, tokenizer)")


if __name__ == "__main__":
    main()
