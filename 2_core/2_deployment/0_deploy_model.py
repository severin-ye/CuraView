#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ ¸å¿ƒéƒ¨ç½²æ¨¡å— - åŸºäºMS-Swiftçš„æ¨¡å‹éƒ¨ç½²å™¨
æ”¯æŒLoRAã€å…¨å‚æ•°æ¨¡å‹çš„æœ¬åœ°å’Œäº‘ç«¯éƒ¨ç½²
"""

import os
import sys
import json
import subprocess
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Union

# æ·»åŠ utilsè·¯å¾„
sys.path.append(str(Path(__file__).parent.parent.parent / "1_utils"))

from config_loader import ConfigLoader
from logger import Logger

class DeploymentManager:
    """éƒ¨ç½²ç®¡ç†å™¨ - ç»Ÿä¸€çš„æ¨¡å‹éƒ¨ç½²æ¥å£"""
    
    def __init__(self, config_path: Optional[str] = None):
        """
        åˆå§‹åŒ–éƒ¨ç½²ç®¡ç†å™¨
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.logger = Logger("DeploymentManager").get_logger()
        self.config_loader = ConfigLoader()
        
        # åŠ è½½é…ç½®
        if config_path:
            self.config = self.config_loader.load_config(config_path)
        else:
            self.config = {}
        
        self.process = None
        self.deployment_info = {}
    
    def detect_model_type(self, checkpoint_path: str) -> str:
        """
        æ£€æµ‹æ¨¡å‹ç±»å‹
        Args:
            checkpoint_path: æ£€æŸ¥ç‚¹è·¯å¾„
        Returns:
            str: æ¨¡å‹ç±»å‹ ('lora' æˆ– 'full')
        """
        path = Path(checkpoint_path)
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºLoRA checkpoint
        adapter_files = [
            "adapter_config.json", 
            "adapter_model.safetensors", 
            "adapter_model.bin",
            "adapter_model.pt"
        ]
        
        has_adapter = any((path / f).exists() for f in adapter_files)
        return "lora" if has_adapter else "full"
    
    def build_deployment_command(self, 
                                checkpoint_path: str,
                                model_type: str,
                                deploy_config: Dict[str, Any]) -> List[str]:
        """
        æ„å»ºéƒ¨ç½²å‘½ä»¤
        Args:
            checkpoint_path: æ£€æŸ¥ç‚¹è·¯å¾„
            model_type: æ¨¡å‹ç±»å‹
            deploy_config: éƒ¨ç½²é…ç½®
        Returns:
            List[str]: éƒ¨ç½²å‘½ä»¤åˆ—è¡¨
        """
        # åŸºç¡€å‘½ä»¤
        cmd = ["swift", "deploy"]
        
        # æ ¹æ®æ¨¡å‹ç±»å‹è®¾ç½®å‚æ•°
        if model_type == "lora":
            cmd.extend(["--adapters", checkpoint_path])
        else:
            cmd.extend(["--model", checkpoint_path])
        
        # ç½‘ç»œé…ç½®
        port = deploy_config.get('port', 8000)
        host = deploy_config.get('host', '0.0.0.0')
        cmd.extend(["--port", str(port)])
        cmd.extend(["--host", host])
        
        # æ¨ç†åç«¯
        infer_backend = deploy_config.get('infer_backend', 'pt')
        cmd.extend(["--infer_backend", infer_backend])
        
        # ç”Ÿæˆå‚æ•°
        if 'temperature' in deploy_config:
            cmd.extend(["--temperature", str(deploy_config['temperature'])])
        
        if 'max_new_tokens' in deploy_config:
            cmd.extend(["--max_new_tokens", str(deploy_config['max_new_tokens'])])
        
        if 'top_p' in deploy_config:
            cmd.extend(["--top_p", str(deploy_config['top_p'])])
        
        # æœåŠ¡æ¨¡å‹åç§°
        if 'served_model_name' in deploy_config:
            cmd.extend(["--served_model_name", deploy_config['served_model_name']])
        
        # GPUè®¾ç½®
        if 'gpu_ids' in deploy_config:
            gpu_ids = deploy_config['gpu_ids']
            if isinstance(gpu_ids, list):
                gpu_str = ",".join(map(str, gpu_ids))
            else:
                gpu_str = str(gpu_ids)
            
            # è®¾ç½®ç¯å¢ƒå˜é‡
            os.environ['CUDA_VISIBLE_DEVICES'] = gpu_str
        
        # vLLMç‰¹å®šé…ç½®
        if infer_backend == 'vllm':
            if 'tensor_parallel_size' in deploy_config:
                cmd.extend(["--tensor_parallel_size", str(deploy_config['tensor_parallel_size'])])
            if 'max_model_len' in deploy_config:
                cmd.extend(["--max_model_len", str(deploy_config['max_model_len'])])
        
        # DeepSpeedé…ç½®
        if 'deepspeed' in deploy_config:
            cmd.extend(["--deepspeed", deploy_config['deepspeed']])
        
        return cmd
    
    def deploy_single_model(self, 
                          checkpoint_path: str, 
                          deploy_config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        éƒ¨ç½²å•ä¸ªæ¨¡å‹
        Args:
            checkpoint_path: æ£€æŸ¥ç‚¹è·¯å¾„
            deploy_config: éƒ¨ç½²é…ç½®
        Returns:
            Dict[str, Any]: éƒ¨ç½²ä¿¡æ¯
        """
        if deploy_config is None:
            deploy_config = self.config.get('deployment', {})
        
        # ç¡®ä¿deploy_configä¸ä¸ºNone
        if not deploy_config:
            deploy_config = {}
        
        # æ£€æµ‹æ¨¡å‹ç±»å‹
        model_type = self.detect_model_type(checkpoint_path)
        self.logger.info(f"ğŸ” æ£€æµ‹åˆ°æ¨¡å‹ç±»å‹: {model_type.upper()}")
        
        # æ„å»ºéƒ¨ç½²å‘½ä»¤
        cmd = self.build_deployment_command(checkpoint_path, model_type, deploy_config)
        
        # è®°å½•éƒ¨ç½²ä¿¡æ¯
        self.deployment_info = {
            'checkpoint_path': checkpoint_path,
            'model_type': model_type,
            'deploy_config': deploy_config,
            'command': ' '.join(cmd),
            'port': deploy_config.get('port', 8000),
            'host': deploy_config.get('host', '0.0.0.0'),
            'status': 'deploying'
        }
        
        self.logger.info(f"ğŸš€ å¼€å§‹éƒ¨ç½²{model_type.upper()}æ¨¡å‹")
        self.logger.info(f"ğŸ“ æ£€æŸ¥ç‚¹è·¯å¾„: {checkpoint_path}")
        self.logger.info(f"ğŸŒ æœåŠ¡åœ°å€: http://{deploy_config.get('host', '0.0.0.0')}:{deploy_config.get('port', 8000)}")
        self.logger.info(f"âš™ï¸  æ¨ç†åç«¯: {deploy_config.get('infer_backend', 'pt')}")
        
        try:
            # å¯åŠ¨éƒ¨ç½²è¿›ç¨‹
            self.logger.info(f"ğŸ’» æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
            
            self.process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                bufsize=1,
                universal_newlines=True
            )
            
            self.deployment_info['status'] = 'running'
            self.deployment_info['pid'] = self.process.pid
            
            self.logger.info(f"âœ… éƒ¨ç½²å¯åŠ¨æˆåŠŸï¼Œè¿›ç¨‹ID: {self.process.pid}")
            
            return self.deployment_info
            
        except Exception as e:
            self.deployment_info['status'] = 'failed'
            self.deployment_info['error'] = str(e)
            self.logger.error(f"âŒ éƒ¨ç½²å¤±è´¥: {str(e)}")
            raise e
    
    def deploy_multi_lora(self, 
                         lora_configs: Dict[str, str], 
                         deploy_config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        éƒ¨ç½²å¤šLoRAæ¨¡å‹
        Args:
            lora_configs: LoRAé…ç½®å­—å…¸ {name: path}
            deploy_config: éƒ¨ç½²é…ç½®
        Returns:
            Dict[str, Any]: éƒ¨ç½²ä¿¡æ¯
        """
        if deploy_config is None:
            deploy_config = self.config.get('deployment', {})
        
        # ç¡®ä¿deploy_configä¸ä¸ºNone
        if not deploy_config:
            deploy_config = {}
        
        # æ„å»ºadapterså­—ç¬¦ä¸²
        adapters_str = " ".join([f"{name}={path}" for name, path in lora_configs.items()])
        
        # åŸºç¡€å‘½ä»¤
        cmd = [
            "swift", "deploy",
            "--adapters", adapters_str,
            "--port", str(deploy_config.get('port', 8000)),
            "--host", deploy_config.get('host', '0.0.0.0')
        ]
        
        # æ·»åŠ å…¶ä»–é…ç½®
        if 'infer_backend' in deploy_config:
            cmd.extend(["--infer_backend", deploy_config['infer_backend']])
        if 'temperature' in deploy_config:
            cmd.extend(["--temperature", str(deploy_config['temperature'])])
        if 'max_new_tokens' in deploy_config:
            cmd.extend(["--max_new_tokens", str(deploy_config['max_new_tokens'])])
        
        self.logger.info(f"ğŸ”„ å¼€å§‹éƒ¨ç½²å¤šLoRAæ¨¡å‹: {list(lora_configs.keys())}")
        
        try:
            self.process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            
            deployment_info = {
                'model_type': 'multi_lora',
                'lora_configs': lora_configs,
                'deploy_config': deploy_config,
                'command': ' '.join(cmd),
                'port': deploy_config.get('port', 8000),
                'host': deploy_config.get('host', '0.0.0.0'),
                'status': 'running',
                'pid': self.process.pid
            }
            
            self.logger.info("âœ… å¤šLoRAæ¨¡å‹éƒ¨ç½²å¯åŠ¨æˆåŠŸ")
            return deployment_info
            
        except Exception as e:
            self.logger.error(f"âŒ å¤šLoRAéƒ¨ç½²å¤±è´¥: {str(e)}")
            raise e
    
    def stop_deployment(self):
        """åœæ­¢éƒ¨ç½²"""
        if self.process and self.process.poll() is None:
            self.logger.info("ğŸ›‘ æ­£åœ¨åœæ­¢éƒ¨ç½²...")
            self.process.terminate()
            
            try:
                self.process.wait(timeout=10)
                self.logger.info("âœ… éƒ¨ç½²å·²åœæ­¢")
            except subprocess.TimeoutExpired:
                self.logger.warning("âš ï¸  å¼ºåˆ¶ç»ˆæ­¢éƒ¨ç½²è¿›ç¨‹")
                self.process.kill()
                self.process.wait()
            
            if 'status' in self.deployment_info:
                self.deployment_info['status'] = 'stopped'
        else:
            self.logger.info("â„¹ï¸  æ²¡æœ‰è¿è¡Œä¸­çš„éƒ¨ç½²")
    
    def get_deployment_status(self) -> Dict[str, Any]:
        """è·å–éƒ¨ç½²çŠ¶æ€"""
        if not self.deployment_info:
            return {'status': 'not_deployed'}
        
        # æ£€æŸ¥è¿›ç¨‹çŠ¶æ€
        if self.process:
            if self.process.poll() is None:
                self.deployment_info['status'] = 'running'
            else:
                self.deployment_info['status'] = 'stopped'
                self.deployment_info['return_code'] = self.process.returncode
        
        return self.deployment_info.copy()
    
    def get_deployment_logs(self, lines: int = 50) -> Dict[str, List[str]]:
        """
        è·å–éƒ¨ç½²æ—¥å¿—
        Args:
            lines: è·å–çš„æ—¥å¿—è¡Œæ•°
        Returns:
            Dict[str, List[str]]: åŒ…å«stdoutå’Œstderrçš„æ—¥å¿—
        """
        if not self.process:
            return {'stdout': [], 'stderr': []}
        
        try:
            # è·å–stdout
            stdout_lines = []
            if self.process.stdout:
                # æ³¨æ„ï¼šè¿™é‡Œåªæ˜¯ç¤ºä¾‹ï¼Œå®é™…å®ç°å¯èƒ½éœ€è¦æ›´å¤æ‚çš„æ—¥å¿—ç®¡ç†
                pass
            
            # è·å–stderr
            stderr_lines = []
            if self.process.stderr:
                # æ³¨æ„ï¼šè¿™é‡Œåªæ˜¯ç¤ºä¾‹ï¼Œå®é™…å®ç°å¯èƒ½éœ€è¦æ›´å¤æ‚çš„æ—¥å¿—ç®¡ç†
                pass
            
            return {
                'stdout': stdout_lines[-lines:] if stdout_lines else [],
                'stderr': stderr_lines[-lines:] if stderr_lines else []
            }
        except Exception as e:
            self.logger.error(f"âŒ è·å–æ—¥å¿—å¤±è´¥: {str(e)}")
            return {'stdout': [], 'stderr': []}

class DeploymentPresets:
    """éƒ¨ç½²é¢„è®¾é…ç½®"""
    
    @staticmethod
    def get_local_preset() -> Dict[str, Any]:
        """è·å–æœ¬åœ°éƒ¨ç½²é¢„è®¾"""
        return {
            'host': '127.0.0.1',
            'port': 8000,
            'infer_backend': 'pt',
            'temperature': 0.7,
            'max_new_tokens': 1024,
            'top_p': 0.9
        }
    
    @staticmethod
    def get_server_preset() -> Dict[str, Any]:
        """è·å–æœåŠ¡å™¨éƒ¨ç½²é¢„è®¾"""
        return {
            'host': '0.0.0.0',
            'port': 8000,
            'infer_backend': 'vllm',
            'temperature': 0.7,
            'max_new_tokens': 2048,
            'top_p': 0.9,
            'tensor_parallel_size': 1,
            'max_model_len': 4096
        }
    
    @staticmethod
    def get_production_preset() -> Dict[str, Any]:
        """è·å–ç”Ÿäº§ç¯å¢ƒé¢„è®¾"""
        return {
            'host': '0.0.0.0',
            'port': 8000,
            'infer_backend': 'vllm',
            'temperature': 0.7,
            'max_new_tokens': 1024,
            'top_p': 0.9,
            'tensor_parallel_size': 2,
            'max_model_len': 8192,
            'served_model_name': 'custom-model'
        }

def create_deployment_manager(config_path: Optional[str] = None) -> DeploymentManager:
    """
    åˆ›å»ºéƒ¨ç½²ç®¡ç†å™¨å®ä¾‹
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
    Returns:
        DeploymentManager: éƒ¨ç½²ç®¡ç†å™¨å®ä¾‹
    """
    return DeploymentManager(config_path)

if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    print("ğŸ§ª éƒ¨ç½²å™¨æµ‹è¯•...")
    
    # åˆ›å»ºéƒ¨ç½²ç®¡ç†å™¨
    deploy_manager = create_deployment_manager()
    
    # æ˜¾ç¤ºé¢„è®¾é…ç½®
    presets = {
        "æœ¬åœ°éƒ¨ç½²": DeploymentPresets.get_local_preset(),
        "æœåŠ¡å™¨éƒ¨ç½²": DeploymentPresets.get_server_preset(),
        "ç”Ÿäº§ç¯å¢ƒ": DeploymentPresets.get_production_preset(),
    }
    
    print("ğŸ”§ å¯ç”¨çš„éƒ¨ç½²é¢„è®¾:")
    for name, preset in presets.items():
        print(f"  {name}: {preset}")
    
    print("âœ… éƒ¨ç½²å™¨æ¨¡å—åŠ è½½æˆåŠŸ")