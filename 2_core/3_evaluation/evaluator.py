#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ ¸å¿ƒè¯„ä¼°æ¨¡å— - æ¨¡å‹æ€§èƒ½è¯„ä¼°å’ŒåŸºå‡†æµ‹è¯•
æ”¯æŒå¤šç§è¯„ä¼°æŒ‡æ ‡å’ŒåŸºå‡†æ•°æ®é›†
"""

import os
import sys
import json
import time
from pathlib import Path
from typing import List, Dict, Any, Optional, Union, Callable

# æ·»åŠ utilsè·¯å¾„
sys.path.append(str(Path(__file__).parent.parent.parent / "1_utils"))

from config_loader import ConfigLoader
from logger import Logger
from metrics import MetricsCalculator

class EvaluationManager:
    """è¯„ä¼°ç®¡ç†å™¨ - ç»Ÿä¸€çš„æ¨¡å‹è¯„ä¼°æ¥å£"""
    
    def __init__(self, config_path: Optional[str] = None):
        """
        åˆå§‹åŒ–è¯„ä¼°ç®¡ç†å™¨
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.logger = Logger("EvaluationManager").get_logger()
        self.config_loader = ConfigLoader()
        self.metrics_calculator = MetricsCalculator()
        
        # åŠ è½½é…ç½®
        if config_path:
            self.config = self.config_loader.load_config(config_path)
        else:
            self.config = {}
        
        self.evaluation_results = {}
    
    def evaluate_generation_quality(self, 
                                   references: List[str], 
                                   candidates: List[str],
                                   task_name: str = "generation") -> Dict[str, Any]:
        """
        è¯„ä¼°æ–‡æœ¬ç”Ÿæˆè´¨é‡
        Args:
            references: å‚è€ƒæ–‡æœ¬åˆ—è¡¨
            candidates: å€™é€‰æ–‡æœ¬åˆ—è¡¨
            task_name: ä»»åŠ¡åç§°
        Returns:
            Dict[str, Any]: è¯„ä¼°ç»“æœ
        """
        self.logger.info(f"ğŸ“Š å¼€å§‹è¯„ä¼°{task_name}ä»»åŠ¡ï¼Œå…±{len(references)}ä¸ªæ ·æœ¬")
        
        start_time = time.time()
        
        # è®¡ç®—å„ç§æŒ‡æ ‡
        results = {
            'task_name': task_name,
            'sample_count': len(references),
            'timestamp': time.time(),
            'metrics': {}
        }
        
        try:
            # BLEUåˆ†æ•°
            bleu_scores = []
            for ref, cand in zip(references, candidates):
                bleu = self.metrics_calculator.calculate_bleu([ref], cand)
                bleu_scores.append(bleu)
            
            avg_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0
            results['metrics']['bleu'] = {
                'average': avg_bleu,
                'scores': bleu_scores
            }
            
            # ROUGEåˆ†æ•°
            rouge_scores = []
            for ref, cand in zip(references, candidates):
                rouge = self.metrics_calculator.calculate_rouge(ref, cand)
                rouge_scores.append(rouge)
            
            # è®¡ç®—å¹³å‡ROUGEåˆ†æ•°
            avg_rouge = {
                'rouge-1': sum(r['rouge-1']['f'] for r in rouge_scores) / len(rouge_scores),
                'rouge-2': sum(r['rouge-2']['f'] for r in rouge_scores) / len(rouge_scores),
                'rouge-l': sum(r['rouge-l']['f'] for r in rouge_scores) / len(rouge_scores)
            }
            results['metrics']['rouge'] = {
                'average': avg_rouge,
                'scores': rouge_scores
            }
            
            # ç²¾ç¡®åŒ¹é…
            exact_matches = [
                self.metrics_calculator.calculate_exact_match(ref, cand)
                for ref, cand in zip(references, candidates)
            ]
            exact_match_rate = sum(exact_matches) / len(exact_matches)
            results['metrics']['exact_match'] = {
                'rate': exact_match_rate,
                'matches': exact_matches
            }
            
            # F1åˆ†æ•°
            f1_scores = [
                self.metrics_calculator.calculate_f1_score(ref, cand)
                for ref, cand in zip(references, candidates)
            ]
            avg_f1 = sum(f1_scores) / len(f1_scores)
            results['metrics']['f1'] = {
                'average': avg_f1,
                'scores': f1_scores
            }
            
            # è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            try:
                semantic_scores = [
                    self.metrics_calculator.calculate_semantic_similarity(ref, cand)
                    for ref, cand in zip(references, candidates)
                ]
                avg_semantic = sum(semantic_scores) / len(semantic_scores)
                results['metrics']['semantic_similarity'] = {
                    'average': avg_semantic,
                    'scores': semantic_scores
                }
            except Exception as e:
                self.logger.warning(f"âš ï¸  è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
                results['metrics']['semantic_similarity'] = None
            
            # è®¡ç®—æ€»ç”¨æ—¶
            end_time = time.time()
            results['evaluation_time'] = end_time - start_time
            
            self.logger.info(f"âœ… è¯„ä¼°å®Œæˆï¼Œç”¨æ—¶{results['evaluation_time']:.2f}ç§’")
            self.logger.info(f"ğŸ“ˆ BLEU: {avg_bleu:.4f}, ROUGE-L: {avg_rouge['rouge-l']:.4f}, F1: {avg_f1:.4f}")
            
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
            raise e
    
    def evaluate_model_performance(self,
                                 model_inference_func: Callable[[str], str],
                                 test_dataset: List[Dict[str, str]],
                                 task_type: str = "qa") -> Dict[str, Any]:
        """
        è¯„ä¼°æ¨¡å‹æ•´ä½“æ€§èƒ½
        Args:
            model_inference_func: æ¨¡å‹æ¨ç†å‡½æ•°
            test_dataset: æµ‹è¯•æ•°æ®é›† [{"input": "...", "output": "..."}]
            task_type: ä»»åŠ¡ç±»å‹
        Returns:
            Dict[str, Any]: è¯„ä¼°ç»“æœ
        """
        self.logger.info(f"ğŸ¯ å¼€å§‹è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œä»»åŠ¡ç±»å‹: {task_type}")
        self.logger.info(f"ğŸ“‹ æµ‹è¯•æ ·æœ¬æ•°: {len(test_dataset)}")
        
        start_time = time.time()
        
        # æ‰§è¡Œæ¨ç†
        predictions = []
        references = []
        inference_times = []
        
        for i, sample in enumerate(test_dataset):
            try:
                # è®°å½•æ¨ç†æ—¶é—´
                infer_start = time.time()
                prediction = model_inference_func(sample['input'])
                infer_end = time.time()
                
                predictions.append(prediction)
                references.append(sample['output'])
                inference_times.append(infer_end - infer_start)
                
                if (i + 1) % 10 == 0:
                    self.logger.info(f"ğŸ“Š å·²å¤„ç† {i + 1}/{len(test_dataset)} ä¸ªæ ·æœ¬")
                
            except Exception as e:
                self.logger.error(f"âŒ æ ·æœ¬ {i+1} æ¨ç†å¤±è´¥: {str(e)}")
                predictions.append("")
                references.append(sample['output'])
                inference_times.append(0)
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        evaluation_results = self.evaluate_generation_quality(
            references, predictions, f"{task_type}_evaluation"
        )
        
        # æ·»åŠ æ€§èƒ½æŒ‡æ ‡
        evaluation_results['performance'] = {
            'total_inference_time': sum(inference_times),
            'average_inference_time': sum(inference_times) / len(inference_times),
            'samples_per_second': len(test_dataset) / sum(inference_times) if sum(inference_times) > 0 else 0,
            'failed_samples': sum(1 for p in predictions if not p.strip())
        }
        
        total_time = time.time() - start_time
        evaluation_results['total_evaluation_time'] = total_time
        
        self.logger.info(f"ğŸ æ¨¡å‹è¯„ä¼°å®Œæˆï¼Œæ€»ç”¨æ—¶{total_time:.2f}ç§’")
        self.logger.info(f"âš¡ å¹³å‡æ¨ç†æ—¶é—´: {evaluation_results['performance']['average_inference_time']:.3f}ç§’/æ ·æœ¬")
        
        return evaluation_results
    
    def benchmark_on_dataset(self,
                           model_inference_func: Callable[[str], str],
                           dataset_name: str,
                           dataset_path: Optional[str] = None,
                           sample_size: Optional[int] = None) -> Dict[str, Any]:
        """
        åœ¨åŸºå‡†æ•°æ®é›†ä¸Šè¯„ä¼°
        Args:
            model_inference_func: æ¨¡å‹æ¨ç†å‡½æ•°
            dataset_name: æ•°æ®é›†åç§°
            dataset_path: æ•°æ®é›†è·¯å¾„
            sample_size: é‡‡æ ·å¤§å°
        Returns:
            Dict[str, Any]: åŸºå‡†æµ‹è¯•ç»“æœ
        """
        self.logger.info(f"ğŸ¯ å¼€å§‹åŸºå‡†æµ‹è¯•: {dataset_name}")
        
        # åŠ è½½æ•°æ®é›†
        test_data = self.load_benchmark_dataset(dataset_name, dataset_path, sample_size)
        
        if not test_data:
            raise ValueError(f"âŒ æ— æ³•åŠ è½½æ•°æ®é›†: {dataset_name}")
        
        # æ‰§è¡Œè¯„ä¼°
        results = self.evaluate_model_performance(
            model_inference_func, test_data, dataset_name
        )
        
        results['dataset_name'] = dataset_name
        results['dataset_size'] = len(test_data)
        
        # ä¿å­˜ç»“æœ
        self.save_evaluation_results(results, f"benchmark_{dataset_name}")
        
        return results
    
    def load_benchmark_dataset(self,
                             dataset_name: str,
                             dataset_path: Optional[str] = None,
                             sample_size: Optional[int] = None) -> List[Dict[str, str]]:
        """
        åŠ è½½åŸºå‡†æ•°æ®é›†
        Args:
            dataset_name: æ•°æ®é›†åç§°
            dataset_path: æ•°æ®é›†è·¯å¾„
            sample_size: é‡‡æ ·å¤§å°
        Returns:
            List[Dict[str, str]]: æ•°æ®é›†
        """
        try:
            if dataset_path and Path(dataset_path).exists():
                # ä»æ–‡ä»¶åŠ è½½
                with open(dataset_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
            else:
                # ä½¿ç”¨å†…ç½®æµ‹è¯•æ•°æ®
                data = self.get_builtin_test_data(dataset_name)
            
            # é‡‡æ ·
            if sample_size and len(data) > sample_size:
                import random
                data = random.sample(data, sample_size)
                self.logger.info(f"ğŸ“Š éšæœºé‡‡æ · {sample_size} ä¸ªæ ·æœ¬")
            
            return data
            
        except Exception as e:
            self.logger.error(f"âŒ åŠ è½½æ•°æ®é›†å¤±è´¥: {str(e)}")
            return []
    
    def get_builtin_test_data(self, dataset_name: str) -> List[Dict[str, str]]:
        """è·å–å†…ç½®æµ‹è¯•æ•°æ®"""
        if dataset_name == "qa_test":
            return [
                {"input": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ", "output": "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œæ—¨åœ¨åˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚"},
                {"input": "Pythonæ˜¯ä»€ä¹ˆï¼Ÿ", "output": "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œä»¥å…¶ç®€æ´çš„è¯­æ³•å’Œå¼ºå¤§çš„åŠŸèƒ½è€Œé—»åã€‚"},
                {"input": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ", "output": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œé€šè¿‡ç®—æ³•è®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼ã€‚"},
                {"input": "å¦‚ä½•å­¦ä¹ ç¼–ç¨‹ï¼Ÿ", "output": "å­¦ä¹ ç¼–ç¨‹éœ€è¦é€‰æ‹©ä¸€é—¨è¯­è¨€ï¼Œç†è§£åŸºç¡€æ¦‚å¿µï¼Œå¤šå®è·µé¡¹ç›®ï¼Œå¹¶æŒç»­å­¦ä¹ æ–°æŠ€æœ¯ã€‚"},
                {"input": "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ", "output": "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å¤„ç†å¤æ‚çš„æ•°æ®æ¨¡å¼ã€‚"}
            ]
        elif dataset_name == "math_test":
            return [
                {"input": "2+3ç­‰äºå¤šå°‘ï¼Ÿ", "output": "5"},
                {"input": "10çš„å¹³æ–¹æ ¹æ˜¯å¤šå°‘ï¼Ÿ", "output": "3.16"},
                {"input": "9Ã—7ç­‰äºå¤šå°‘ï¼Ÿ", "output": "63"},
                {"input": "100é™¤ä»¥4ç­‰äºå¤šå°‘ï¼Ÿ", "output": "25"},
                {"input": "åœ†å‘¨ç‡çš„è¿‘ä¼¼å€¼æ˜¯å¤šå°‘ï¼Ÿ", "output": "3.14159"}
            ]
        else:
            return [
                {"input": "ä½ å¥½", "output": "ä½ å¥½ï¼æˆ‘æ˜¯AIåŠ©æ‰‹ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ã€‚"},
                {"input": "ä½ èƒ½åšä»€ä¹ˆï¼Ÿ", "output": "æˆ‘å¯ä»¥å›ç­”é—®é¢˜ã€æä¾›ä¿¡æ¯ã€å¸®åŠ©è§£å†³é—®é¢˜ç­‰ã€‚"},
                {"input": "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ", "output": "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•è·å–å®æ—¶å¤©æ°”ä¿¡æ¯ã€‚"},
            ]
    
    def save_evaluation_results(self, results: Dict[str, Any], filename: str):
        """
        ä¿å­˜è¯„ä¼°ç»“æœ
        Args:
            results: è¯„ä¼°ç»“æœ
            filename: æ–‡ä»¶å
        """
        try:
            # åˆ›å»ºè¾“å‡ºç›®å½•
            output_dir = Path("evaluation_results")
            output_dir.mkdir(exist_ok=True)
            
            # æ·»åŠ æ—¶é—´æˆ³
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            filepath = output_dir / f"{filename}_{timestamp}.json"
            
            # ä¿å­˜ç»“æœ
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"ğŸ’¾ è¯„ä¼°ç»“æœå·²ä¿å­˜: {filepath}")
            
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜è¯„ä¼°ç»“æœå¤±è´¥: {str(e)}")
    
    def compare_models(self, model_results: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """
        æ¯”è¾ƒå¤šä¸ªæ¨¡å‹çš„è¯„ä¼°ç»“æœ
        Args:
            model_results: æ¨¡å‹ç»“æœå­—å…¸ {model_name: results}
        Returns:
            Dict[str, Any]: æ¯”è¾ƒç»“æœ
        """
        self.logger.info(f"ğŸ“Š å¼€å§‹æ¯”è¾ƒ{len(model_results)}ä¸ªæ¨¡å‹")
        
        comparison = {
            'models': list(model_results.keys()),
            'comparison_time': time.time(),
            'metrics_comparison': {}
        }
        
        # æå–å…³é”®æŒ‡æ ‡è¿›è¡Œæ¯”è¾ƒ
        metrics_to_compare = ['bleu', 'rouge', 'f1', 'exact_match']
        
        for metric in metrics_to_compare:
            comparison['metrics_comparison'][metric] = {}
            
            for model_name, results in model_results.items():
                if 'metrics' in results and metric in results['metrics']:
                    if isinstance(results['metrics'][metric], dict):
                        if 'average' in results['metrics'][metric]:
                            comparison['metrics_comparison'][metric][model_name] = results['metrics'][metric]['average']
                        elif 'rate' in results['metrics'][metric]:
                            comparison['metrics_comparison'][metric][model_name] = results['metrics'][metric]['rate']
        
        # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
        best_models = {}
        for metric, scores in comparison['metrics_comparison'].items():
            if scores:
                if metric == 'rouge':
                    # ROUGEæ˜¯å­—å…¸ï¼Œå–rouge-l
                    rouge_l_scores = {model: score['rouge-l'] if isinstance(score, dict) else score for model, score in scores.items()}
                    best_model = max(rouge_l_scores.items(), key=lambda x: x[1])
                else:
                    best_model = max(scores.items(), key=lambda x: x[1])
                best_models[metric] = best_model
        
        comparison['best_models'] = best_models
        
        self.logger.info("ğŸ† æ¨¡å‹æ¯”è¾ƒå®Œæˆ")
        for metric, (model, score) in best_models.items():
            self.logger.info(f"  {metric}: {model} ({score:.4f})")
        
        return comparison

class EvaluationPresets:
    """è¯„ä¼°é¢„è®¾é…ç½®"""
    
    @staticmethod
    def get_quick_eval_config() -> Dict[str, Any]:
        """å¿«é€Ÿè¯„ä¼°é…ç½®"""
        return {
            'sample_size': 50,
            'metrics': ['bleu', 'rouge', 'f1'],
            'save_results': True
        }
    
    @staticmethod
    def get_comprehensive_eval_config() -> Dict[str, Any]:
        """å…¨é¢è¯„ä¼°é…ç½®"""
        return {
            'sample_size': None,  # ä½¿ç”¨å…¨éƒ¨æ•°æ®
            'metrics': ['bleu', 'rouge', 'f1', 'exact_match', 'semantic_similarity'],
            'save_results': True,
            'generate_report': True
        }
    
    @staticmethod
    def get_benchmark_config() -> Dict[str, Any]:
        """åŸºå‡†æµ‹è¯•é…ç½®"""
        return {
            'datasets': ['qa_test', 'math_test'],
            'sample_size': 100,
            'compare_models': True,
            'save_results': True
        }

def create_evaluation_manager(config_path: Optional[str] = None) -> EvaluationManager:
    """
    åˆ›å»ºè¯„ä¼°ç®¡ç†å™¨å®ä¾‹
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
    Returns:
        EvaluationManager: è¯„ä¼°ç®¡ç†å™¨å®ä¾‹
    """
    return EvaluationManager(config_path)

if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    print("ğŸ§ª è¯„ä¼°å™¨æµ‹è¯•...")
    
    # åˆ›å»ºè¯„ä¼°ç®¡ç†å™¨
    eval_manager = create_evaluation_manager()
    
    # æµ‹è¯•æ•°æ®
    references = ["è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å¥å­ã€‚", "äººå·¥æ™ºèƒ½å¾ˆæœ‰è¶£ã€‚"]
    candidates = ["è¿™æ˜¯æµ‹è¯•å¥å­ã€‚", "AIå¾ˆæœ‰è¶£ã€‚"]
    
    # æ‰§è¡Œè¯„ä¼°
    results = eval_manager.evaluate_generation_quality(references, candidates, "test")
    
    print("ğŸ“Š è¯„ä¼°ç»“æœ:")
    print(f"  BLEU: {results['metrics']['bleu']['average']:.4f}")
    print(f"  F1: {results['metrics']['f1']['average']:.4f}")
    
    print("âœ… è¯„ä¼°å™¨æ¨¡å—åŠ è½½æˆåŠŸ")