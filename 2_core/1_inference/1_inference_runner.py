#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ ¸å¿ƒæ¨ç†æ¨¡å— - åŸºäºMS-Swiftçš„æ¨¡å‹æ¨ç†å™¨
æ”¯æŒLoRAã€å…¨å‚æ•°å¾®è°ƒåçš„æ¨¡å‹æ¨ç†
"""

import os
import sys
import json
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Union

# æ·»åŠ utilsè·¯å¾„
sys.path.append(str(Path(__file__).parent.parent.parent / "1_utils"))

from config_loader import ConfigLoader
from logger import Logger

try:
    import torch
    from swift.llm import (
        PtEngine, RequestConfig, InferRequest, 
        get_model_tokenizer, get_template, BaseArguments
    )
    from swift.tuners import Swift
except ImportError as e:
    print(f"âŒ å¯¼å…¥ä¾èµ–æ¨¡å—å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿å·²å®‰è£…: pip install ms-swift torch")
    sys.exit(1)

class InferenceManager:
    """æ¨ç†ç®¡ç†å™¨ - ç»Ÿä¸€çš„æ¨¡å‹æ¨ç†æ¥å£"""
    
    def __init__(self, checkpoint_path: Optional[str] = None, config_path: Optional[str] = None):
        """
        åˆå§‹åŒ–æ¨ç†ç®¡ç†å™¨
        Args:
            checkpoint_path: æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.logger = Logger("InferenceManager").get_logger()
        self.config_loader = ConfigLoader()
        
        # åŠ è½½é…ç½®
        if config_path:
            self.config = self.config_loader.load_config(config_path)
        else:
            self.config = {}
        
        self.checkpoint_path = Path(checkpoint_path) if checkpoint_path else None
        self.engine = None
        self.is_lora = False
        self.model_info = {}
    
    def detect_model_type(self, checkpoint_path: Path) -> str:
        """
        æ£€æµ‹æ¨¡å‹ç±»å‹
        Args:
            checkpoint_path: æ£€æŸ¥ç‚¹è·¯å¾„
        Returns:
            str: æ¨¡å‹ç±»å‹ ('lora' æˆ– 'full')
        """
        # æ£€æŸ¥æ˜¯å¦ä¸ºLoRA checkpointï¼ˆåŒ…å«adapterç›¸å…³æ–‡ä»¶ï¼‰
        adapter_files = [
            "adapter_config.json", 
            "adapter_model.safetensors", 
            "adapter_model.bin",
            "adapter_model.pt"
        ]
        
        has_adapter = any((checkpoint_path / f).exists() for f in adapter_files)
        return "lora" if has_adapter else "full"
    
    def load_lora_model(self, checkpoint_path: Path):
        """
        åŠ è½½LoRAæ¨¡å‹
        Args:
            checkpoint_path: LoRAæ£€æŸ¥ç‚¹è·¯å¾„
        """
        self.logger.info(f"ğŸ“ åŠ è½½LoRA checkpoint: {checkpoint_path}")
        
        try:
            # åŠ è½½è®­ç»ƒå‚æ•°
            args = BaseArguments.from_pretrained(str(checkpoint_path))
            model_id = getattr(args, 'model', None)
            
            if not model_id:
                raise ValueError("âŒ æ— æ³•è·å–åŸºç¡€æ¨¡å‹è·¯å¾„")
            
            self.logger.info(f"ğŸ”§ åŸºç¡€æ¨¡å‹: {model_id}")
            self.logger.info(f"ğŸ¨ æ¨¡æ¿ç±»å‹: {getattr(args, 'template', 'default')}")
            
            # è®°å½•æ¨¡å‹ä¿¡æ¯
            self.model_info = {
                'base_model': model_id,
                'template_type': getattr(args, 'template', 'default'),
                'system_prompt': getattr(args, 'system', None),
                'checkpoint_path': str(checkpoint_path),
                'model_type': 'lora'
            }
            
            # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
            model, tokenizer = get_model_tokenizer(model_id)
            
            # åŠ è½½LoRAæƒé‡
            model = Swift.from_pretrained(model, str(checkpoint_path))
            
            # åˆ›å»ºæ¨¡æ¿
            template_type = getattr(args, 'template', 'default')
            system_prompt = getattr(args, 'system', None)
            template = get_template(template_type, tokenizer, default_system=system_prompt)
            
            # åˆ›å»ºæ¨ç†å¼•æ“
            self.engine = PtEngine.from_model_template(model, template, max_batch_size=1)
            self.is_lora = True
            
            self.logger.info("âœ… LoRAæ¨¡å‹åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            self.logger.error(f"âŒ LoRAæ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            raise e
    
    def load_full_model(self, checkpoint_path: Path):
        """
        åŠ è½½å…¨å‚æ•°æ¨¡å‹
        Args:
            checkpoint_path: å…¨å‚æ•°æ¨¡å‹è·¯å¾„
        """
        self.logger.info(f"ğŸ“ åŠ è½½å…¨å‚æ•°checkpoint: {checkpoint_path}")
        
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰è®­ç»ƒå‚æ•°æ–‡ä»¶
            args_file = checkpoint_path / "args.json"
            if args_file.exists():
                args = BaseArguments.from_pretrained(str(checkpoint_path))
                model_path = str(checkpoint_path)
                template_type = getattr(args, 'template', 'default')
                default_system = getattr(args, 'system', None)
            else:
                # å¦‚æœæ²¡æœ‰args.jsonï¼Œå‡è®¾checkpointå°±æ˜¯æ¨¡å‹è·¯å¾„
                model_path = str(checkpoint_path)
                template_type = 'default'
                default_system = None
            
            # è®°å½•æ¨¡å‹ä¿¡æ¯
            self.model_info = {
                'model_path': model_path,
                'template_type': template_type,
                'system_prompt': default_system,
                'checkpoint_path': str(checkpoint_path),
                'model_type': 'full'
            }
            
            # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
            model, tokenizer = get_model_tokenizer(model_path)
            
            # åˆ›å»ºæ¨¡æ¿
            template = get_template(template_type, tokenizer, default_system=default_system)
            
            # åˆ›å»ºæ¨ç†å¼•æ“
            self.engine = PtEngine.from_model_template(model, template, max_batch_size=1)
            self.is_lora = False
            
            self.logger.info("âœ… å…¨å‚æ•°æ¨¡å‹åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            self.logger.error(f"âŒ å…¨å‚æ•°æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            raise e
    
    def load_model(self, checkpoint_path: Optional[str] = None):
        """
        è‡ªåŠ¨æ£€æµ‹å¹¶åŠ è½½æ¨¡å‹
        Args:
            checkpoint_path: æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„
        """
        if checkpoint_path:
            self.checkpoint_path = Path(checkpoint_path)
        
        if not self.checkpoint_path or not self.checkpoint_path.exists():
            raise ValueError("âŒ æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„æ— æ•ˆ")
        
        self.logger.info("ğŸš€ å¼€å§‹åŠ è½½æ¨¡å‹...")
        
        # æ£€æµ‹æ¨¡å‹ç±»å‹
        model_type = self.detect_model_type(self.checkpoint_path)
        self.logger.info(f"ğŸ” æ£€æµ‹åˆ°æ¨¡å‹ç±»å‹: {model_type.upper()}")
        
        # æ ¹æ®ç±»å‹åŠ è½½æ¨¡å‹
        if model_type == "lora":
            self.load_lora_model(self.checkpoint_path)
        else:
            self.load_full_model(self.checkpoint_path)
    
    def extract_response_content(self, response) -> str:
        """
        ä»å“åº”ä¸­æå–å†…å®¹
        Args:
            response: æ¨ç†å“åº”å¯¹è±¡
        Returns:
            str: æå–çš„æ–‡æœ¬å†…å®¹
        """
        try:
            if hasattr(response, 'choices') and response.choices:
                choice = response.choices[0]
                if hasattr(choice, 'message') and hasattr(choice.message, 'content'):
                    content = choice.message.content
                    return str(content) if content is not None else ""
        except (AttributeError, IndexError, TypeError) as e:
            self.logger.warning(f"âš ï¸  æå–å“åº”å†…å®¹å¤±è´¥: {e}")
        return ""
    
    def infer_single(self, 
                    question: str, 
                    max_tokens: int = 512, 
                    temperature: float = 0.7,
                    top_p: float = 0.9) -> str:
        """
        å•æ¡æ¨ç†
        Args:
            question: è¾“å…¥é—®é¢˜
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            temperature: ç”Ÿæˆæ¸©åº¦
            top_p: æ ¸é‡‡æ ·æ¦‚ç‡
        Returns:
            str: ç”Ÿæˆçš„å›ç­”
        """
        if not self.engine:
            raise ValueError("âŒ æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨load_model()")
        
        try:
            # åˆ›å»ºè¯·æ±‚é…ç½®
            request_config = RequestConfig(
                max_tokens=max_tokens, 
                temperature=temperature,
                top_p=top_p
            )
            
            # åˆ›å»ºæ¨ç†è¯·æ±‚
            infer_request = InferRequest(
                messages=[{'role': 'user', 'content': question}]
            )
            
            # æ‰§è¡Œæ¨ç†
            resp_list = self.engine.infer([infer_request], request_config)
            
            if resp_list and len(resp_list) > 0:
                return self.extract_response_content(resp_list[0])
            
            return ""
            
        except Exception as e:
            self.logger.error(f"âŒ æ¨ç†å¤±è´¥: {str(e)}")
            raise e
    
    def infer_batch(self, 
                   questions: List[str], 
                   max_tokens: int = 512, 
                   temperature: float = 0.7) -> List[str]:
        """
        æ‰¹é‡æ¨ç†
        Args:
            questions: é—®é¢˜åˆ—è¡¨
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            temperature: ç”Ÿæˆæ¸©åº¦
        Returns:
            List[str]: å›ç­”åˆ—è¡¨
        """
        if not self.engine:
            raise ValueError("âŒ æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨load_model()")
        
        self.logger.info(f"ğŸ”„ å¼€å§‹æ‰¹é‡æ¨ç†ï¼Œå…±{len(questions)}ä¸ªé—®é¢˜")
        
        results = []
        for i, question in enumerate(questions, 1):
            try:
                self.logger.info(f"ğŸ“ å¤„ç†é—®é¢˜ {i}/{len(questions)}")
                response = self.infer_single(question, max_tokens, temperature)
                results.append(response)
            except Exception as e:
                self.logger.error(f"âŒ é—®é¢˜ {i} æ¨ç†å¤±è´¥: {str(e)}")
                results.append("")
        
        return results
    
    def get_model_info(self) -> Dict[str, Any]:
        """
        è·å–æ¨¡å‹ä¿¡æ¯
        Returns:
            Dict[str, Any]: æ¨¡å‹ä¿¡æ¯å­—å…¸
        """
        return self.model_info.copy()
    
    def interactive_chat(self):
        """äº¤äº’å¼å¯¹è¯"""
        if not self.engine:
            raise ValueError("âŒ æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨load_model()")
        
        self.logger.info("ğŸ® è¿›å…¥äº¤äº’å¼å¯¹è¯æ¨¡å¼ (è¾“å…¥ 'quit' é€€å‡º)")
        print("\n" + "="*50)
        print("ğŸ¤– AIåŠ©æ‰‹å·²å°±ç»ªï¼Œå¼€å§‹å¯¹è¯å§ï¼")
        print("ğŸ’¡ è¾“å…¥ 'quit', 'exit' æˆ– 'é€€å‡º' ç»“æŸå¯¹è¯")
        print("="*50)
        
        conversation_history = []
        
        while True:
            try:
                # è·å–ç”¨æˆ·è¾“å…¥
                user_input = input("\nğŸ‘¤ æ‚¨: ").strip()
                
                # æ£€æŸ¥é€€å‡ºå‘½ä»¤
                if user_input.lower() in ['quit', 'exit', 'é€€å‡º', 'q']:
                    print("ğŸ‘‹ å†è§ï¼")
                    break
                
                if not user_input:
                    continue
                
                # è®°å½•å¯¹è¯å†å²
                conversation_history.append(f"ç”¨æˆ·: {user_input}")
                
                # æ‰§è¡Œæ¨ç†
                print("ğŸ¤– AI: ", end="", flush=True)
                response = self.infer_single(user_input)
                print(response)
                
                # è®°å½•AIå›å¤
                conversation_history.append(f"AI: {response}")
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ å¯¹è¯å·²ä¸­æ–­ï¼Œå†è§ï¼")
                break
            except Exception as e:
                self.logger.error(f"âŒ å¯¹è¯è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
                print(f"ğŸ˜“ æŠ±æ­‰ï¼Œå‡ºç°äº†ä¸€äº›é—®é¢˜: {str(e)}")

class InferencePresets:
    """æ¨ç†é¢„è®¾é…ç½®"""
    
    @staticmethod
    def get_default_config() -> Dict[str, Any]:
        """è·å–é»˜è®¤æ¨ç†é…ç½®"""
        return {
            'max_tokens': 512,
            'temperature': 0.7,
            'top_p': 0.9,
        }
    
    @staticmethod
    def get_creative_config() -> Dict[str, Any]:
        """è·å–åˆ›æ„ç”Ÿæˆé…ç½®"""
        return {
            'max_tokens': 1024,
            'temperature': 0.9,
            'top_p': 0.95,
        }
    
    @staticmethod
    def get_precise_config() -> Dict[str, Any]:
        """è·å–ç²¾ç¡®å›ç­”é…ç½®"""
        return {
            'max_tokens': 256,
            'temperature': 0.1,
            'top_p': 0.8,
        }

def create_inference_manager(checkpoint_path: Optional[str] = None, 
                           config_path: Optional[str] = None) -> InferenceManager:
    """
    åˆ›å»ºæ¨ç†ç®¡ç†å™¨å®ä¾‹
    Args:
        checkpoint_path: æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
    Returns:
        InferenceManager: æ¨ç†ç®¡ç†å™¨å®ä¾‹
    """
    return InferenceManager(checkpoint_path, config_path)

if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    print("ğŸ§ª æ¨ç†å™¨æµ‹è¯•...")
    
    # åˆ›å»ºæ¨ç†ç®¡ç†å™¨
    inference_manager = create_inference_manager()
    
    # æ˜¾ç¤ºé¢„è®¾é…ç½®
    configs = {
        "é»˜è®¤é…ç½®": InferencePresets.get_default_config(),
        "åˆ›æ„é…ç½®": InferencePresets.get_creative_config(),
        "ç²¾ç¡®é…ç½®": InferencePresets.get_precise_config(),
    }
    
    print("ğŸ”§ å¯ç”¨çš„æ¨ç†é…ç½®:")
    for name, config in configs.items():
        print(f"  {name}: {config}")
    
    print("âœ… æ¨ç†å™¨æ¨¡å—åŠ è½½æˆåŠŸ")