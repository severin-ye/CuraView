#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ ¸å¿ƒå±‚ç»Ÿä¸€æ¥å£ - æ•´åˆè®­ç»ƒã€æ¨ç†ã€éƒ¨ç½²ã€è¯„ä¼°åŠŸèƒ½
æä¾›ç®€æ´çš„APIæ¥å£ä¾›ä¸Šå±‚è°ƒç”¨
"""

import sys
from pathlib import Path
from typing import Dict, Any, Optional, List, Callable

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
current_dir = Path(__file__).parent
sys.path.append(str(current_dir))

# å¯¼å…¥å„ä¸ªæ¨¡å—
from training.trainer import TrainingManager, TrainingPresets
from inference.inference import InferenceManager, InferencePresets
from deployment.deploy import DeploymentManager, DeploymentPresets
from evaluation.evaluator import EvaluationManager, EvaluationPresets

# æ·»åŠ utilsè·¯å¾„
sys.path.append(str(current_dir.parent / "1_utils"))
from logger import Logger

class CoreAPI:
    """æ ¸å¿ƒAPI - ç»Ÿä¸€çš„æ¨¡å‹è®­ç»ƒã€æ¨ç†ã€éƒ¨ç½²ã€è¯„ä¼°æ¥å£"""
    
    def __init__(self, config_path: Optional[str] = None):
        """
        åˆå§‹åŒ–æ ¸å¿ƒAPI
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.logger = Logger("CoreAPI").get_logger()
        self.config_path = config_path
        
        # åˆå§‹åŒ–å„ä¸ªç®¡ç†å™¨
        self.trainer = None
        self.inference_manager = None
        self.deployment_manager = None
        self.evaluation_manager = None
        
        self.logger.info("ğŸš€ æ ¸å¿ƒAPIåˆå§‹åŒ–å®Œæˆ")
    
    def get_trainer(self) -> TrainingManager:
        """è·å–è®­ç»ƒç®¡ç†å™¨"""
        if self.trainer is None:
            self.trainer = TrainingManager(self.config_path)
        return self.trainer
    
    def get_inference_manager(self, checkpoint_path: Optional[str] = None) -> InferenceManager:
        """è·å–æ¨ç†ç®¡ç†å™¨"""
        if self.inference_manager is None:
            self.inference_manager = InferenceManager(checkpoint_path, self.config_path)
        return self.inference_manager
    
    def get_deployment_manager(self) -> DeploymentManager:
        """è·å–éƒ¨ç½²ç®¡ç†å™¨"""
        if self.deployment_manager is None:
            self.deployment_manager = DeploymentManager(self.config_path)
        return self.deployment_manager
    
    def get_evaluation_manager(self) -> EvaluationManager:
        """è·å–è¯„ä¼°ç®¡ç†å™¨"""
        if self.evaluation_manager is None:
            self.evaluation_manager = EvaluationManager(self.config_path)
        return self.evaluation_manager
    
    # è®­ç»ƒç›¸å…³æ¥å£
    def train_model(self, 
                   train_type: str = "lora",
                   model: str = "Qwen/Qwen2.5-7B-Instruct",
                   dataset: Optional[List[str]] = None,
                   output_dir: str = "./output",
                   **kwargs) -> str:
        """
        è®­ç»ƒæ¨¡å‹
        Args:
            train_type: è®­ç»ƒç±»å‹ (lora/qlora/full/multimodal)
            model: åŸºç¡€æ¨¡å‹
            dataset: è®­ç»ƒæ•°æ®é›†
            output_dir: è¾“å‡ºç›®å½•
            **kwargs: å…¶ä»–è®­ç»ƒå‚æ•°
        Returns:
            str: è®­ç»ƒè¾“å‡ºç›®å½•
        """
        self.logger.info(f"ğŸ¯ å¼€å§‹{train_type.upper()}è®­ç»ƒ")
        
        trainer = self.get_trainer()
        
        if train_type == "lora":
            return trainer.train_lora(
                model=model, dataset=dataset, output_dir=output_dir, **kwargs
            )
        elif train_type == "qlora":
            return trainer.train_qlora(
                model=model, dataset=dataset, output_dir=output_dir, **kwargs
            )
        elif train_type == "full":
            return trainer.train_full_params(
                model=model, dataset=dataset, output_dir=output_dir, **kwargs
            )
        elif train_type == "multimodal":
            return trainer.train_multimodal(
                model=model, dataset=dataset, output_dir=output_dir, **kwargs
            )
        else:
            raise ValueError(f"âŒ ä¸æ”¯æŒçš„è®­ç»ƒç±»å‹: {train_type}")
    
    def train_with_preset(self, preset_name: str, **overrides) -> str:
        """
        ä½¿ç”¨é¢„è®¾é…ç½®è®­ç»ƒ
        Args:
            preset_name: é¢„è®¾åç§° (lora/qlora/full/multimodal)
            **overrides: è¦†ç›–å‚æ•°
        Returns:
            str: è®­ç»ƒè¾“å‡ºç›®å½•
        """
        presets = {
            'lora': TrainingPresets.get_lora_preset,
            'qlora': TrainingPresets.get_qlora_preset,
            'full': TrainingPresets.get_full_preset,
            'multimodal': TrainingPresets.get_multimodal_preset
        }
        
        if preset_name not in presets:
            raise ValueError(f"âŒ æœªçŸ¥é¢„è®¾: {preset_name}")
        
        config = presets[preset_name]()
        config.update(overrides)
        
        trainer = self.get_trainer()
        trainer.config = config
        return trainer.run_training_from_config()
    
    # æ¨ç†ç›¸å…³æ¥å£
    def load_model_for_inference(self, checkpoint_path: str):
        """
        åŠ è½½æ¨¡å‹ç”¨äºæ¨ç†
        Args:
            checkpoint_path: æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„
        """
        self.logger.info(f"ğŸ“‚ åŠ è½½æ¨ç†æ¨¡å‹: {checkpoint_path}")
        inference_manager = self.get_inference_manager(checkpoint_path)
        inference_manager.load_model()
        self.logger.info("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def infer_single(self, 
                    question: str, 
                    checkpoint_path: Optional[str] = None,
                    **infer_config) -> str:
        """
        å•æ¡æ¨ç†
        Args:
            question: è¾“å…¥é—®é¢˜
            checkpoint_path: æ£€æŸ¥ç‚¹è·¯å¾„ï¼ˆå¦‚æœæœªé¢„åŠ è½½ï¼‰
            **infer_config: æ¨ç†é…ç½®
        Returns:
            str: æ¨ç†ç»“æœ
        """
        inference_manager = self.get_inference_manager(checkpoint_path)
        
        # å¦‚æœæ¨¡å‹æœªåŠ è½½ï¼Œå…ˆåŠ è½½
        if not inference_manager.engine:
            if checkpoint_path:
                inference_manager.load_model(checkpoint_path)
            else:
                raise ValueError("âŒ æ¨¡å‹æœªåŠ è½½ä¸”æœªæä¾›æ£€æŸ¥ç‚¹è·¯å¾„")
        
        return inference_manager.infer_single(question, **infer_config)
    
    def infer_batch(self, 
                   questions: List[str], 
                   checkpoint_path: Optional[str] = None,
                   **infer_config) -> List[str]:
        """
        æ‰¹é‡æ¨ç†
        Args:
            questions: é—®é¢˜åˆ—è¡¨
            checkpoint_path: æ£€æŸ¥ç‚¹è·¯å¾„
            **infer_config: æ¨ç†é…ç½®
        Returns:
            List[str]: æ¨ç†ç»“æœåˆ—è¡¨
        """
        inference_manager = self.get_inference_manager(checkpoint_path)
        
        if not inference_manager.engine and checkpoint_path:
            inference_manager.load_model(checkpoint_path)
        
        return inference_manager.infer_batch(questions, **infer_config)
    
    def start_interactive_chat(self, checkpoint_path: Optional[str] = None):
        """
        å¯åŠ¨äº¤äº’å¼å¯¹è¯
        Args:
            checkpoint_path: æ£€æŸ¥ç‚¹è·¯å¾„
        """
        inference_manager = self.get_inference_manager(checkpoint_path)
        
        if not inference_manager.engine and checkpoint_path:
            inference_manager.load_model(checkpoint_path)
        
        inference_manager.interactive_chat()
    
    # éƒ¨ç½²ç›¸å…³æ¥å£
    def deploy_model(self, 
                    checkpoint_path: str, 
                    **deploy_config) -> Dict[str, Any]:
        """
        éƒ¨ç½²æ¨¡å‹
        Args:
            checkpoint_path: æ£€æŸ¥ç‚¹è·¯å¾„
            **deploy_config: éƒ¨ç½²é…ç½®
        Returns:
            Dict[str, Any]: éƒ¨ç½²ä¿¡æ¯
        """
        self.logger.info(f"ğŸš€ éƒ¨ç½²æ¨¡å‹: {checkpoint_path}")
        
        deployment_manager = self.get_deployment_manager()
        return deployment_manager.deploy_single_model(checkpoint_path, deploy_config)
    
    def deploy_with_preset(self, 
                          checkpoint_path: str, 
                          preset_name: str = "local", 
                          **overrides) -> Dict[str, Any]:
        """
        ä½¿ç”¨é¢„è®¾é…ç½®éƒ¨ç½²
        Args:
            checkpoint_path: æ£€æŸ¥ç‚¹è·¯å¾„
            preset_name: é¢„è®¾åç§° (local/server/production)
            **overrides: è¦†ç›–å‚æ•°
        Returns:
            Dict[str, Any]: éƒ¨ç½²ä¿¡æ¯
        """
        presets = {
            'local': DeploymentPresets.get_local_preset,
            'server': DeploymentPresets.get_server_preset,
            'production': DeploymentPresets.get_production_preset
        }
        
        if preset_name not in presets:
            raise ValueError(f"âŒ æœªçŸ¥é¢„è®¾: {preset_name}")
        
        config = presets[preset_name]()
        config.update(overrides)
        
        return self.deploy_model(checkpoint_path, **config)
    
    def stop_deployment(self):
        """åœæ­¢éƒ¨ç½²"""
        if self.deployment_manager:
            self.deployment_manager.stop_deployment()
    
    def get_deployment_status(self) -> Dict[str, Any]:
        """è·å–éƒ¨ç½²çŠ¶æ€"""
        if self.deployment_manager:
            return self.deployment_manager.get_deployment_status()
        return {'status': 'not_deployed'}
    
    # è¯„ä¼°ç›¸å…³æ¥å£
    def evaluate_model(self, 
                      checkpoint_path: str,
                      test_data: List[Dict[str, str]],
                      task_type: str = "qa") -> Dict[str, Any]:
        """
        è¯„ä¼°æ¨¡å‹æ€§èƒ½
        Args:
            checkpoint_path: æ£€æŸ¥ç‚¹è·¯å¾„
            test_data: æµ‹è¯•æ•°æ®
            task_type: ä»»åŠ¡ç±»å‹
        Returns:
            Dict[str, Any]: è¯„ä¼°ç»“æœ
        """
        self.logger.info(f"ğŸ“Š è¯„ä¼°æ¨¡å‹: {checkpoint_path}")
        
        # åˆ›å»ºæ¨ç†å‡½æ•°
        inference_manager = self.get_inference_manager(checkpoint_path)
        inference_manager.load_model()
        
        def inference_func(question: str) -> str:
            return inference_manager.infer_single(question)
        
        # æ‰§è¡Œè¯„ä¼°
        evaluation_manager = self.get_evaluation_manager()
        return evaluation_manager.evaluate_model_performance(
            inference_func, test_data, task_type
        )
    
    def benchmark_model(self, 
                       checkpoint_path: str,
                       dataset_name: str = "qa_test",
                       sample_size: Optional[int] = None) -> Dict[str, Any]:
        """
        åŸºå‡†æµ‹è¯•
        Args:
            checkpoint_path: æ£€æŸ¥ç‚¹è·¯å¾„
            dataset_name: æ•°æ®é›†åç§°
            sample_size: é‡‡æ ·å¤§å°
        Returns:
            Dict[str, Any]: åŸºå‡†æµ‹è¯•ç»“æœ
        """
        self.logger.info(f"ğŸ¯ åŸºå‡†æµ‹è¯•: {dataset_name}")
        
        # åˆ›å»ºæ¨ç†å‡½æ•°
        inference_manager = self.get_inference_manager(checkpoint_path)
        inference_manager.load_model()
        
        def inference_func(question: str) -> str:
            return inference_manager.infer_single(question)
        
        # æ‰§è¡ŒåŸºå‡†æµ‹è¯•
        evaluation_manager = self.get_evaluation_manager()
        return evaluation_manager.benchmark_on_dataset(
            inference_func, dataset_name, sample_size=sample_size
        )
    
    # å·¥ä½œæµæ¥å£
    def full_pipeline(self, 
                     train_config: Dict[str, Any],
                     eval_config: Optional[Dict[str, Any]] = None,
                     deploy_config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        å®Œæ•´çš„è®­ç»ƒ-è¯„ä¼°-éƒ¨ç½²æµæ°´çº¿
        Args:
            train_config: è®­ç»ƒé…ç½®
            eval_config: è¯„ä¼°é…ç½®
            deploy_config: éƒ¨ç½²é…ç½®
        Returns:
            Dict[str, Any]: æµæ°´çº¿ç»“æœ
        """
        results = {
            'pipeline_start': self.logger.info("ğŸ”„ å¼€å§‹å®Œæ•´æµæ°´çº¿"),
            'training': None,
            'evaluation': None,
            'deployment': None
        }
        
        try:
            # 1. è®­ç»ƒ
            self.logger.info("ğŸ“š ç¬¬1æ­¥: å¼€å§‹è®­ç»ƒ")
            output_dir = self.train_model(**train_config)
            results['training'] = {'output_dir': output_dir, 'status': 'success'}
            
            # 2. è¯„ä¼°ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
            if eval_config:
                self.logger.info("ğŸ“Š ç¬¬2æ­¥: å¼€å§‹è¯„ä¼°")
                eval_results = self.evaluate_model(output_dir, **eval_config)
                results['evaluation'] = eval_results
            
            # 3. éƒ¨ç½²ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
            if deploy_config:
                self.logger.info("ğŸš€ ç¬¬3æ­¥: å¼€å§‹éƒ¨ç½²")
                deploy_results = self.deploy_model(output_dir, **deploy_config)
                results['deployment'] = deploy_results
            
            self.logger.info("âœ… å®Œæ•´æµæ°´çº¿æ‰§è¡ŒæˆåŠŸ")
            results['status'] = 'success'
            
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ æµæ°´çº¿æ‰§è¡Œå¤±è´¥: {str(e)}")
            results['status'] = 'failed'
            results['error'] = str(e)
            return results

# å…¨å±€APIå®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
_core_api_instance = None

def get_core_api(config_path: Optional[str] = None) -> CoreAPI:
    """
    è·å–æ ¸å¿ƒAPIå®ä¾‹ï¼ˆå•ä¾‹ï¼‰
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
    Returns:
        CoreAPI: æ ¸å¿ƒAPIå®ä¾‹
    """
    global _core_api_instance
    if _core_api_instance is None:
        _core_api_instance = CoreAPI(config_path)
    return _core_api_instance

# ä¾¿åˆ©å‡½æ•°
def train_model(**kwargs) -> str:
    """ä¾¿åˆ©å‡½æ•°ï¼šè®­ç»ƒæ¨¡å‹"""
    api = get_core_api()
    return api.train_model(**kwargs)

def infer_single(question: str, checkpoint_path: str, **kwargs) -> str:
    """ä¾¿åˆ©å‡½æ•°ï¼šå•æ¡æ¨ç†"""
    api = get_core_api()
    return api.infer_single(question, checkpoint_path, **kwargs)

def deploy_model(checkpoint_path: str, **kwargs) -> Dict[str, Any]:
    """ä¾¿åˆ©å‡½æ•°ï¼šéƒ¨ç½²æ¨¡å‹"""
    api = get_core_api()
    return api.deploy_model(checkpoint_path, **kwargs)

def evaluate_model(checkpoint_path: str, test_data: List[Dict[str, str]], **kwargs) -> Dict[str, Any]:
    """ä¾¿åˆ©å‡½æ•°ï¼šè¯„ä¼°æ¨¡å‹"""
    api = get_core_api()
    return api.evaluate_model(checkpoint_path, test_data, **kwargs)

if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    print("ğŸ§ª æ ¸å¿ƒAPIæµ‹è¯•...")
    
    # åˆ›å»ºAPIå®ä¾‹
    api = get_core_api()
    
    print("ğŸ”§ å¯ç”¨çš„åŠŸèƒ½:")
    print("  ğŸ“š è®­ç»ƒ: train_model()")
    print("  ğŸ¤– æ¨ç†: infer_single(), infer_batch()")
    print("  ğŸš€ éƒ¨ç½²: deploy_model()")
    print("  ğŸ“Š è¯„ä¼°: evaluate_model(), benchmark_model()")
    print("  ğŸ”„ æµæ°´çº¿: full_pipeline()")
    
    print("âœ… æ ¸å¿ƒAPIåŠ è½½æˆåŠŸ")