#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€è®­ç»ƒè„šæœ¬ - åŸºäºæ–°æ¶æ„çš„æ¨¡å‹è®­ç»ƒå…¥å£
æ”¯æŒLoRAã€QLoRAã€å…¨å‚æ•°å¾®è°ƒã€å¤šæ¨¡æ€å¾®è°ƒ
"""

import sys
import argparse
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))
sys.path.append(str(project_root / "2_core"))

from logger import Logger
from config_loader import ConfigLoader

def main():
    parser = argparse.ArgumentParser(description="ç»Ÿä¸€è®­ç»ƒè„šæœ¬", formatter_class=argparse.RawDescriptionHelpFormatter)
    
    # åŸºç¡€å‚æ•°
    parser.add_argument("--config", type=str, help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--model", type=str, default="Qwen/Qwen2.5-7B-Instruct", help="åŸºç¡€æ¨¡å‹è·¯å¾„")
    parser.add_argument("--dataset", type=str, nargs='+', help="è®­ç»ƒæ•°æ®é›†")
    parser.add_argument("--output_dir", type=str, default="./6_output", help="è¾“å‡ºç›®å½•")
    
    # è®­ç»ƒç±»å‹
    parser.add_argument("--train_type", choices=['lora', 'qlora', 'full', 'multimodal'], 
                        default='lora', help="è®­ç»ƒç±»å‹")
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument("--num_train_epochs", type=int, default=3, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--per_device_train_batch_size", type=int, default=2, help="æ¯è®¾å¤‡æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=8, help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°")
    parser.add_argument("--learning_rate", type=float, default=1e-4, help="å­¦ä¹ ç‡")
    parser.add_argument("--max_length", type=int, default=2048, help="æœ€å¤§åºåˆ—é•¿åº¦")
    
    # LoRAå‚æ•°
    parser.add_argument("--lora_rank", type=int, default=8, help="LoRAç§©")
    parser.add_argument("--lora_alpha", type=int, default=32, help="LoRA alpha")
    parser.add_argument("--lora_dropout", type=float, default=0.05, help="LoRA dropout")
    
    # å…¶ä»–å‚æ•°
    parser.add_argument("--save_steps", type=int, default=100, help="ä¿å­˜æ­¥æ•°")
    parser.add_argument("--logging_steps", type=int, default=10, help="æ—¥å¿—æ­¥æ•°")
    parser.add_argument("--warmup_ratio", type=float, default=0.03, help="é¢„çƒ­æ¯”ä¾‹")
    parser.add_argument("--weight_decay", type=float, default=0.1, help="æƒé‡è¡°å‡")
    parser.add_argument("--gradient_checkpointing", action="store_true", help="å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹")
    
    # DeepSpeedå‚æ•°ï¼ˆå…¨å‚æ•°å¾®è°ƒï¼‰
    parser.add_argument("--deepspeed", type=str, default="zero2", help="DeepSpeedé…ç½®")
    
    # é¢„è®¾é…ç½®
    parser.add_argument("--preset", choices=['lora', 'qlora', 'full', 'multimodal'], 
                        help="ä½¿ç”¨é¢„è®¾é…ç½®")
    
    # éªŒè¯æ¨¡å¼
    parser.add_argument("--dry_run", action="store_true", help="åªéªŒè¯é…ç½®ï¼Œä¸æ‰§è¡Œè®­ç»ƒ")
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æ—¥å¿—
    logger = Logger("TrainScript").get_logger()
    logger.info("ğŸš€ å¯åŠ¨ç»Ÿä¸€è®­ç»ƒè„šæœ¬")
    
    try:
        # å¯¼å…¥æ ¸å¿ƒAPI
        from training.trainer import TrainingManager, TrainingPresets
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = TrainingManager(args.config)
        
        # æ„å»ºè®­ç»ƒé…ç½®
        if args.preset:
            # ä½¿ç”¨é¢„è®¾é…ç½®
            logger.info(f"ğŸ“‹ ä½¿ç”¨é¢„è®¾é…ç½®: {args.preset}")
            presets = {
                'lora': TrainingPresets.get_lora_preset,
                'qlora': TrainingPresets.get_qlora_preset,
                'full': TrainingPresets.get_full_preset,
                'multimodal': TrainingPresets.get_multimodal_preset
            }
            train_config = presets[args.preset](args.model)
            
            # åº”ç”¨å‘½ä»¤è¡Œè¦†ç›–
            if args.dataset:
                train_config['dataset'] = args.dataset
            if args.output_dir != "./6_output":
                train_config['output_dir'] = args.output_dir
        else:
            # ä»å‘½ä»¤è¡Œå‚æ•°æ„å»ºé…ç½®
            train_config = {
                'model': args.model,
                'dataset': args.dataset or ['AI-ModelScope/alpaca-gpt4-data-zh#1000'],
                'train_type': args.train_type,
                'output_dir': args.output_dir,
                'num_train_epochs': args.num_train_epochs,
                'per_device_train_batch_size': args.per_device_train_batch_size,
                'gradient_accumulation_steps': args.gradient_accumulation_steps,
                'learning_rate': args.learning_rate,
                'max_length': args.max_length,
                'lora_rank': args.lora_rank,
                'lora_alpha': args.lora_alpha,
                'lora_dropout': args.lora_dropout,
                'save_steps': args.save_steps,
                'logging_steps': args.logging_steps,
                'warmup_ratio': args.warmup_ratio,
                'weight_decay': args.weight_decay,
                'gradient_checkpointing': args.gradient_checkpointing,
                'deepspeed': args.deepspeed if args.train_type == 'full' else None
            }
        
        # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
        logger.info("ğŸ“Š è®­ç»ƒé…ç½®:")
        for key, value in train_config.items():
            logger.info(f"  {key}: {value}")
        
        if args.dry_run:
            logger.info("ğŸ” éªŒè¯æ¨¡å¼ï¼Œé…ç½®æ£€æŸ¥å®Œæˆ")
            return
        
        # æ‰§è¡Œè®­ç»ƒ
        logger.info("â³ å¼€å§‹è®­ç»ƒ...")
        result = trainer.execute_training(train_config)
        
        logger.info(f"âœ… è®­ç»ƒå®Œæˆï¼ç»“æœä¿å­˜åœ¨: {result}")
        
    except Exception as e:
        logger.error(f"âŒ è®­ç»ƒå¤±è´¥: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    main()