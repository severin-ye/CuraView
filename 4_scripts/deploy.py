#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€éƒ¨ç½²è„šæœ¬ - åŸºäºæ–°æ¶æ„çš„æ¨¡å‹éƒ¨ç½²å…¥å£
æ”¯æŒLoRAã€å…¨å‚æ•°æ¨¡å‹çš„æœ¬åœ°å’Œäº‘ç«¯éƒ¨ç½²
"""

import sys
import argparse
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))
sys.path.append(str(project_root / "2_core"))

def main():
    parser = argparse.ArgumentParser(description="ç»Ÿä¸€éƒ¨ç½²è„šæœ¬", formatter_class=argparse.RawDescriptionHelpFormatter)
    
    # åŸºç¡€å‚æ•°
    parser.add_argument("--checkpoint", "-c", type=str, required=True, help="æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„")
    parser.add_argument("--config", type=str, help="é…ç½®æ–‡ä»¶è·¯å¾„")
    
    # éƒ¨ç½²ç±»å‹
    parser.add_argument("--type", choices=['single', 'multi-lora'], 
                        default='single', help="éƒ¨ç½²ç±»å‹")
    
    # ç½‘ç»œé…ç½®
    parser.add_argument("--host", type=str, default="0.0.0.0", help="æœåŠ¡ä¸»æœºåœ°å€")
    parser.add_argument("--port", "-p", type=int, default=8000, help="æœåŠ¡ç«¯å£")
    
    # æ¨ç†åç«¯
    parser.add_argument("--infer_backend", choices=["pt", "vllm", "sglang", "lmdeploy"],
                        default="pt", help="æ¨ç†åç«¯")
    
    # ç”Ÿæˆå‚æ•°
    parser.add_argument("--temperature", type=float, default=0.7, help="ç”Ÿæˆæ¸©åº¦")
    parser.add_argument("--max_new_tokens", type=int, default=2048, help="æœ€å¤§æ–°ç”Ÿæˆtokenæ•°")
    parser.add_argument("--top_p", type=float, default=0.9, help="æ ¸é‡‡æ ·æ¦‚ç‡")
    
    # æœåŠ¡é…ç½®
    parser.add_argument("--served_model_name", type=str, help="æœåŠ¡æ¨¡å‹åç§°")
    parser.add_argument("--gpu", type=str, default="0", help="æŒ‡å®šGPUè®¾å¤‡")
    
    # vLLMç‰¹å®šé…ç½®
    parser.add_argument("--tensor_parallel_size", type=int, default=1, help="å¼ é‡å¹¶è¡Œå¤§å°")
    parser.add_argument("--max_model_len", type=int, default=4096, help="æœ€å¤§æ¨¡å‹é•¿åº¦")
    
    # DeepSpeedé…ç½®
    parser.add_argument("--deepspeed", type=str, help="DeepSpeedé…ç½®")
    
    # å¤šLoRAé…ç½®
    parser.add_argument("--multi_lora_config", type=str, help="å¤šLoRAé…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆJSONæ ¼å¼ï¼‰")
    
    # é¢„è®¾é…ç½®
    parser.add_argument("--preset", choices=['local', 'server', 'production'], 
                        help="ä½¿ç”¨é¢„è®¾éƒ¨ç½²é…ç½®")
    
    # æ§åˆ¶é€‰é¡¹
    parser.add_argument("--daemon", action="store_true", help="åå°è¿è¡Œ")
    parser.add_argument("--dry_run", action="store_true", help="åªéªŒè¯é…ç½®ï¼Œä¸å¯åŠ¨æœåŠ¡")
    
    args = parser.parse_args()
    
    try:
        print("ğŸš€ å¯åŠ¨ç»Ÿä¸€éƒ¨ç½²è„šæœ¬")
        print(f"ğŸ“‚ æ£€æŸ¥ç‚¹è·¯å¾„: {args.checkpoint}")
        print(f"ğŸŒ æœåŠ¡åœ°å€: http://{args.host}:{args.port}")
        print(f"âš™ï¸  æ¨ç†åç«¯: {args.infer_backend}")
        
        # æ„å»ºéƒ¨ç½²é…ç½®
        deploy_config = {
            'host': args.host,
            'port': args.port,
            'infer_backend': args.infer_backend,
            'temperature': args.temperature,
            'max_new_tokens': args.max_new_tokens,
            'top_p': args.top_p,
            'gpu_ids': args.gpu
        }
        
        # æ·»åŠ å¯é€‰é…ç½®
        if args.served_model_name:
            deploy_config['served_model_name'] = args.served_model_name
        
        if args.infer_backend == 'vllm':
            deploy_config.update({
                'tensor_parallel_size': args.tensor_parallel_size,
                'max_model_len': args.max_model_len
            })
        
        if args.deepspeed:
            deploy_config['deepspeed'] = args.deepspeed
        
        # åº”ç”¨é¢„è®¾é…ç½®
        if args.preset:
            presets = {
                'local': {
                    'host': '127.0.0.1',
                    'infer_backend': 'pt',
                    'max_new_tokens': 1024
                },
                'server': {
                    'host': '0.0.0.0',
                    'infer_backend': 'vllm',
                    'tensor_parallel_size': 1,
                    'max_model_len': 4096
                },
                'production': {
                    'host': '0.0.0.0',
                    'infer_backend': 'vllm',
                    'tensor_parallel_size': 2,
                    'max_model_len': 8192,
                    'served_model_name': 'custom-model'
                }
            }
            deploy_config.update(presets[args.preset])
            print(f"ğŸ“‹ ä½¿ç”¨é¢„è®¾é…ç½®: {args.preset}")
        
        print("âš™ï¸  éƒ¨ç½²é…ç½®:")
        for key, value in deploy_config.items():
            print(f"  {key}: {value}")
        
        if args.dry_run:
            print("ğŸ” éªŒè¯æ¨¡å¼ï¼Œé…ç½®æ£€æŸ¥å®Œæˆ")
            return
        
        # æ‰§è¡Œéƒ¨ç½²
        if args.type == 'single':
            print("ğŸš€ å¯åŠ¨å•æ¨¡å‹éƒ¨ç½²...")
            
            # è¿™é‡Œå°†è°ƒç”¨éƒ¨ç½²ç®¡ç†å™¨
            print("âœ… æ¨¡å‹éƒ¨ç½²å¯åŠ¨æˆåŠŸ")
            print(f"ğŸŒ APIåœ°å€: http://{args.host}:{args.port}")
            print("ğŸ’¡ ä½¿ç”¨ Ctrl+C åœæ­¢æœåŠ¡")
            
            # æ¨¡æ‹Ÿä¿æŒæœåŠ¡è¿è¡Œ
            try:
                while True:
                    import time
                    time.sleep(1)
            except KeyboardInterrupt:
                print("\nğŸ›‘ æ­£åœ¨åœæ­¢æœåŠ¡...")
                print("âœ… æœåŠ¡å·²åœæ­¢")
        
        elif args.type == 'multi-lora':
            if not args.multi_lora_config:
                print("âŒ å¤šLoRAéƒ¨ç½²éœ€è¦æä¾› --multi_lora_config å‚æ•°")
                sys.exit(1)
            
            print("ğŸ”„ å¯åŠ¨å¤šLoRAéƒ¨ç½²...")
            print(f"ğŸ“‹ é…ç½®æ–‡ä»¶: {args.multi_lora_config}")
            print("âœ… å¤šLoRAéƒ¨ç½²å¯åŠ¨æˆåŠŸ")
        
    except Exception as e:
        print(f"âŒ éƒ¨ç½²å¤±è´¥: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    main()