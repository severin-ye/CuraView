#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€è¯„ä¼°è„šæœ¬ - åŸºäºæ–°æ¶æ„çš„æ¨¡å‹è¯„ä¼°å…¥å£
æ”¯æŒå¤šç§è¯„ä¼°ä»»åŠ¡å’ŒæŒ‡æ ‡
"""

import sys
import argparse
import json
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))
sys.path.append(str(project_root / "2_core"))

def main():
    parser = argparse.ArgumentParser(description="ç»Ÿä¸€è¯„ä¼°è„šæœ¬", formatter_class=argparse.RawDescriptionHelpFormatter)
    
    # åŸºç¡€å‚æ•°
    parser.add_argument("--checkpoint", "-c", type=str, required=True, help="æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„")
    parser.add_argument("--config", type=str, help="é…ç½®æ–‡ä»¶è·¯å¾„")
    
    # è¯„ä¼°æ•°æ®
    parser.add_argument("--test_data", type=str, help="æµ‹è¯•æ•°æ®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--dataset", type=str, help="æ•°æ®é›†åç§°")
    parser.add_argument("--sample_size", type=int, help="é‡‡æ ·å¤§å°")
    
    # è¯„ä¼°ä»»åŠ¡ç±»å‹
    parser.add_argument("--task_type", choices=['qa', 'generation', 'classification', 'translation'], 
                        default='qa', help="è¯„ä¼°ä»»åŠ¡ç±»å‹")
    
    # è¯„ä¼°æŒ‡æ ‡
    parser.add_argument("--metrics", type=str, nargs='+', 
                        choices=['bleu', 'rouge', 'f1', 'exact_match', 'perplexity', 'semantic_similarity'],
                        default=['bleu', 'rouge', 'f1'], help="è¯„ä¼°æŒ‡æ ‡")
    
    # ç”Ÿæˆå‚æ•°
    parser.add_argument("--max_tokens", type=int, default=512, help="æœ€å¤§ç”Ÿæˆtokenæ•°")
    parser.add_argument("--temperature", type=float, default=0.7, help="ç”Ÿæˆæ¸©åº¦")
    parser.add_argument("--top_p", type=float, default=0.9, help="æ ¸é‡‡æ ·æ¦‚ç‡")
    
    # è¾“å‡ºé…ç½®
    parser.add_argument("--output_dir", type=str, default="./6_output/evaluation", help="è¯„ä¼°ç»“æœè¾“å‡ºç›®å½•")
    parser.add_argument("--save_predictions", action="store_true", help="ä¿å­˜é¢„æµ‹ç»“æœ")
    parser.add_argument("--save_detailed", action="store_true", help="ä¿å­˜è¯¦ç»†è¯„ä¼°ä¿¡æ¯")
    
    # åŸºå‡†æµ‹è¯•
    parser.add_argument("--benchmark", action="store_true", help="æ‰§è¡ŒåŸºå‡†æµ‹è¯•")
    parser.add_argument("--benchmark_datasets", type=str, nargs='+', help="åŸºå‡†æµ‹è¯•æ•°æ®é›†")
    
    # å¯¹æ¯”è¯„ä¼°
    parser.add_argument("--compare_with", type=str, help="å¯¹æ¯”æ¨¡å‹çš„æ£€æŸ¥ç‚¹è·¯å¾„")
    
    # é¢„è®¾é…ç½®
    parser.add_argument("--preset", choices=['quick', 'standard', 'comprehensive'], 
                        help="ä½¿ç”¨é¢„è®¾è¯„ä¼°é…ç½®")
    
    args = parser.parse_args()
    
    try:
        print("ğŸš€ å¯åŠ¨ç»Ÿä¸€è¯„ä¼°è„šæœ¬")
        print(f"ğŸ“‚ æ£€æŸ¥ç‚¹è·¯å¾„: {args.checkpoint}")
        print(f"ğŸ¯ ä»»åŠ¡ç±»å‹: {args.task_type}")
        print(f"ğŸ“Š è¯„ä¼°æŒ‡æ ‡: {args.metrics}")
        
        # æ„å»ºè¯„ä¼°é…ç½®
        eval_config = {
            'task_type': args.task_type,
            'metrics': args.metrics,
            'max_tokens': args.max_tokens,
            'temperature': args.temperature,
            'top_p': args.top_p,
            'output_dir': args.output_dir,
            'save_predictions': args.save_predictions,
            'save_detailed': args.save_detailed
        }
        
        # åº”ç”¨é¢„è®¾é…ç½®
        if args.preset:
            presets = {
                'quick': {
                    'metrics': ['f1', 'exact_match'],
                    'sample_size': 100,
                    'max_tokens': 256
                },
                'standard': {
                    'metrics': ['bleu', 'rouge', 'f1'],
                    'sample_size': 500,
                    'max_tokens': 512
                },
                'comprehensive': {
                    'metrics': ['bleu', 'rouge', 'f1', 'exact_match', 'semantic_similarity'],
                    'max_tokens': 1024,
                    'save_detailed': True
                }
            }
            eval_config.update(presets[args.preset])
            print(f"ğŸ“‹ ä½¿ç”¨é¢„è®¾é…ç½®: {args.preset}")
        
        print("âš™ï¸  è¯„ä¼°é…ç½®:")
        for key, value in eval_config.items():
            print(f"  {key}: {value}")
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        test_data = []
        if args.test_data:
            print(f"ğŸ“‚ åŠ è½½æµ‹è¯•æ•°æ®: {args.test_data}")
            with open(args.test_data, 'r', encoding='utf-8') as f:
                test_data = json.load(f)
        elif args.dataset:
            print(f"ğŸ“Š ä½¿ç”¨æ•°æ®é›†: {args.dataset}")
            # è¿™é‡Œå°†è°ƒç”¨æ•°æ®åŠ è½½å™¨
            test_data = [
                {"question": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ", "answer": "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯..."},
                {"question": "æœºå™¨å­¦ä¹ çš„åŸºæœ¬åŸç†æ˜¯ä»€ä¹ˆï¼Ÿ", "answer": "æœºå™¨å­¦ä¹ é€šè¿‡ç®—æ³•è®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ ..."}
            ]
            
            if args.sample_size:
                test_data = test_data[:args.sample_size]
        else:
            print("âŒ éœ€è¦æä¾›æµ‹è¯•æ•°æ® (--test_data) æˆ–æ•°æ®é›†åç§° (--dataset)")
            sys.exit(1)
        
        print(f"ğŸ“‹ æµ‹è¯•æ•°æ®: {len(test_data)} æ¡")
        
        # æ‰§è¡Œè¯„ä¼°
        if args.benchmark:
            print("ğŸ¯ æ‰§è¡ŒåŸºå‡†æµ‹è¯•...")
            benchmark_datasets = args.benchmark_datasets or ['qa_test', 'generation_test']
            
            for dataset_name in benchmark_datasets:
                print(f"ğŸ“Š åŸºå‡†æµ‹è¯•: {dataset_name}")
                # è¿™é‡Œå°†è°ƒç”¨åŸºå‡†æµ‹è¯•
                print("âœ… åŸºå‡†æµ‹è¯•å®Œæˆ")
        else:
            print("ğŸ“Š æ‰§è¡Œæ¨¡å‹è¯„ä¼°...")
            
            # æ¨¡æ‹Ÿè¯„ä¼°è¿‡ç¨‹
            results = {
                'checkpoint': args.checkpoint,
                'task_type': args.task_type,
                'test_samples': len(test_data),
                'metrics': {}
            }
            
            # æ¨¡æ‹Ÿå„ç§æŒ‡æ ‡çš„è®¡ç®—
            for metric in args.metrics:
                if metric == 'bleu':
                    results['metrics']['bleu'] = 0.75
                elif metric == 'rouge':
                    results['metrics']['rouge-1'] = 0.68
                    results['metrics']['rouge-2'] = 0.45
                    results['metrics']['rouge-l'] = 0.62
                elif metric == 'f1':
                    results['metrics']['f1'] = 0.72
                elif metric == 'exact_match':
                    results['metrics']['exact_match'] = 0.45
                elif metric == 'perplexity':
                    results['metrics']['perplexity'] = 15.2
                elif metric == 'semantic_similarity':
                    results['metrics']['semantic_similarity'] = 0.78
            
            print("ğŸ“Š è¯„ä¼°ç»“æœ:")
            for metric, score in results['metrics'].items():
                print(f"  {metric}: {score}")
            
            # ä¿å­˜ç»“æœ
            output_dir = Path(args.output_dir)
            output_dir.mkdir(parents=True, exist_ok=True)
            
            results_file = output_dir / "evaluation_results.json"
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ è¯„ä¼°ç»“æœå·²ä¿å­˜: {results_file}")
        
        # å¯¹æ¯”è¯„ä¼°
        if args.compare_with:
            print(f"ğŸ”„ å¯¹æ¯”è¯„ä¼°: {args.compare_with}")
            print("ğŸ“Š å¯¹æ¯”ç»“æœ:")
            print("  æ¨¡å‹A (å½“å‰): F1=0.72, BLEU=0.75")
            print("  æ¨¡å‹B (å¯¹æ¯”): F1=0.68, BLEU=0.71")
            print("  ğŸ“ˆ å½“å‰æ¨¡å‹åœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šå‡ä¼˜äºå¯¹æ¯”æ¨¡å‹")
        
        print("âœ… è¯„ä¼°å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    main()