import os
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# æŒ‡å®šGPU
# os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # æ³¨é‡Šæ‰ï¼Œä½¿ç”¨æ‰€æœ‰å¯ç”¨GPU

# æ˜¾ç¤ºGPUä¿¡æ¯
print("ğŸ”§ GPU é…ç½®ä¿¡æ¯:")
print(f"CUDA å¯ç”¨: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"æ£€æµ‹åˆ° GPU æ•°é‡: {torch.cuda.device_count()}")
    print(f"å½“å‰è®¾å¤‡: {torch.cuda.current_device()}")
    
    # æ˜¾ç¤ºæ‰€æœ‰GPUä¿¡æ¯
    for i in range(torch.cuda.device_count()):
        gpu_name = torch.cuda.get_device_name(i)
        gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3  # GB
        print(f"  GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)")
    
    print(f"CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES', 'æœªè®¾ç½®ï¼ˆå°†ä½¿ç”¨æ‰€æœ‰GPUï¼‰')}")
else:
    print("âš ï¸  æœªæ£€æµ‹åˆ°å¯ç”¨çš„ GPU")

print("\n" + "="*50)

# ä¿®å¤åçš„æ¨¡å‹è·¯å¾„
# model_path = "/home/work/hd/models/Qwen3-4B-Thinking-2507-FP8"
model_path = "/home/work/hd/models/Qwen3-30B-A3B-Thinking-2507"

print("æ­£åœ¨åŠ è½½æ¨¡å‹...")

# ä½¿ç”¨ transformers ç›´æ¥åŠ è½½ï¼ˆé¿å…å…¼å®¹æ€§é—®é¢˜ï¼‰
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.bfloat16,
    device_map="auto",  # è‡ªåŠ¨åˆ†é…åˆ°å¤šä¸ªGPU
    trust_remote_code=True
)

print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")

# æ˜¾ç¤ºæ¨¡å‹åˆ†å¸ƒä¿¡æ¯
print("\nğŸ“Š æ¨¡å‹è®¾å¤‡åˆ†å¸ƒ:")
try:
    if hasattr(model, 'hf_device_map') and model.hf_device_map:
        print("è®¾å¤‡æ˜ å°„:")
        device_map = model.hf_device_map
        if isinstance(device_map, dict):
            for layer, device in device_map.items():
                print(f"  {layer}: {device}")
        else:
            print(f"  è®¾å¤‡æ˜ å°„ç±»å‹: {type(device_map)}")
    else:
        print(f"æ¨¡å‹ä¸»è®¾å¤‡: {next(model.parameters()).device}")
        
    # æ£€æŸ¥æ¨¡å‹å‚æ•°åˆ†å¸ƒ
    device_count = {}
    for name, param in model.named_parameters():
        device = str(param.device)
        device_count[device] = device_count.get(device, 0) + 1
    
    print("å‚æ•°åˆ†å¸ƒç»Ÿè®¡:")
    for device, count in device_count.items():
        print(f"  {device}: {count} ä¸ªå‚æ•°å¼ é‡")
        
except Exception as e:
    print(f"æ— æ³•è·å–è®¾å¤‡æ˜ å°„ä¿¡æ¯: {e}")
    print(f"æ¨¡å‹è®¾å¤‡: {next(model.parameters()).device}")

# æ˜¾ç¤ºæ¯ä¸ªGPUçš„æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
print("\nğŸ’¾ GPU æ˜¾å­˜ä½¿ç”¨æƒ…å†µ:")
for i in range(torch.cuda.device_count()):
    allocated = torch.cuda.memory_allocated(i) / 1024**3  # GB
    reserved = torch.cuda.memory_reserved(i) / 1024**3   # GB
    total = torch.cuda.get_device_properties(i).total_memory / 1024**3  # GB
    print(f"  GPU {i}: å·²åˆ†é… {allocated:.2f}GB / é¢„ç•™ {reserved:.2f}GB / æ€»è®¡ {total:.1f}GB")

# æ¨ç†å‡½æ•°
def generate_response(prompt, max_tokens=float('inf'), temperature=0.7):
    messages = [{"role": "user", "content": prompt}]
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer(text, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            temperature=temperature,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response[len(text):].strip()

# æ‰§è¡Œæ¨ç†
prompt = "è¯·è¯¦ç»†è§£é‡Šä¸€ä¸‹å¤§è¯­è¨€æ¨¡å‹çš„æ€è€ƒæ¨¡å¼ï¼ˆThinking modeï¼‰æ˜¯ä»€ä¹ˆï¼Ÿ"
# æ‰“å°ç”¨æˆ·çš„é—®é¢˜
print(f"\nç”¨æˆ·é—®é¢˜: {prompt}")
# æ‰“å°åˆ†éš”çº¿
print("=" * 50) 
# æ˜¾ç¤ºAIå›å¤æ ‡è®°
print("AIå›å¤:")

response = generate_response(prompt)
print(response)
