#!/usr/bin/env python3
"""
å¾®è°ƒåæ¨¡å‹éƒ¨ç½²è„šæœ¬
æ”¯æŒLoRAå’Œå…¨å‚æ•°å¾®è°ƒåçš„æ¨¡å‹éƒ¨ç½²
"""

import os
import argparse
import subprocess
from pathlib import Path


def deploy_lora_model(checkpoint_path: str, port: int = 8000, **kwargs):
    """éƒ¨ç½²LoRAå¾®è°ƒåçš„æ¨¡å‹"""
    cmd = [
        "swift", "deploy",
        "--adapters", checkpoint_path,
        "--port", str(port),
        "--host", "0.0.0.0"
    ]
    
    # æ·»åŠ å¯é€‰å‚æ•°
    if kwargs.get("infer_backend"):
        cmd.extend(["--infer_backend", kwargs["infer_backend"]])
    if kwargs.get("temperature") is not None:
        cmd.extend(["--temperature", str(kwargs["temperature"])])
    if kwargs.get("max_new_tokens"):
        cmd.extend(["--max_new_tokens", str(kwargs["max_new_tokens"])])
    if kwargs.get("served_model_name"):
        cmd.extend(["--served_model_name", kwargs["served_model_name"]])
    
    return cmd


def deploy_full_model(checkpoint_path: str, port: int = 8000, **kwargs):
    """éƒ¨ç½²å…¨å‚æ•°å¾®è°ƒåçš„æ¨¡å‹"""
    cmd = [
        "swift", "deploy",
        "--model", checkpoint_path,
        "--port", str(port),
        "--host", "0.0.0.0"
    ]
    
    # æ·»åŠ å¯é€‰å‚æ•°
    if kwargs.get("infer_backend"):
        cmd.extend(["--infer_backend", kwargs["infer_backend"]])
    if kwargs.get("temperature") is not None:
        cmd.extend(["--temperature", str(kwargs["temperature"])])
    if kwargs.get("max_new_tokens"):
        cmd.extend(["--max_new_tokens", str(kwargs["max_new_tokens"])])
    if kwargs.get("served_model_name"):
        cmd.extend(["--served_model_name", kwargs["served_model_name"]])
    
    return cmd


def deploy_multi_lora(lora_configs: dict, port: int = 8000, **kwargs):
    """éƒ¨ç½²å¤šLoRAæ¨¡å‹"""
    adapters_str = " ".join([f"{name}={path}" for name, path in lora_configs.items()])
    
    cmd = [
        "swift", "deploy",
        "--adapters", adapters_str,
        "--port", str(port),
        "--host", "0.0.0.0"
    ]
    
    # æ·»åŠ å¯é€‰å‚æ•°
    if kwargs.get("infer_backend"):
        cmd.extend(["--infer_backend", kwargs["infer_backend"]])
    if kwargs.get("temperature") is not None:
        cmd.extend(["--temperature", str(kwargs["temperature"])])
    if kwargs.get("max_new_tokens"):
        cmd.extend(["--max_new_tokens", str(kwargs["max_new_tokens"])])
    
    return cmd


def main():
    parser = argparse.ArgumentParser(description="å¾®è°ƒåæ¨¡å‹éƒ¨ç½²")
    parser.add_argument("--checkpoint", "-c", type=str, required=True,
                       help="æ¨¡å‹checkpointè·¯å¾„")
    parser.add_argument("--type", "-t", choices=["lora", "full", "multi-lora"], 
                       default="auto", help="éƒ¨ç½²ç±»å‹")
    parser.add_argument("--port", "-p", type=int, default=8000,
                       help="æœåŠ¡ç«¯å£")
    parser.add_argument("--infer-backend", choices=["pt", "vllm", "sglang", "lmdeploy"],
                       default="pt", help="æ¨ç†åç«¯")
    parser.add_argument("--temperature", type=float, default=0.7,
                       help="ç”Ÿæˆæ¸©åº¦")
    parser.add_argument("--max-new-tokens", type=int, default=2048,
                       help="æœ€å¤§æ–°ç”Ÿæˆtokenæ•°")
    parser.add_argument("--served-model-name", type=str,
                       help="æœåŠ¡æ¨¡å‹åç§°")
    parser.add_argument("--gpu", type=str, default="0",
                       help="æŒ‡å®šGPUè®¾å¤‡")
    parser.add_argument("--multi-lora-config", type=str,
                       help="å¤šLoRAé…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆJSONæ ¼å¼ï¼‰")
    
    args = parser.parse_args()
    
    # è®¾ç½®GPU
    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu
    
    # éƒ¨ç½²å‚æ•°
    deploy_kwargs = {
        "infer_backend": args.infer_backend,
        "temperature": args.temperature,
        "max_new_tokens": args.max_new_tokens,
        "served_model_name": args.served_model_name,
    }
    
    # æ£€æµ‹éƒ¨ç½²ç±»å‹
    if args.type == "auto":
        checkpoint_path = Path(args.checkpoint)
        adapter_files = ["adapter_config.json", "adapter_model.safetensors", "adapter_model.bin"]
        has_adapter = any((checkpoint_path / f).exists() for f in adapter_files)
        deploy_type = "lora" if has_adapter else "full"
    else:
        deploy_type = args.type
    
    # ç”Ÿæˆéƒ¨ç½²å‘½ä»¤
    if deploy_type == "lora":
        cmd = deploy_lora_model(args.checkpoint, args.port, **deploy_kwargs)
        print(f"ğŸš€ éƒ¨ç½²LoRAæ¨¡å‹: {args.checkpoint}")
    elif deploy_type == "full":
        cmd = deploy_full_model(args.checkpoint, args.port, **deploy_kwargs)
        print(f"ğŸš€ éƒ¨ç½²å…¨å‚æ•°æ¨¡å‹: {args.checkpoint}")
    elif deploy_type == "multi-lora":
        if not args.multi_lora_config:
            print("âŒ å¤šLoRAéƒ¨ç½²éœ€è¦æŒ‡å®š--multi-lora-configå‚æ•°")
            return
        
        import json
        with open(args.multi_lora_config, 'r') as f:
            lora_configs = json.load(f)
        
        cmd = deploy_multi_lora(lora_configs, args.port, **deploy_kwargs)
        print(f"ğŸš€ éƒ¨ç½²å¤šLoRAæ¨¡å‹: {list(lora_configs.keys())}")
    
    # è¾“å‡ºéƒ¨ç½²å‘½ä»¤
    print("ğŸ“‹ éƒ¨ç½²å‘½ä»¤:")
    print(" ".join(cmd))
    print()
    
    # æ‰§è¡Œéƒ¨ç½²
    try:
        print("ğŸ”„ å¯åŠ¨éƒ¨ç½²æœåŠ¡...")
        subprocess.run(cmd, check=True)
    except subprocess.CalledProcessError as e:
        print(f"âŒ éƒ¨ç½²å¤±è´¥: {e}")
    except KeyboardInterrupt:
        print("\nğŸ‘‹ éƒ¨ç½²æœåŠ¡å·²åœæ­¢")


if __name__ == "__main__":
    main()