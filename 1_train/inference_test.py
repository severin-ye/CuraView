#!/usr/bin/env python3
"""
å¾®è°ƒåæ¨¡å‹æ¨ç†æµ‹è¯•è„šæœ¬
æ”¯æŒLoRAã€å…¨å‚æ•°å¾®è°ƒåçš„æ¨¡å‹æ¨ç†æµ‹è¯•
"""

import os
import json
import argparse
from pathlib import Path
from typing import List, Dict, Any, Optional

import torch
from swift.llm import (
    PtEngine, RequestConfig, InferRequest, 
    get_model_tokenizer, get_template, BaseArguments
)
from swift.tuners import Swift


class ModelInferenceTester:
    """å¾®è°ƒåæ¨¡å‹æ¨ç†æµ‹è¯•å™¨"""
    
    def __init__(self, checkpoint_path: str, test_questions: Optional[List[str]] = None):
        self.checkpoint_path = Path(checkpoint_path)
        self.test_questions = test_questions if test_questions is not None else self._get_default_questions()
        self.engine = None
        self.is_lora = False
        
    def _get_default_questions(self) -> List[str]:
        """è·å–é»˜è®¤æµ‹è¯•é—®é¢˜"""
        return [
            "ä½ æ˜¯è°ï¼Ÿ",
            "ä½ èƒ½åšä»€ä¹ˆï¼Ÿ",
            "è¯·å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—",
            "è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ",
            "ç”¨Pythonå†™ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•",
            "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
            "è¯·æ€»ç»“ä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•å†å²",
            "ä»€ä¹ˆæ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼Ÿ",
        ]
    
    def _load_model_from_lora_checkpoint(self):
        """ä»LoRA checkpointåŠ è½½æ¨¡å‹"""
        print(f"ğŸ“ åŠ è½½LoRA checkpoint: {self.checkpoint_path}")
        
        # åŠ è½½è®­ç»ƒå‚æ•°
        args = BaseArguments.from_pretrained(str(self.checkpoint_path))
        print(f"ğŸ”§ åŸºç¡€æ¨¡å‹: {args.model}")
        print(f"ğŸ¨ æ¨¡æ¿ç±»å‹: {getattr(args, 'template', 'default')}")
        print(f"ğŸ’­ ç³»ç»Ÿæç¤º: {getattr(args, 'system', None)}")
        
        # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
        model_id = getattr(args, 'model', None)
        if not model_id:
            raise ValueError("æ— æ³•è·å–åŸºç¡€æ¨¡å‹è·¯å¾„")
        model, tokenizer = get_model_tokenizer(model_id)
        
        # åŠ è½½LoRAæƒé‡
        model = Swift.from_pretrained(model, str(self.checkpoint_path))
        
        # åˆ›å»ºæ¨¡æ¿
        template_type = getattr(args, 'template', 'default')
        system_prompt = getattr(args, 'system', None)
        template = get_template(template_type, tokenizer, default_system=system_prompt)
        
        # åˆ›å»ºæ¨ç†å¼•æ“
        self.engine = PtEngine.from_model_template(model, template, max_batch_size=1)
        self.is_lora = True
        
    def _load_model_from_full_checkpoint(self):
        """ä»å…¨å‚æ•°checkpointåŠ è½½æ¨¡å‹"""
        print(f"ğŸ“ åŠ è½½å…¨å‚æ•°checkpoint: {self.checkpoint_path}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è®­ç»ƒå‚æ•°æ–‡ä»¶
        args_file = self.checkpoint_path / "args.json"
        if args_file.exists():
            args = BaseArguments.from_pretrained(str(self.checkpoint_path))
            model_path = str(self.checkpoint_path)
            template_type = getattr(args, 'template', 'default')
            default_system = getattr(args, 'system', None)
        else:
            # å¦‚æœæ²¡æœ‰args.jsonï¼Œå‡è®¾checkpointå°±æ˜¯æ¨¡å‹è·¯å¾„
            model_path = str(self.checkpoint_path)
            template_type = 'default'
            default_system = None
        
        # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
        model, tokenizer = get_model_tokenizer(model_path)
        
        # åˆ›å»ºæ¨¡æ¿
        template = get_template(template_type, tokenizer, default_system=default_system)
        
        # åˆ›å»ºæ¨ç†å¼•æ“
        self.engine = PtEngine.from_model_template(model, template, max_batch_size=1)
        self.is_lora = False
    
    def load_model(self):
        """è‡ªåŠ¨æ£€æµ‹å¹¶åŠ è½½æ¨¡å‹"""
        print("ğŸš€ å¼€å§‹åŠ è½½æ¨¡å‹...")
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºLoRA checkpointï¼ˆåŒ…å«adapterç›¸å…³æ–‡ä»¶ï¼‰
        adapter_files = ["adapter_config.json", "adapter_model.safetensors", "adapter_model.bin"]
        has_adapter = any((self.checkpoint_path / f).exists() for f in adapter_files)
        
        if has_adapter:
            self._load_model_from_lora_checkpoint()
        else:
            self._load_model_from_full_checkpoint()
    def _extract_response_content(self, resp) -> str:
        """ä»å“åº”ä¸­æå–å†…å®¹"""
        try:
            if hasattr(resp, 'choices') and resp.choices:
                choice = resp.choices[0]
                if hasattr(choice, 'message') and hasattr(choice.message, 'content'):
                    content = choice.message.content
                    return str(content) if content is not None else ""
        except (AttributeError, IndexError, TypeError):
            pass
        return ""
    
    def test_single_question(self, question: str, max_tokens: int = 512, temperature: float = 0.7) -> str:
        """æµ‹è¯•å•ä¸ªé—®é¢˜"""
        if not self.engine:
            raise ValueError("æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨load_model()")
        
        request_config = RequestConfig(max_tokens=max_tokens, temperature=temperature)
        infer_request = InferRequest(messages=[{'role': 'user', 'content': question}])
        
        resp_list = self.engine.infer([infer_request], request_config)
        if resp_list and len(resp_list) > 0:
            return self._extract_response_content(resp_list[0])
        return ""
    
    def run_batch_test(self, save_results: bool = True, output_file: str = "") -> Dict[str, Any]:
        """æ‰¹é‡æµ‹è¯•"""
        print("ğŸ§ª å¼€å§‹æ‰¹é‡æµ‹è¯•...")
        
        results = {
            "checkpoint_path": str(self.checkpoint_path),
            "model_type": "LoRA" if self.is_lora else "Full",
            "test_results": []
        }
        
        for i, question in enumerate(self.test_questions, 1):
            print(f"\nğŸ“ é—®é¢˜ {i}/{len(self.test_questions)}: {question}")
            print("-" * 50)
            
            try:
                response = self.test_single_question(question)
                print(f"ğŸ¤– å›ç­”: {response}")
                
                results["test_results"].append({
                    "question": question,
                    "response": response,
                    "status": "success"
                })
                
            except Exception as e:
                error_msg = f"æ¨ç†å¤±è´¥: {str(e)}"
                print(f"âŒ {error_msg}")
                
                results["test_results"].append({
                    "question": question,
                    "response": "",
                    "status": "error",
                    "error": error_msg
                })
        
        # ä¿å­˜ç»“æœ
        if save_results:
            if not output_file:
                output_file = f"inference_results_{self.checkpoint_path.name}.json"
            
            output_path = Path(output_file)
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            print(f"\nğŸ’¾ æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        
        return results
    
    def interactive_test(self):
        """äº¤äº’å¼æµ‹è¯•"""
        print("ğŸ® è¿›å…¥äº¤äº’å¼æµ‹è¯•æ¨¡å¼ (è¾“å…¥ 'quit' é€€å‡º)")
        
        while True:
            try:
                question = input("\nğŸ‘¤ è¯·è¾“å…¥é—®é¢˜: ").strip()
                
                if question.lower() in ['quit', 'exit', 'é€€å‡º']:
                    print("ğŸ‘‹ å†è§!")
                    break
                
                if not question:
                    continue
                
                print("ğŸ¤– AIå›ç­”:")
                response = self.test_single_question(question)
                print(response)
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ å†è§!")
                break
            except Exception as e:
                print(f"âŒ æ¨ç†é”™è¯¯: {e}")


def main():
    parser = argparse.ArgumentParser(description="å¾®è°ƒåæ¨¡å‹æ¨ç†æµ‹è¯•")
    parser.add_argument("--checkpoint", "-c", type=str, required=True,
                       help="æ¨¡å‹checkpointè·¯å¾„")
    parser.add_argument("--mode", "-m", choices=["batch", "interactive", "single"], 
                       default="batch", help="æµ‹è¯•æ¨¡å¼")
    parser.add_argument("--question", "-q", type=str, 
                       help="å•ä¸ªé—®é¢˜æµ‹è¯•ï¼ˆä»…åœ¨singleæ¨¡å¼ä¸‹ï¼‰")
    parser.add_argument("--questions-file", type=str,
                       help="è‡ªå®šä¹‰é—®é¢˜æ–‡ä»¶è·¯å¾„ï¼ˆJSONæ ¼å¼ï¼‰")
    parser.add_argument("--output", "-o", type=str,
                       help="ç»“æœè¾“å‡ºæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--max-tokens", type=int, default=512,
                       help="æœ€å¤§ç”Ÿæˆtokenæ•°")
    parser.add_argument("--temperature", type=float, default=0.7,
                       help="ç”Ÿæˆæ¸©åº¦")
    parser.add_argument("--gpu", type=str, default="0",
                       help="æŒ‡å®šGPUè®¾å¤‡")
    
    args = parser.parse_args()
    
    # è®¾ç½®GPU
    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu
    
    # åŠ è½½è‡ªå®šä¹‰é—®é¢˜ï¼ˆå¦‚æœæä¾›ï¼‰
    test_questions = None
    # å¤„ç†æµ‹è¯•é—®é¢˜
    test_questions = None
    if args.questions_file:
        with open(args.questions_file, 'r', encoding='utf-8') as f:
            questions_data = json.load(f)
            test_questions = questions_data if isinstance(questions_data, list) else questions_data.get('questions', [])
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = ModelInferenceTester(args.checkpoint, test_questions)
    
    # åŠ è½½æ¨¡å‹
    tester.load_model()
    
    # æ ¹æ®æ¨¡å¼æ‰§è¡Œæµ‹è¯•
    if args.mode == "batch":
        tester.run_batch_test(save_results=True, output_file=args.output)
    elif args.mode == "interactive":
        tester.interactive_test()
    elif args.mode == "single":
        if not args.question:
            print("âŒ singleæ¨¡å¼éœ€è¦æŒ‡å®š--questionå‚æ•°")
            return
        
        print(f"ğŸ“ é—®é¢˜: {args.question}")
        response = tester.test_single_question(
            args.question, 
            max_tokens=args.max_tokens, 
            temperature=args.temperature
        )
        print(f"ğŸ¤– å›ç­”: {response}")


if __name__ == "__main__":
    main()