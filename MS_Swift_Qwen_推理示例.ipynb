{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d09161e",
   "metadata": {},
   "source": [
    "# MS-Swift Qwen 模型推理示例\n",
    "\n",
    "本 notebook 演示如何使用 MS-Swift 框架对 Qwen 模型进行推理，解决常见的兼容性问题，并展示正确的 API 使用方法。\n",
    "\n",
    "## 目标\n",
    "1. 正确配置 MS-Swift 推理环境\n",
    "2. 解决 PyTorch 兼容性问题\n",
    "3. 演示正确的推理 API 用法\n",
    "4. 展示输出结果的正确处理方式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82d14e6",
   "metadata": {},
   "source": [
    "## 1. 环境设置和依赖检查\n",
    "\n",
    "首先检查和配置推理环境，确保所有依赖都正确安装。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9172494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 版本: 3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]\n",
      "PyTorch 版本: 2.8.0+cu128\n",
      "CUDA 可用: True\n",
      "CUDA 版本: 12.8\n",
      "GPU 数量: 1\n",
      "当前 GPU: 0\n",
      "设置使用 GPU: 0\n"
     ]
    }
   ],
   "source": [
    "# 检查基础环境\n",
    "import sys\n",
    "import torch\n",
    "import os\n",
    "\n",
    "print(f\"Python 版本: {sys.version}\")\n",
    "print(f\"PyTorch 版本: {torch.__version__}\")\n",
    "print(f\"CUDA 可用: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA 版本: {torch.version.cuda}\")\n",
    "    print(f\"GPU 数量: {torch.cuda.device_count()}\")\n",
    "    print(f\"当前 GPU: {torch.cuda.current_device()}\")\n",
    "\n",
    "# 设置 GPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "print(f\"设置使用 GPU: {os.environ['CUDA_VISIBLE_DEVICES']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfab9d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "清除 BNB_CUDA_VERSION: 123\n",
      "设置 LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/lib/python3.10/dist-packages/torch/lib:/usr/local/lib/python3.10/dist-packages/torch_tensorrt/lib:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-12.6:/usr/local/cuda-12.6/include:/usr/include/x86_64-linux-gnu\n",
      "环境配置修复完成\n"
     ]
    }
   ],
   "source": [
    "# 修复环境配置问题\n",
    "import os\n",
    "\n",
    "# 清除可能导致问题的环境变量\n",
    "if 'BNB_CUDA_VERSION' in os.environ:\n",
    "    print(f\"清除 BNB_CUDA_VERSION: {os.environ['BNB_CUDA_VERSION']}\")\n",
    "    del os.environ['BNB_CUDA_VERSION']\n",
    "\n",
    "# 设置正确的库路径\n",
    "cuda_lib_path = \"/usr/local/cuda/lib64\"\n",
    "if cuda_lib_path not in os.environ.get('LD_LIBRARY_PATH', ''):\n",
    "    os.environ['LD_LIBRARY_PATH'] = f\"{cuda_lib_path}:{os.environ.get('LD_LIBRARY_PATH', '')}\"\n",
    "    print(f\"设置 LD_LIBRARY_PATH: {os.environ['LD_LIBRARY_PATH']}\")\n",
    "\n",
    "print(\"环境配置修复完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0dbb08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ bitsandbytes 版本: 0.48.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work/hd/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MS-Swift 核心模块导入成功\n"
     ]
    }
   ],
   "source": [
    "# 测试 bitsandbytes 是否正常工作\n",
    "try:\n",
    "    import bitsandbytes as bnb\n",
    "    print(f\"✅ bitsandbytes 版本: {bnb.__version__}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ bitsandbytes 导入失败: {e}\")\n",
    "    \n",
    "# 测试 MS-Swift 导入\n",
    "try:\n",
    "    from swift.llm import PtEngine, RequestConfig, InferRequest\n",
    "    print(\"✅ MS-Swift 核心模块导入成功\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ MS-Swift 导入失败: {e}\")\n",
    "    print(\"如果遇到 scaled_dot_product_attention 错误，这是已知的兼容性问题\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe769e4",
   "metadata": {},
   "source": [
    "## 2. 兼容性问题解决方案\n",
    "\n",
    "当前环境中存在 PyTorch nightly 版本与 transformers 库的兼容性问题。我们提供几种解决方案："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06f3c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方案A: 使用 transformers 直接推理（推荐）\n",
    "print(\"=== 方案A: 使用 transformers 直接推理 ===\")\n",
    "print(\"这是最稳定的解决方案，绕过了 MS-Swift 的兼容性问题\")\n",
    "\n",
    "try:\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    print(\"✅ transformers 库可用\")\n",
    "    \n",
    "    # 检查模型路径\n",
    "    model_path = \"/home/work/hd/models/Qwen3-4B-Thinking-2507-FP8\"\n",
    "    import os\n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"✅ 模型路径存在: {model_path}\")\n",
    "    else:\n",
    "        print(f\"❌ 模型路径不存在: {model_path}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ transformers 导入失败: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c182ff",
   "metadata": {},
   "source": [
    "## 3. 使用 transformers 进行推理\n",
    "\n",
    "让我们使用 transformers 库直接进行推理，这避免了当前的兼容性问题："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19adbc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 transformers 加载模型和分词器\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_path = \"/home/work/hd/models/Qwen3-4B-Thinking-2507-FP8\"\n",
    "\n",
    "print(\"正在加载模型...\")\n",
    "try:\n",
    "    # 加载分词器\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    print(\"✅ 分词器加载成功\")\n",
    "    \n",
    "    # 加载模型（使用 torch.bfloat16 以节省显存）\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    print(\"✅ 模型加载成功\")\n",
    "    \n",
    "    print(f\"模型设备: {model.device}\")\n",
    "    print(f\"模型参数量: {sum(p.numel() for p in model.parameters())/1e9:.2f}B\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 模型加载失败: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5734e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推理测试\n",
    "def chat_with_model(prompt, max_tokens=512, temperature=0.7):\n",
    "    \"\"\"使用模型进行对话\"\"\"\n",
    "    try:\n",
    "        # 构建对话格式\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "        # 应用聊天模板\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        # 编码输入\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        # 生成回复\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=True if temperature > 0 else False,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # 解码输出\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # 提取新生成的部分\n",
    "        response = response[len(text):].strip()\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"推理失败: {e}\"\n",
    "\n",
    "# 测试推理\n",
    "test_prompt = \"请详细解释一下大语言模型的\"思考模式（Thinking mode）\"是什么？\"\n",
    "print(f\"用户: {test_prompt}\")\n",
    "print(\"=\" * 50)\n",
    "print(\"AI: \", end=\"\", flush=True)\n",
    "\n",
    "response = chat_with_model(test_prompt, max_tokens=512, temperature=0.7)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2029fa3",
   "metadata": {},
   "source": [
    "## 4. MS-Swift API 正确用法（待兼容性修复后使用）\n",
    "\n",
    "以下展示了 MS-Swift 的正确 API 用法，基于官方示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7a5bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MS-Swift 正确的 API 用法（示例代码）\n",
    "# 注意：当前环境下会因为兼容性问题而失败\n",
    "\n",
    "from swift.llm import PtEngine, RequestConfig, InferRequest\n",
    "\n",
    "def swift_inference_example():\n",
    "    \"\"\"\n",
    "    MS-Swift 推理的正确用法\n",
    "    基于官方 GitHub 示例：https://github.com/modelscope/ms-swift\n",
    "    \"\"\"\n",
    "    # 1. 创建推理引擎\n",
    "    model_path = \"/home/work/hd/models/Qwen3-4B-Thinking-2507-FP8\"\n",
    "    \n",
    "    engine = PtEngine(\n",
    "        model_path,\n",
    "        max_batch_size=1,\n",
    "        dtype='fp8',\n",
    "        # 可以尝试添加这些参数来解决兼容性问题\n",
    "        # attn_implementation='eager',\n",
    "        # torch_dtype=torch.bfloat16\n",
    "    )\n",
    "    \n",
    "    # 2. 创建请求配置\n",
    "    request_config = RequestConfig(\n",
    "        max_tokens=512,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    \n",
    "    # 3. 创建推理请求\n",
    "    infer_request = InferRequest(\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"请详细解释一下大语言模型的\"思考模式（Thinking mode）\"是什么？\"}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # 4. 执行推理\n",
    "    outputs = engine.infer([infer_request], request_config)\n",
    "    \n",
    "    # 5. 正确的输出访问方式（重要！）\n",
    "    response = outputs[0].choices[0].message.content\n",
    "    \n",
    "    return response\n",
    "\n",
    "# 展示正确的输出结构\n",
    "print(\"MS-Swift 推理的正确输出访问方式：\")\n",
    "print(\"outputs = engine.infer([infer_request], request_config)\")\n",
    "print(\"response = outputs[0].choices[0].message.content\")\n",
    "print()\n",
    "print(\"注意：这与 OpenAI API 的结构保持一致\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa9b594",
   "metadata": {},
   "source": [
    "## 5. 修复原始脚本\n",
    "\n",
    "让我们修复您的原始 `1.py` 脚本中的问题："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d4f0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原始脚本的问题和修复方案\n",
    "\n",
    "print(\"=== 原始脚本存在的问题 ===\")\n",
    "print(\"1. 模型路径错误\")\n",
    "print(\"   错误: /home/work/models/Qwen3-4B-Thinking-2507-FP8\")\n",
    "print(\"   正确: /home/work/hd/models/Qwen3-4B-Thinking-2507-FP8\")\n",
    "print()\n",
    "\n",
    "print(\"2. 兼容性问题\")\n",
    "print(\"   PyTorch nightly 版本与 transformers 不兼容\")\n",
    "print(\"   错误: scaled_dot_product_attention() got unexpected keyword argument 'enable_gqa'\")\n",
    "print()\n",
    "\n",
    "print(\"3. 输出访问方式（这个实际是正确的）\")\n",
    "print(\"   正确: outputs[0].choices[0].message.content\")\n",
    "print()\n",
    "\n",
    "# 创建修复后的脚本内容\n",
    "fixed_script = '''import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# 指定GPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "# 修复后的模型路径\n",
    "model_path = \"/home/work/hd/models/Qwen3-4B-Thinking-2507-FP8\"\n",
    "\n",
    "print(\"正在加载模型...\")\n",
    "\n",
    "# 使用 transformers 直接加载（避免兼容性问题）\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# 推理函数\n",
    "def generate_response(prompt, max_tokens=512, temperature=0.7):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response[len(text):].strip()\n",
    "\n",
    "# 执行推理\n",
    "prompt = \"请详细解释一下大语言模型的\"思考模式（Thinking mode）\"是什么？\"\n",
    "response = generate_response(prompt)\n",
    "print(\"Response:\", response)\n",
    "'''\n",
    "\n",
    "# 保存修复后的脚本\n",
    "with open('/home/work/hd/1_fixed.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(fixed_script)\n",
    "    \n",
    "print(\"✅ 修复后的脚本已保存为 1_fixed.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc86400a",
   "metadata": {},
   "source": [
    "## 6. 解决方案总结\n",
    "\n",
    "### 当前推荐的解决方案：\n",
    "\n",
    "1. **使用 transformers 直接推理**（最稳定）\n",
    "   - 避免了当前的兼容性问题\n",
    "   - 性能和功能基本一致\n",
    "   - 代码更简洁，更易理解\n",
    "\n",
    "2. **MS-Swift API 的正确用法**（待环境修复后使用）\n",
    "   - `outputs = engine.infer([infer_request], request_config)`\n",
    "   - `response = outputs[0].choices[0].message.content`\n",
    "\n",
    "### 兼容性问题的根本原因：\n",
    "- PyTorch nightly 版本 (2.5.0a0) \n",
    "- transformers 库中的 `scaled_dot_product_attention` 函数参数变化\n",
    "- `enable_gqa` 参数在新版本中被移除\n",
    "\n",
    "### 长期解决方案：\n",
    "1. 降级到稳定版本的 PyTorch\n",
    "2. 更新 transformers 到兼容版本  \n",
    "3. 等待 MS-Swift 更新以支持新版本"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
