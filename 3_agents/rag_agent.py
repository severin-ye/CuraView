#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RAGæ™ºèƒ½ä½“ - æ£€ç´¢å¢å¼ºç”Ÿæˆæ™ºèƒ½ä½“
ç»“åˆå‘é‡æ•°æ®åº“æ£€ç´¢å’Œå¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆèƒ½åŠ›
"""

import os
import sys
import json
import time
from pathlib import Path
from typing import Dict, Any, List, Optional, Union

# æ·»åŠ çˆ¶çº§è·¯å¾„
current_dir = Path(__file__).parent
parent_dir = current_dir.parent
sys.path.append(str(parent_dir))
sys.path.append(str(parent_dir / "1_utils"))

from base_agent import BaseAgent
from logger import Logger
from config_loader import ConfigLoader

try:
    import numpy as np
    from sentence_transformers import SentenceTransformer
    import faiss
except ImportError as e:
    print(f"âš ï¸  RAGä¾èµ–æœªå®‰è£…: {e}")
    print("è¯·å®‰è£…: pip install sentence-transformers faiss-cpu numpy")

class RAGAgent(BaseAgent):
    """RAGæ™ºèƒ½ä½“ - æ”¯æŒçŸ¥è¯†åº“æ£€ç´¢çš„å¯¹è¯æ™ºèƒ½ä½“"""
    
    def __init__(self, 
                 agent_name: str = "RAGåŠ©æ‰‹",
                 config_path: Optional[str] = None,
                 checkpoint_path: Optional[str] = None,
                 knowledge_base_path: Optional[str] = None):
        """
        åˆå§‹åŒ–RAGæ™ºèƒ½ä½“
        Args:
            agent_name: æ™ºèƒ½ä½“åç§°
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            checkpoint_path: æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„
            knowledge_base_path: çŸ¥è¯†åº“è·¯å¾„
        """
        super().__init__(agent_name, config_path, checkpoint_path)
        
        # RAGç‰¹å®šé…ç½®
        self.knowledge_base_path = knowledge_base_path
        self.embedding_model = None
        self.vector_index = None
        self.documents = []
        self.document_embeddings = None
        
        # RAGå‚æ•°
        self.top_k = self.config.get('top_k', 5)
        self.similarity_threshold = self.config.get('similarity_threshold', 0.5)
        self.max_context_length = self.config.get('max_context_length', 2000)
        
        # åŠ è½½åµŒå…¥æ¨¡å‹
        self.load_embedding_model()
        
        # åŠ è½½çŸ¥è¯†åº“
        if knowledge_base_path:
            self.load_knowledge_base(knowledge_base_path)
        
        self.logger.info("ğŸ” RAGæ™ºèƒ½ä½“åˆå§‹åŒ–å®Œæˆ")
    
    def get_system_prompt(self) -> str:
        """è·å–RAGæ™ºèƒ½ä½“çš„ç³»ç»Ÿæç¤ºè¯"""
        return """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æ™ºèƒ½ä½“ã€‚ä½ å…·æœ‰ä»¥ä¸‹èƒ½åŠ›ï¼š

1. ğŸ” çŸ¥è¯†æ£€ç´¢ï¼šä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯
2. ğŸ“š ä¸Šä¸‹æ–‡ç†è§£ï¼šåŸºäºæ£€ç´¢åˆ°çš„ä¿¡æ¯å›ç­”é—®é¢˜
3. ğŸ¯ å‡†ç¡®å›ç­”ï¼šç»“åˆçŸ¥è¯†åº“å†…å®¹æä¾›å‡†ç¡®çš„ç­”æ¡ˆ
4. ğŸ”— æ¥æºå¼•ç”¨ï¼šåœ¨å›ç­”ä¸­æ ‡æ³¨ä¿¡æ¯æ¥æº

å›ç­”è§„åˆ™ï¼š
- ä¼˜å…ˆä½¿ç”¨æ£€ç´¢åˆ°çš„çŸ¥è¯†åº“ä¿¡æ¯
- å¦‚æœçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜
- å¯¹äºæ£€ç´¢åˆ°çš„ä¿¡æ¯ï¼Œè¯·åœ¨å›ç­”æœ«å°¾æ³¨æ˜æ¥æº
- ä¿æŒå›ç­”çš„å‡†ç¡®æ€§å’Œç›¸å…³æ€§
- é¿å…ç¼–é€ ä¸å­˜åœ¨çš„ä¿¡æ¯

è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œæ£€ç´¢ç›¸å…³çŸ¥è¯†å¹¶ç»™å‡ºå‡†ç¡®çš„å›ç­”ã€‚"""
    
    def load_embedding_model(self):
        """åŠ è½½åµŒå…¥æ¨¡å‹"""
        try:
            model_name = self.config.get('embedding_model', 'all-MiniLM-L6-v2')
            self.embedding_model = SentenceTransformer(model_name)
            self.logger.info(f"âœ… åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ: {model_name}")
        except Exception as e:
            self.logger.error(f"âŒ åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            self.embedding_model = None
    
    def load_knowledge_base(self, kb_path: str):
        """
        åŠ è½½çŸ¥è¯†åº“
        Args:
            kb_path: çŸ¥è¯†åº“è·¯å¾„
        """
        try:
            kb_path_obj = Path(kb_path)
            
            if not kb_path_obj.exists():
                self.logger.warning(f"âš ï¸  çŸ¥è¯†åº“è·¯å¾„ä¸å­˜åœ¨: {kb_path_obj}")
                return
            
            # æ ¹æ®æ–‡ä»¶ç±»å‹åŠ è½½çŸ¥è¯†åº“
            if kb_path_obj.suffix == '.json':
                self.load_json_knowledge_base(kb_path_obj)
            elif kb_path_obj.suffix == '.txt':
                self.load_text_knowledge_base(kb_path_obj)
            elif kb_path_obj.is_dir():
                self.load_directory_knowledge_base(kb_path_obj)
            else:
                self.logger.warning(f"âš ï¸  ä¸æ”¯æŒçš„çŸ¥è¯†åº“æ ¼å¼: {kb_path_obj.suffix}")
                return
            
            # æ„å»ºå‘é‡ç´¢å¼•
            self.build_vector_index()
            
            self.logger.info(f"âœ… çŸ¥è¯†åº“åŠ è½½å®Œæˆï¼Œå…±{len(self.documents)}ä¸ªæ–‡æ¡£")
            
        except Exception as e:
            self.logger.error(f"âŒ çŸ¥è¯†åº“åŠ è½½å¤±è´¥: {str(e)}")
    
    def load_json_knowledge_base(self, json_path: Path):
        """åŠ è½½JSONæ ¼å¼çŸ¥è¯†åº“"""
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if isinstance(data, list):
            # æ–‡æ¡£åˆ—è¡¨æ ¼å¼
            for item in data:
                if isinstance(item, dict):
                    content = item.get('content', str(item))
                    title = item.get('title', f"æ–‡æ¡£{len(self.documents) + 1}")
                    source = item.get('source', str(json_path))
                else:
                    content = str(item)
                    title = f"æ–‡æ¡£{len(self.documents) + 1}"
                    source = str(json_path)
                
                self.documents.append({
                    'content': content,
                    'title': title,
                    'source': source,
                    'id': len(self.documents)
                })
        elif isinstance(data, dict):
            # é”®å€¼å¯¹æ ¼å¼
            for key, value in data.items():
                self.documents.append({
                    'content': str(value),
                    'title': str(key),
                    'source': str(json_path),
                    'id': len(self.documents)
                })
    
    def load_text_knowledge_base(self, txt_path: Path):
        """åŠ è½½æ–‡æœ¬æ ¼å¼çŸ¥è¯†åº“"""
        with open(txt_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
        
        for i, paragraph in enumerate(paragraphs):
            self.documents.append({
                'content': paragraph,
                'title': f"æ®µè½{i + 1}",
                'source': str(txt_path),
                'id': len(self.documents)
            })
    
    def load_directory_knowledge_base(self, dir_path: Path):
        """åŠ è½½ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡æ¡£"""
        supported_formats = ['.txt', '.json', '.md']
        
        for file_path in dir_path.rglob('*'):
            if file_path.is_file() and file_path.suffix in supported_formats:
                try:
                    if file_path.suffix == '.json':
                        self.load_json_knowledge_base(file_path)
                    else:
                        # å¤„ç†æ–‡æœ¬æ–‡ä»¶
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                        
                        self.documents.append({
                            'content': content,
                            'title': file_path.stem,
                            'source': str(file_path),
                            'id': len(self.documents)
                        })
                        
                except Exception as e:
                    self.logger.warning(f"âš ï¸  æ–‡ä»¶åŠ è½½å¤±è´¥ {file_path}: {e}")
    
    def build_vector_index(self):
        """æ„å»ºå‘é‡ç´¢å¼•"""
        if not self.embedding_model or not self.documents:
            self.logger.warning("âš ï¸  åµŒå…¥æ¨¡å‹æˆ–æ–‡æ¡£ä¸ºç©ºï¼Œæ— æ³•æ„å»ºç´¢å¼•")
            return
        
        try:
            self.logger.info("ğŸ”¨ å¼€å§‹æ„å»ºå‘é‡ç´¢å¼•...")
            
            # æå–æ–‡æ¡£å†…å®¹
            doc_contents = [doc['content'] for doc in self.documents]
            
            # ç”ŸæˆåµŒå…¥
            embeddings = self.embedding_model.encode(doc_contents, show_progress_bar=True)
            self.document_embeddings = embeddings
            
            # æ„å»ºFAISSç´¢å¼•
            dimension = embeddings.shape[1]
            self.vector_index = faiss.IndexFlatIP(dimension)  # å†…ç§¯ç›¸ä¼¼åº¦
            
            # å½’ä¸€åŒ–åµŒå…¥å‘é‡
            faiss.normalize_L2(embeddings)
            self.vector_index.add(embeddings.astype('float32'))
            
            self.logger.info(f"âœ… å‘é‡ç´¢å¼•æ„å»ºå®Œæˆï¼Œç»´åº¦: {dimension}")
            
        except Exception as e:
            self.logger.error(f"âŒ å‘é‡ç´¢å¼•æ„å»ºå¤±è´¥: {str(e)}")
            self.vector_index = None
    
    def retrieve_documents(self, query: str, top_k: Optional[int] = None) -> List[Dict[str, Any]]:
        """
        æ£€ç´¢ç›¸å…³æ–‡æ¡£
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›æ–‡æ¡£æ•°é‡
        Returns:
            List[Dict[str, Any]]: æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨
        """
        if not self.vector_index or not self.embedding_model:
            self.logger.warning("âš ï¸  å‘é‡ç´¢å¼•æˆ–åµŒå…¥æ¨¡å‹æœªå°±ç»ª")
            return []
        
        if top_k is None:
            top_k = self.top_k
        
        try:
            # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
            query_embedding = self.embedding_model.encode([query])
            faiss.normalize_L2(query_embedding)
            
            # æ£€ç´¢ç›¸ä¼¼æ–‡æ¡£
            scores, indices = self.vector_index.search(query_embedding.astype('float32'), top_k)
            
            # æ„å»ºç»“æœ
            results = []
            for score, idx in zip(scores[0], indices[0]):
                if idx < len(self.documents) and score >= self.similarity_threshold:
                    doc = self.documents[idx].copy()
                    doc['similarity_score'] = float(score)
                    results.append(doc)
            
            self.logger.info(f"ğŸ” æ£€ç´¢åˆ°{len(results)}ä¸ªç›¸å…³æ–‡æ¡£")
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ æ–‡æ¡£æ£€ç´¢å¤±è´¥: {str(e)}")
            return []
    
    def format_retrieved_context(self, retrieved_docs: List[Dict[str, Any]]) -> str:
        """
        æ ¼å¼åŒ–æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
        Args:
            retrieved_docs: æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨
        Returns:
            str: æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡
        """
        if not retrieved_docs:
            return ""
        
        context_parts = []
        current_length = 0
        
        for i, doc in enumerate(retrieved_docs):
            content = doc['content']
            title = doc['title']
            source = doc['source']
            score = doc.get('similarity_score', 0)
            
            # æ ¼å¼åŒ–æ–‡æ¡£
            doc_text = f"[æ–‡æ¡£{i+1}] {title}\næ¥æº: {source}\nç›¸ä¼¼åº¦: {score:.3f}\nå†…å®¹: {content}\n"
            
            # æ£€æŸ¥é•¿åº¦é™åˆ¶
            if current_length + len(doc_text) > self.max_context_length:
                break
            
            context_parts.append(doc_text)
            current_length += len(doc_text)
        
        return "\n" + "="*50 + "\n".join(context_parts) + "="*50 + "\n"
    
    def process_input(self, user_input: str, context: Optional[Dict[str, Any]] = None) -> str:
        """
        å¤„ç†ç”¨æˆ·è¾“å…¥ï¼ˆRAGæµç¨‹ï¼‰
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
        Returns:
            str: å¤„ç†ç»“æœ
        """
        try:
            # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
            retrieved_docs = self.retrieve_documents(user_input)
            
            if not retrieved_docs:
                # æ²¡æœ‰æ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£ï¼Œä½¿ç”¨åŸºç¡€å›ç­”
                if self.model_loaded:
                    base_prompt = f"ç”¨æˆ·é—®é¢˜: {user_input}\n\næ³¨æ„ï¼šçŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ï¼Œè¯·åŸºäºä½ çš„åŸºç¡€çŸ¥è¯†å›ç­”ï¼Œå¹¶è¯´æ˜è¿™ä¸æ˜¯åŸºäºçŸ¥è¯†åº“çš„å›ç­”ã€‚"
                    return self.infer(base_prompt)
                else:
                    return "æŠ±æ­‰ï¼Œæˆ‘åœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ï¼Œä¸”æ¨¡å‹æœªåŠ è½½ã€‚è¯·æ£€æŸ¥é—®é¢˜æˆ–è”ç³»ç®¡ç†å‘˜ã€‚"
            
            # 2. æ ¼å¼åŒ–æ£€ç´¢ä¸Šä¸‹æ–‡
            context_text = self.format_retrieved_context(retrieved_docs)
            
            # 3. æ„å»ºå¢å¼ºæç¤º
            enhanced_prompt = f"""åŸºäºä»¥ä¸‹æ£€ç´¢åˆ°çš„çŸ¥è¯†åº“ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ï¼š

{context_text}

ç”¨æˆ·é—®é¢˜: {user_input}

è¯·åŸºäºä¸Šè¿°çŸ¥è¯†åº“ä¿¡æ¯å›ç­”é—®é¢˜ï¼Œå¹¶åœ¨å›ç­”æœ«å°¾æ³¨æ˜å‚è€ƒçš„æ–‡æ¡£æ¥æºã€‚å¦‚æœçŸ¥è¯†åº“ä¿¡æ¯ä¸è¶³ä»¥å®Œå…¨å›ç­”é—®é¢˜ï¼Œè¯·è¯´æ˜ã€‚"""
            
            # 4. ç”Ÿæˆå›ç­”
            if self.model_loaded:
                response = self.infer(enhanced_prompt, max_tokens=1024)
                
                # 5. åå¤„ç†ï¼šæ·»åŠ æ¥æºä¿¡æ¯
                sources = list(set([doc['source'] for doc in retrieved_docs]))
                source_info = f"\n\nğŸ“š å‚è€ƒæ¥æº:\n" + "\n".join([f"â€¢ {source}" for source in sources[:3]])
                
                return response + source_info
            else:
                # æ¨¡å‹æœªåŠ è½½æ—¶çš„ç®€åŒ–å›ç­”
                return f"åŸºäºçŸ¥è¯†åº“æ£€ç´¢ï¼Œæˆ‘æ‰¾åˆ°äº†{len(retrieved_docs)}ä¸ªç›¸å…³æ–‡æ¡£ã€‚ä½†ç”±äºæ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•ç”Ÿæˆè¯¦ç»†å›ç­”ã€‚\n\næ£€ç´¢åˆ°çš„å…³é”®ä¿¡æ¯:\n" + \
                       "\n".join([f"â€¢ {doc['title']}: {doc['content'][:100]}..." for doc in retrieved_docs[:3]])
        
        except Exception as e:
            self.logger.error(f"âŒ RAGå¤„ç†å¤±è´¥: {str(e)}")
            return f"æŠ±æ­‰ï¼Œåœ¨å¤„ç†æ‚¨çš„é—®é¢˜æ—¶é‡åˆ°äº†é”™è¯¯: {str(e)}"
    
    def add_document(self, content: str, title: str, source: str = "manual_add"):
        """
        æ·»åŠ æ–°æ–‡æ¡£åˆ°çŸ¥è¯†åº“
        Args:
            content: æ–‡æ¡£å†…å®¹
            title: æ–‡æ¡£æ ‡é¢˜
            source: æ–‡æ¡£æ¥æº
        """
        try:
            doc = {
                'content': content,
                'title': title,
                'source': source,
                'id': len(self.documents)
            }
            
            self.documents.append(doc)
            
            # é‡æ–°æ„å»ºç´¢å¼•
            self.build_vector_index()
            
            self.logger.info(f"âœ… æ–‡æ¡£æ·»åŠ æˆåŠŸ: {title}")
            
        except Exception as e:
            self.logger.error(f"âŒ æ–‡æ¡£æ·»åŠ å¤±è´¥: {str(e)}")
    
    def search_documents(self, query: str, top_k: int = 10) -> List[Dict[str, Any]]:
        """
        æœç´¢æ–‡æ¡£ï¼ˆä¸ç”Ÿæˆå›ç­”ï¼‰
        Args:
            query: æœç´¢æŸ¥è¯¢
            top_k: è¿”å›æ–‡æ¡£æ•°é‡
        Returns:
            List[Dict[str, Any]]: æœç´¢ç»“æœ
        """
        return self.retrieve_documents(query, top_k)
    
    def get_knowledge_base_stats(self) -> Dict[str, Any]:
        """
        è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯
        Returns:
            Dict[str, Any]: ç»Ÿè®¡ä¿¡æ¯
        """
        if not self.documents:
            return {
                'total_documents': 0,
                'total_characters': 0,
                'sources': [],
                'index_ready': False
            }
        
        total_chars = sum(len(doc['content']) for doc in self.documents)
        sources = list(set(doc['source'] for doc in self.documents))
        
        return {
            'total_documents': len(self.documents),
            'total_characters': total_chars,
            'average_doc_length': total_chars / len(self.documents),
            'sources': sources,
            'index_ready': self.vector_index is not None,
            'embedding_model': getattr(self.embedding_model, 'model_name', 'unknown') if self.embedding_model else None
        }
    
    def get_capabilities(self) -> List[str]:
        """è·å–RAGæ™ºèƒ½ä½“èƒ½åŠ›åˆ—è¡¨"""
        base_capabilities = super().get_capabilities()
        rag_capabilities = [
            "çŸ¥è¯†åº“æ£€ç´¢",
            "å‘é‡ç›¸ä¼¼åº¦æœç´¢",
            "ä¸Šä¸‹æ–‡å¢å¼ºç”Ÿæˆ",
            "æ¥æºå¼•ç”¨",
            "æ–‡æ¡£ç®¡ç†",
            "å®æ—¶çŸ¥è¯†æ›´æ–°"
        ]
        return base_capabilities + rag_capabilities

def create_rag_agent(agent_name: str = "RAGåŠ©æ‰‹",
                    config_path: Optional[str] = None,
                    checkpoint_path: Optional[str] = None,
                    knowledge_base_path: Optional[str] = None) -> RAGAgent:
    """
    åˆ›å»ºRAGæ™ºèƒ½ä½“å®ä¾‹
    Args:
        agent_name: æ™ºèƒ½ä½“åç§°
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        checkpoint_path: æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„
        knowledge_base_path: çŸ¥è¯†åº“è·¯å¾„
    Returns:
        RAGAgent: RAGæ™ºèƒ½ä½“å®ä¾‹
    """
    return RAGAgent(agent_name, config_path, checkpoint_path, knowledge_base_path)

if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    print("ğŸ§ª RAGæ™ºèƒ½ä½“æµ‹è¯•...")
    
    # åˆ›å»ºæµ‹è¯•çŸ¥è¯†åº“
    test_kb = [
        {
            "title": "äººå·¥æ™ºèƒ½ç®€ä»‹",
            "content": "äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œæ—¨åœ¨åˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚",
            "source": "AIæ•™ç¨‹"
        },
        {
            "title": "æœºå™¨å­¦ä¹ ",
            "content": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œé€šè¿‡ç®—æ³•è®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼ï¼Œæ— éœ€æ˜ç¡®ç¼–ç¨‹ã€‚",
            "source": "MLæ•™ç¨‹"
        }
    ]
    
    # ä¿å­˜æµ‹è¯•çŸ¥è¯†åº“
    test_kb_path = "test_knowledge_base.json"
    with open(test_kb_path, 'w', encoding='utf-8') as f:
        json.dump(test_kb, f, ensure_ascii=False, indent=2)
    
    # åˆ›å»ºRAGæ™ºèƒ½ä½“
    rag_agent = create_rag_agent(
        agent_name="æµ‹è¯•RAGåŠ©æ‰‹",
        knowledge_base_path=test_kb_path
    )
    
    # æµ‹è¯•æ£€ç´¢
    results = rag_agent.search_documents("ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½")
    print(f"ğŸ” æ£€ç´¢ç»“æœ: {len(results)}ä¸ªæ–‡æ¡£")
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = rag_agent.get_knowledge_base_stats()
    print(f"ğŸ“Š çŸ¥è¯†åº“ç»Ÿè®¡: {stats}")
    
    # æ¸…ç†æµ‹è¯•æ–‡ä»¶
    os.remove(test_kb_path)
    
    print("âœ… RAGæ™ºèƒ½ä½“æ¨¡å—åŠ è½½æˆåŠŸ")