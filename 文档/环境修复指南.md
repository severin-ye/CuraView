# MS-SWIFT ç¯å¢ƒä¿®å¤å’Œä½¿ç”¨æŒ‡å—

## ğŸ”§ ç¯å¢ƒé—®é¢˜ä¿®å¤

### 1. ä¿®å¤ bitsandbytes é—®é¢˜

æ ¹æ®æ‚¨é‡åˆ°çš„é”™è¯¯ï¼Œä¸»è¦æ˜¯ `bitsandbytes` å’Œ `triton` ä¾èµ–é—®é¢˜ã€‚ä»¥ä¸‹æ˜¯è§£å†³æ–¹æ¡ˆï¼š

```bash
# æ–¹æ¡ˆ1: é‡æ–°å®‰è£… bitsandbytes
pip uninstall bitsandbytes -y
pip install bitsandbytes

# æ–¹æ¡ˆ2: è®¾ç½®ç¯å¢ƒå˜é‡ç¦ç”¨é‡åŒ–
export USE_BNB=0
export BNB_CUDA_VERSION=""

# æ–¹æ¡ˆ3: å®‰è£…å…¼å®¹ç‰ˆæœ¬
pip install bitsandbytes==0.43.0
pip install triton>=2.0.0
```

### 2. åˆ›å»ºç¯å¢ƒé…ç½®è„šæœ¬

åˆ›å»º `setup_env.sh`:

```bash
#!/bin/bash
# ç¯å¢ƒé…ç½®è„šæœ¬

echo "é…ç½® MS-SWIFT ç¯å¢ƒ..."

# ç¦ç”¨æœ‰é—®é¢˜çš„é‡åŒ–åº“
export USE_BNB=0
export BNB_CUDA_VERSION=""
export CUDA_VISIBLE_DEVICES=0

# è®¾ç½® Python è·¯å¾„
export PYTHONPATH=$PYTHONPATH:/home/work/hd

echo "ç¯å¢ƒé…ç½®å®Œæˆï¼"
echo "USE_BNB=$USE_BNB"
echo "CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES"
```

## ğŸš€ æ¨èä½¿ç”¨æ–¹æ³•

### æ–¹æ³•1: å‘½ä»¤è¡Œæ¨ç†ï¼ˆæ¨èï¼‰

```bash
# è®¾ç½®ç¯å¢ƒ
source setup_env.sh

# æ¨ç†å°æ¨¡å‹
CUDA_VISIBLE_DEVICES=0 swift infer \
    --model ./models/Qwen3-4B-Thinking-2507-FP8 \
    --stream true \
    --infer_backend pt \
    --max_new_tokens 256 \
    --temperature 0.7

# å¦‚æœä¸Šé¢å¤±è´¥ï¼Œå°è¯•ä¸ä½¿ç”¨æµå¼è¾“å‡º
CUDA_VISIBLE_DEVICES=0 swift infer \
    --model ./models/Qwen3-4B-Thinking-2507-FP8 \
    --infer_backend pt \
    --max_new_tokens 256
```

### æ–¹æ³•2: Web UI ç•Œé¢ï¼ˆæœ€ç®€å•ï¼‰

```bash
# å¯åŠ¨ä¸­æ–‡ Web ç•Œé¢
SWIFT_UI_LANG=zh swift web-ui

# æˆ–å¯åŠ¨è‹±æ–‡ç•Œé¢
SWIFT_UI_LANG=en swift web-ui
```

ç„¶ååœ¨æµè§ˆå™¨æ‰“å¼€ `http://localhost:7860`

### æ–¹æ³•3: ä½¿ç”¨ transformers æ›¿ä»£

å¦‚æœ MS-SWIFT æœ‰é—®é¢˜ï¼Œå¯ä»¥ä½¿ç”¨åŸç”Ÿ transformersï¼š

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# åŠ è½½æœ¬åœ°æ¨¡å‹
model_path = "./models/Qwen3-4B-Thinking-2507-FP8"
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True
)

# ç®€å•å¯¹è¯
def chat(question):
    messages = [{"role": "user", "content": question}]
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer([text], return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(**inputs, max_new_tokens=256, temperature=0.7)
    
    response = tokenizer.decode(outputs[0][len(inputs.input_ids[0]):], skip_special_tokens=True)
    return response

# æµ‹è¯•
print(chat("ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±"))
```

## ğŸ› ï¸ æ•…éšœæ’é™¤

### é—®é¢˜1: ImportError æˆ– ModuleNotFoundError

```bash
# é‡æ–°å®‰è£… ms-swift
pip uninstall ms-swift -y
pip install ms-swift -U

# å®‰è£…å®Œæ•´ä¾èµ–
pip install torch transformers accelerate
```

### é—®é¢˜2: CUDA ç›¸å…³é”™è¯¯

```bash
# æ£€æŸ¥ CUDA çŠ¶æ€
nvidia-smi

# é‡æ–°å®‰è£… PyTorch
pip uninstall torch torchvision torchaudio -y
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### é—®é¢˜3: æ˜¾å­˜ä¸è¶³

```bash
# ä½¿ç”¨é‡åŒ–ï¼ˆå¦‚æœ bitsandbytes å·¥ä½œï¼‰
--load_in_8bit true

# æˆ–å‡å°‘æ‰¹å¤„ç†å¤§å°
--per_device_train_batch_size 1
--max_length 1024
```

## ğŸ“‹ å¿«é€Ÿæµ‹è¯•å‘½ä»¤

åˆ›å»ºæµ‹è¯•è„šæœ¬ `quick_test.py`:

```python
#!/usr/bin/env python3
import os
os.environ['USE_BNB'] = '0'  # ç¦ç”¨æœ‰é—®é¢˜çš„é‡åŒ–

try:
    # æµ‹è¯• swift å¯¼å…¥
    from swift.llm import get_model_list
    print("âœ… MS-SWIFT å¯¼å…¥æˆåŠŸ")
    
    # æ˜¾ç¤ºæ”¯æŒçš„æ¨¡å‹
    models = get_model_list()
    print(f"æ”¯æŒçš„æ¨¡å‹æ•°é‡: {len(models)}")
    
except Exception as e:
    print(f"âŒ MS-SWIFT å¯¼å…¥å¤±è´¥: {e}")
    
try:
    # æµ‹è¯• transformers
    from transformers import AutoTokenizer
    print("âœ… Transformers å¯ç”¨")
    
except Exception as e:
    print(f"âŒ Transformers ä¸å¯ç”¨: {e}")

# æ£€æŸ¥æœ¬åœ°æ¨¡å‹
models = [
    "./models/Qwen3-4B-Thinking-2507-FP8",
    "./models/Qwen3-30B-A3B-Thinking-2507"
]

for model_path in models:
    if os.path.exists(model_path):
        print(f"âœ… æ¨¡å‹å­˜åœ¨: {model_path}")
        # æ£€æŸ¥å…³é”®æ–‡ä»¶
        config_file = os.path.join(model_path, "config.json")
        if os.path.exists(config_file):
            print(f"  âœ… config.json")
        else:
            print(f"  âŒ config.json ç¼ºå¤±")
    else:
        print(f"âŒ æ¨¡å‹ä¸å­˜åœ¨: {model_path}")
```

## ğŸ¯ æ¨èå·¥ä½œæµç¨‹

1. **ç¯å¢ƒå‡†å¤‡**
   ```bash
   # è®¾ç½®ç¯å¢ƒå˜é‡
   export USE_BNB=0
   export BNB_CUDA_VERSION=""
   
   # æµ‹è¯•ç¯å¢ƒ
   python quick_test.py
   ```

2. **é€‰æ‹©ä½¿ç”¨æ–¹å¼**
   - æ–°æ‰‹ï¼šä½¿ç”¨ Web UI (`swift web-ui`)
   - å‘½ä»¤è¡Œï¼šä½¿ç”¨ `swift infer`
   - ç¼–ç¨‹ï¼šä½¿ç”¨ transformers åº“

3. **ä»å°æ¨¡å‹å¼€å§‹**
   - å…ˆæµ‹è¯• 4B æ¨¡å‹
   - æˆåŠŸåå†å°è¯• 30B æ¨¡å‹

4. **é€æ­¥å¢åŠ å¤æ‚åº¦**
   - æ¨ç† â†’ å¾®è°ƒ â†’ éƒ¨ç½²

## ğŸ“ è·å–å¸®åŠ©

- æŸ¥çœ‹è¯¦ç»†æ–‡æ¡£ï¼š`MS-SWIFT_ä½¿ç”¨æŒ‡å—.md`
- è¿è¡Œæµ‹è¯•è„šæœ¬ï¼š`python test_swift_fixed.py`
- GitHub Issues: https://github.com/modelscope/ms-swift/issues
- å®˜æ–¹æ–‡æ¡£: https://swift.readthedocs.io/zh-cn/latest/