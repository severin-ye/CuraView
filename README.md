# MS-Swift æ¨¡å‹å¾®è°ƒå®Œæ•´å·¥å…·åŒ…

æœ¬å·¥å…·åŒ…æä¾›äº†ä½¿ç”¨ MS-Swift æ¡†æ¶è¿›è¡Œå¤§è¯­è¨€æ¨¡å‹å¾®è°ƒçš„å®Œæ•´è§£å†³æ–¹æ¡ˆï¼ŒåŒ…æ‹¬è®­ç»ƒã€æ¨ç†å’Œéƒ¨ç½²çš„å…¨å¥—è„šæœ¬ã€‚

## ğŸ“‹ ç›®å½•ç»“æ„

```
/home/work/hd/
â”œâ”€â”€ finetune_trainer.py      # ä¸»å¾®è°ƒè„šæœ¬
â”œâ”€â”€ inference_test.py        # æ¨ç†æµ‹è¯•è„šæœ¬
â”œâ”€â”€ deploy_model.py          # æ¨¡å‹éƒ¨ç½²è„šæœ¬
â”œâ”€â”€ configs/                 # é…ç½®æ–‡ä»¶ç›®å½•
â”‚   â”œâ”€â”€ lora_sft.json       # LoRAå¾®è°ƒé…ç½®
â”‚   â”œâ”€â”€ qlora_sft.json      # QLoRAå¾®è°ƒé…ç½®
â”‚   â”œâ”€â”€ full_sft.json       # å…¨å‚æ•°å¾®è°ƒé…ç½®
â”‚   â””â”€â”€ multimodal_sft.json # å¤šæ¨¡æ€å¾®è°ƒé…ç½®
â”œâ”€â”€ test_questions.json     # æµ‹è¯•é—®é¢˜é›†
â””â”€â”€ README.md               # æœ¬æ–‡æ¡£
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# å®‰è£…MS-Swift
pip install ms-swift -U

# å¯é€‰ï¼šå®‰è£…æ¨ç†åŠ é€Ÿåç«¯
pip install vllm -U          # vLLMåç«¯
pip install sglang -U        # SGlangåç«¯
pip install lmdeploy -U      # LMDeployåç«¯

# å¯é€‰ï¼šå®‰è£…æ·±åº¦å¹¶è¡Œè®­ç»ƒ
pip install deepspeed -U     # DeepSpeed
```

### 2. åŸºç¡€å¾®è°ƒ

#### LoRAå¾®è°ƒï¼ˆæ¨èï¼‰
```bash
# ä½¿ç”¨é…ç½®æ–‡ä»¶
python finetune_trainer.py --config configs/lora_sft.json

# æˆ–å‘½ä»¤è¡Œå‚æ•°
python finetune_trainer.py \
    --model "Qwen/Qwen2.5-7B-Instruct" \
    --train_type "lora" \
    --dataset "AI-ModelScope/alpaca-gpt4-data-zh#1000" \
    --output_dir "./output/lora_test" \
    --num_train_epochs 3 \
    --learning_rate 1e-4
```

#### QLoRAå¾®è°ƒï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰
```bash
python finetune_trainer.py --config configs/qlora_sft.json
```

#### å…¨å‚æ•°å¾®è°ƒ
```bash
python finetune_trainer.py --config configs/full_sft.json
```

### 3. æ¨ç†æµ‹è¯•

#### æ‰¹é‡æµ‹è¯•
```bash
# æµ‹è¯•å¾®è°ƒåçš„æ¨¡å‹
python inference_test.py --checkpoint ./output/lora_test/checkpoint-100

# ä½¿ç”¨è‡ªå®šä¹‰é—®é¢˜é›†
python inference_test.py \
    --checkpoint ./output/lora_test/checkpoint-100 \
    --questions-file test_questions.json \
    --output results.json
```

#### äº¤äº’å¼æµ‹è¯•
```bash
python inference_test.py \
    --checkpoint ./output/lora_test/checkpoint-100 \
    --mode interactive
```

#### å•ä¸ªé—®é¢˜æµ‹è¯•
```bash
python inference_test.py \
    --checkpoint ./output/lora_test/checkpoint-100 \
    --mode single \
    --question "ä½ æ˜¯è°ï¼Ÿ"
```

### 4. æ¨¡å‹éƒ¨ç½²

#### éƒ¨ç½²LoRAæ¨¡å‹
```bash
python deploy_model.py \
    --checkpoint ./output/lora_test/checkpoint-100 \
    --type lora \
    --port 8000 \
    --infer-backend vllm
```

#### éƒ¨ç½²å…¨å‚æ•°æ¨¡å‹
```bash
python deploy_model.py \
    --checkpoint ./output/full_test/checkpoint-100 \
    --type full \
    --port 8000
```

## ğŸ“– è¯¦ç»†ä½¿ç”¨è¯´æ˜

### å¾®è°ƒè„šæœ¬ (finetune_trainer.py)

è¿™æ˜¯ä¸»è¦çš„å¾®è°ƒè„šæœ¬ï¼Œæ”¯æŒå¤šç§å¾®è°ƒæ–¹å¼ï¼š

#### ä¸»è¦åŠŸèƒ½
- **LoRAå¾®è°ƒ**: è½»é‡çº§å‚æ•°é«˜æ•ˆå¾®è°ƒ
- **QLoRAå¾®è°ƒ**: é‡åŒ–+LoRAï¼Œè¿›ä¸€æ­¥èŠ‚çœæ˜¾å­˜
- **å…¨å‚æ•°å¾®è°ƒ**: æ›´æ–°æ‰€æœ‰æ¨¡å‹å‚æ•°
- **å¤šæ¨¡æ€å¾®è°ƒ**: æ”¯æŒå›¾åƒã€è§†é¢‘ã€éŸ³é¢‘ç­‰å¤šæ¨¡æ€æ•°æ®
- **åˆ†å¸ƒå¼è®­ç»ƒ**: æ”¯æŒå¤šGPUã€å¤šæœºè®­ç»ƒ
- **æµå¼æ•°æ®**: æ”¯æŒå¤§è§„æ¨¡æ•°æ®é›†çš„æµå¼åŠ è½½

#### ä½¿ç”¨æ–¹å¼

**æ–¹å¼1ï¼šä½¿ç”¨é…ç½®æ–‡ä»¶ï¼ˆæ¨èï¼‰**
```bash
python finetune_trainer.py --config configs/lora_sft.json
```

**æ–¹å¼2ï¼šå‘½ä»¤è¡Œå‚æ•°**
```bash
python finetune_trainer.py \
    --model "Qwen/Qwen2.5-7B-Instruct" \
    --train_type "lora" \
    --dataset "AI-ModelScope/alpaca-gpt4-data-zh#1000" \
    --output_dir "./output/test" \
    --num_train_epochs 3 \
    --learning_rate 1e-4 \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 8 \
    --save_steps 100 \
    --eval_steps 100
```

**æ–¹å¼3ï¼šPython API**
```python
from finetune_trainer import FineTuner

trainer = FineTuner("configs/lora_sft.json")
trainer.run_training()
```

#### é‡è¦å‚æ•°è¯´æ˜

| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|--------|
| `model` | åŸºç¡€æ¨¡å‹è·¯å¾„æˆ–ID | å¿…éœ€ |
| `train_type` | è®­ç»ƒç±»å‹ï¼šlora/qlora/full | "lora" |
| `dataset` | è®­ç»ƒæ•°æ®é›† | å¿…éœ€ |
| `output_dir` | è¾“å‡ºç›®å½• | "./output" |
| `num_train_epochs` | è®­ç»ƒè½®æ•° | 3 |
| `learning_rate` | å­¦ä¹ ç‡ | 1e-4 |
| `lora_rank` | LoRAç§© | 8 |
| `lora_alpha` | LoRA alpha | 32 |
| `max_length` | æœ€å¤§åºåˆ—é•¿åº¦ | 2048 |

### æ¨ç†æµ‹è¯•è„šæœ¬ (inference_test.py)

ç”¨äºæµ‹è¯•å¾®è°ƒåæ¨¡å‹çš„æ€§èƒ½å’Œæ•ˆæœã€‚

#### åŠŸèƒ½ç‰¹æ€§
- **è‡ªåŠ¨æ£€æµ‹æ¨¡å‹ç±»å‹**: è‡ªåŠ¨è¯†åˆ«LoRAæˆ–å…¨å‚æ•°æ¨¡å‹
- **æ‰¹é‡æµ‹è¯•**: ä½¿ç”¨é¢„å®šä¹‰é—®é¢˜é›†è¿›è¡Œæ‰¹é‡æµ‹è¯•
- **äº¤äº’å¼æµ‹è¯•**: å®æ—¶å¯¹è¯æµ‹è¯•
- **ç»“æœä¿å­˜**: è‡ªåŠ¨ä¿å­˜æµ‹è¯•ç»“æœåˆ°JSONæ–‡ä»¶
- **è‡ªå®šä¹‰é—®é¢˜**: æ”¯æŒè‡ªå®šä¹‰æµ‹è¯•é—®é¢˜é›†

#### æµ‹è¯•æ¨¡å¼

**1. æ‰¹é‡æµ‹è¯•æ¨¡å¼**
```bash
python inference_test.py \
    --checkpoint ./output/lora_test/checkpoint-100 \
    --mode batch \
    --output test_results.json
```

**2. äº¤äº’å¼æµ‹è¯•æ¨¡å¼**
```bash
python inference_test.py \
    --checkpoint ./output/lora_test/checkpoint-100 \
    --mode interactive
```

**3. å•é—®é¢˜æµ‹è¯•æ¨¡å¼**
```bash
python inference_test.py \
    --checkpoint ./output/lora_test/checkpoint-100 \
    --mode single \
    --question "è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½"
```

### éƒ¨ç½²è„šæœ¬ (deploy_model.py)

ç”¨äºå°†å¾®è°ƒåçš„æ¨¡å‹éƒ¨ç½²ä¸ºAPIæœåŠ¡ã€‚

#### æ”¯æŒçš„åç«¯
- **pt**: PyTorchåŸç”Ÿåç«¯
- **vllm**: vLLMé«˜æ€§èƒ½æ¨ç†åç«¯
- **sglang**: SGLangæ¨ç†åç«¯
- **lmdeploy**: LMDeployæ¨ç†åç«¯

#### éƒ¨ç½²ç¤ºä¾‹

**å•æ¨¡å‹éƒ¨ç½²**
```bash
python deploy_model.py \
    --checkpoint ./output/lora_test/checkpoint-100 \
    --port 8000 \
    --infer-backend vllm \
    --served-model-name "my-custom-model"
```

**å¤šLoRAéƒ¨ç½²**
```bash
# é¦–å…ˆåˆ›å»ºå¤šLoRAé…ç½®æ–‡ä»¶ multi_lora_config.json
{
  "model1": "./output/lora1/checkpoint-100",
  "model2": "./output/lora2/checkpoint-100"
}

# ç„¶åéƒ¨ç½²
python deploy_model.py \
    --type multi-lora \
    --multi-lora-config multi_lora_config.json \
    --port 8000 \
    --infer-backend vllm
```

#### å®¢æˆ·ç«¯è°ƒç”¨ç¤ºä¾‹

éƒ¨ç½²å®Œæˆåï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼è°ƒç”¨ï¼š

**ä½¿ç”¨curl**
```bash
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "my-custom-model",
    "messages": [{"role": "user", "content": "ä½ å¥½ï¼Œä½ æ˜¯è°ï¼Ÿ"}],
    "max_tokens": 512,
    "temperature": 0.7
  }'
```

**ä½¿ç”¨Python OpenAIå®¢æˆ·ç«¯**
```python
from openai import OpenAI

client = OpenAI(
    api_key='EMPTY',
    base_url='http://localhost:8000/v1'
)

response = client.chat.completions.create(
    model='my-custom-model',
    messages=[{'role': 'user', 'content': 'ä½ å¥½ï¼Œä½ æ˜¯è°ï¼Ÿ'}],
    max_tokens=512,
    temperature=0.7
)

print(response.choices[0].message.content)
```

## âš™ï¸ é…ç½®æ–‡ä»¶è¯´æ˜

### LoRAé…ç½® (configs/lora_sft.json)
é€‚ç”¨äºå¤§å¤šæ•°å¾®è°ƒåœºæ™¯ï¼Œå¹³è¡¡æ•ˆæœå’Œèµ„æºæ¶ˆè€—ã€‚

### QLoRAé…ç½® (configs/qlora_sft.json)
é€‚ç”¨äºæ˜¾å­˜å—é™çš„ç¯å¢ƒï¼Œä½¿ç”¨é‡åŒ–æŠ€æœ¯è¿›ä¸€æ­¥èŠ‚çœæ˜¾å­˜ã€‚

### å…¨å‚æ•°é…ç½® (configs/full_sft.json)
é€‚ç”¨äºè¿½æ±‚æœ€ä½³æ•ˆæœçš„åœºæ™¯ï¼Œéœ€è¦æ›´å¤šè®¡ç®—èµ„æºã€‚

### å¤šæ¨¡æ€é…ç½® (configs/multimodal_sft.json)
é€‚ç”¨äºå¤šæ¨¡æ€æ¨¡å‹çš„å¾®è°ƒï¼Œæ”¯æŒå›¾åƒã€è§†é¢‘ã€éŸ³é¢‘æ•°æ®ã€‚

## ğŸ¯ æœ€ä½³å®è·µ

### 1. æ•°æ®é›†å‡†å¤‡
```python
# è‡ªå®šä¹‰æ•°æ®é›†æ ¼å¼
[
    {
        "system": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ã€‚",
        "conversations": [
            {"from": "user", "value": "ä½ å¥½"},
            {"from": "assistant", "value": "ä½ å¥½ï¼æˆ‘æ˜¯AIåŠ©æ‰‹ï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"}
        ]
    }
]
```

### 2. è¶…å‚æ•°è°ƒä¼˜å»ºè®®

| å‚æ•° | å°æ¨¡å‹(7B) | å¤§æ¨¡å‹(30B+) | è¯´æ˜ |
|------|------------|--------------|------|
| `learning_rate` | 1e-4 | 5e-5 | å¤§æ¨¡å‹ç”¨æ›´å°å­¦ä¹ ç‡ |
| `lora_rank` | 8-16 | 32-64 | å¤§æ¨¡å‹å¯ç”¨æ›´å¤§rank |
| `batch_size` | 4-8 | 1-2 | æ ¹æ®æ˜¾å­˜è°ƒæ•´ |
| `gradient_accumulation_steps` | 4-8 | 16-32 | ä¿è¯æœ‰æ•ˆbatch size |

### 3. æ˜¾å­˜ä¼˜åŒ–æŠ€å·§

**èŠ‚çœæ˜¾å­˜çš„é…ç½®ç»„åˆï¼š**
```json
{
    "train_type": "qlora",
    "quantization_bit": 4,
    "gradient_checkpointing": true,
    "deepspeed": "zero2",
    "per_device_train_batch_size": 1,
    "gradient_accumulation_steps": 16
}
```

### 4. å¤šGPUè®­ç»ƒ

**æ•°æ®å¹¶è¡Œ**
```bash
# ä½¿ç”¨æ‰€æœ‰å¯ç”¨GPU
python finetune_trainer.py --config configs/lora_sft.json

# æŒ‡å®šç‰¹å®šGPU
CUDA_VISIBLE_DEVICES=0,1,2,3 python finetune_trainer.py --config configs/lora_sft.json
```

**DeepSpeedåŠ é€Ÿ**
```bash
python finetune_trainer.py \
    --config configs/lora_sft.json \
    --deepspeed zero2
```

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

**1. æ˜¾å­˜ä¸è¶³ (CUDA Out of Memory)**
- å‡å° `per_device_train_batch_size`
- å¢åŠ  `gradient_accumulation_steps`
- ä½¿ç”¨ `gradient_checkpointing`
- å°è¯• QLoRA æˆ– DeepSpeed

**2. è®­ç»ƒé€Ÿåº¦æ…¢**
- æ£€æŸ¥æ˜¯å¦å¯ç”¨äº† `flash_attn`
- ä½¿ç”¨æ›´å¤§çš„ `batch_size`
- è€ƒè™‘å¤šGPUè®­ç»ƒ
- å°è¯• `pack_sequence_as_prompt` æé«˜GPUåˆ©ç”¨ç‡

**3. æ¨¡å‹æ•ˆæœä¸ç†æƒ³**
- å¢åŠ è®­ç»ƒæ•°æ®é‡
- è°ƒæ•´å­¦ä¹ ç‡
- å¢åŠ è®­ç»ƒè½®æ•°
- æ£€æŸ¥æ•°æ®è´¨é‡

**4. æ¨ç†é€Ÿåº¦æ…¢**
- ä½¿ç”¨ vLLM æˆ–å…¶ä»–æ¨ç†åŠ é€Ÿåç«¯
- å¯ç”¨ `flash_attn`
- è€ƒè™‘æ¨¡å‹é‡åŒ–

### è°ƒè¯•æŠ€å·§

**å¯ç”¨è¯¦ç»†æ—¥å¿—**
```bash
export SWIFT_DEBUG=1
python finetune_trainer.py --config configs/lora_sft.json
```

**éªŒè¯æ•°æ®åŠ è½½**
```python
from finetune_trainer import FineTuner

trainer = FineTuner("configs/lora_sft.json")
trainer.verify_data()  # æ£€æŸ¥æ•°æ®æ ¼å¼
```

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

| æ–¹æ³• | æ˜¾å­˜å ç”¨ | è®­ç»ƒé€Ÿåº¦ | æ•ˆæœè´¨é‡ | éƒ¨ç½²å…¼å®¹æ€§ |
|------|----------|----------|----------|------------|
| å…¨å‚æ•° | â­â­â­â­â­ | â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ |
| LoRA | â­â­â­ | â­â­â­â­ | â­â­â­â­ | â­â­â­â­â­ |
| QLoRA | â­â­ | â­â­â­ | â­â­â­ | â­â­â­ |

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestæ¥æ”¹è¿›è¿™ä¸ªå·¥å…·åŒ…ï¼

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®éµå¾ªMITè®¸å¯è¯ã€‚

## ğŸ”— ç›¸å…³é“¾æ¥

- [MS-Swiftå®˜æ–¹æ–‡æ¡£](https://github.com/modelscope/ms-swift)
- [ModelScopeæ¨¡å‹åº“](https://modelscope.cn/models)
- [Hugging Faceæ¨¡å‹åº“](https://huggingface.co/models)

---

**å¿«é€Ÿå¼€å§‹ç¤ºä¾‹ï¼š**

```bash
# 1. å…‹éš†æˆ–ä¸‹è½½æœ¬å·¥å…·åŒ…
# 2. å®‰è£…ä¾èµ–
pip install ms-swift -U

# 3. å¼€å§‹LoRAå¾®è°ƒ
python finetune_trainer.py --config configs/lora_sft.json

# 4. æµ‹è¯•å¾®è°ƒæ•ˆæœ
python inference_test.py --checkpoint ./output/xxx/checkpoint-xxx

# 5. éƒ¨ç½²æ¨¡å‹æœåŠ¡
python deploy_model.py --checkpoint ./output/xxx/checkpoint-xxx
```

ç°åœ¨æ‚¨å°±å¯ä»¥å¼€å§‹ä½¿ç”¨è¿™ä¸ªå®Œæ•´çš„å¾®è°ƒå·¥å…·åŒ…äº†ï¼ğŸ‰